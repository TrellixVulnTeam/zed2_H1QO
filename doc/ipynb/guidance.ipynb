{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"guidance.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"EjB9UB4Qf8Yq","colab_type":"text"},"source":["## google drive"]},{"cell_type":"code","metadata":{"id":"aN3pPGzdQesl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1601442350142,"user_tz":-540,"elapsed":30976,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"abff1869-9e1a-4ae5-a6f2-b1c9f281d9f0"},"source":["from google.colab import files\n","from google.colab import drive\n","drive.mount('/content/mount')\n","# lab.ai.ch\n","#Ctrl + M + L"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/mount\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mTv7cwfZDOrP","colab_type":"code","colab":{}},"source":["%cd '/content/mount/My Drive/00_work/USE'\n","#!tar zxvf STAIR-captions/stair_captions_v1.2.tar.gz \n","!tar zxvf  'universal-sentence-encoder-multilingual-large_3.tar.gz'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1gHDr0fTU55L","colab_type":"code","colab":{}},"source":["%cd '/content/mount/My Drive/00_work/bleurt/bleurt'\n","!wget https://storage.googleapis.com/bleurt-oss/bleurt-base-128.zip\n","!unzip bleurt-base-128.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fOhrzHMxZSN9","colab_type":"code","colab":{}},"source":["%cd '/content/mount/My Drive/00_work/bleurt/bleurt'\n","!wget https://storage.googleapis.com/bleurt-oss/bleurt-large-512.zip\n","!unzip bleurt-large-512.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-I_nyFOTU9hX","colab_type":"code","colab":{}},"source":["!git clone https://github.com/google-research/bleurt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h0fLMf4Gdp-L","colab_type":"text"},"source":["#01_title\n"]},{"cell_type":"code","metadata":{"id":"FHjpqFaXWPiq","colab_type":"code","colab":{}},"source":["import tensorflow_hub as hub\n","# import tensorflow_text\n","import numpy as np\n","import pandas as pd\n","import json\n","\n","\n","def createFile(col,pathout):  \n","  json_remark = json.load(open(pathout+\"sentence.json\", \"r\"))\n","  json_title = json.load(open(pathout+\"word_\"+col+\".json\", \"r\"))\n","\n","  tiltles = []\n","  remarks = []\n","  outFile=[]\n","  for remark in json_remark:\n","      # remark_vec = embed(remark[\"sentence\"])\n","      for title in json_title:\n","          tiltles.append(title[col])\n","          remarks.append(remark[\"sentence\"])\n","          outFile.append([remark[\"sentence\"],title['word'],title[col]])\n","\n","  df = pd.DataFrame(tiltles, columns=['title'])\n","  df.to_csv(pathout+\"00_candidate_file.csv\", header=None,index=None)\n","  df = pd.DataFrame(remarks, columns=['sentence'])\n","  df.to_csv(pathout+\"00_reference_file_file.csv\", header=None,index=None)\n","\n","  df = pd.DataFrame(outFile, columns=['sentence','word',col])\n","  df.to_csv(pathout+\"00_out_col.csv\",index=None)\n","\n","def linkResult(out_col,score_col,pathout,col):  \n","    df_out=pd.read_csv(out_col,sep=',',header=0)\n","    df_score=pd.read_csv(score_col,sep=',',header=None)\n","    df_score.columns = ['similarity']\n","    df=pd.concat([df_out,df_score],axis=1)\n","    #00_base128_score\n","    df.to_csv(pathout+\"00_out_base128_\"+col+\"_similarity.csv\", header=None,index=None)\n","# 01_title\n","columns='title'\n","path='/content/mount/My Drive/00_work/guidance/20200701/01_title/'\n","# createFile(columns,path)\n","linkResult(path+'00_out_col.csv',path+'00_base128_score.csv',path,columns)\n","\n","#02_confirm\n","columns='confirmation'\n","path='/content/mount/My Drive/00_work/guidance/20200701/02_confirm/'\n","# createFile(columns,path)\n","linkResult(path+'00_out_col.csv',path+'00_base128_score.csv',path,columns)\n","\n","#03_detail \n","columns='detail'\n","path='/content/mount/My Drive/00_work/guidance/20200701/03_detail/'\n","# createFile(columns,path)\n","linkResult(path+'00_out_col.csv',path+'00_base128_score.csv',path,columns)\n","\n","#04_word  \n","columns='synonyms'\n","path='/content/mount/My Drive/00_work/guidance/20200701/04_word/'\n","# createFile(columns,path)\n","linkResult(path+'00_out_col.csv',path+'00_base128_score.csv',path,columns)\n","\n","#05_all  \n","columns='all'\n","path='/content/mount/My Drive/00_work/guidance/20200701/05_all/'\n","# createFile(columns,path)\n","linkResult(path+'00_out_col.csv',path+'00_base128_score.csv',path,columns)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fBdnAivBaMub","colab_type":"code","colab":{}},"source":["%cd '/content/mount/My Drive/00_work/bleurt'\n","path='/content/mount/My\\ Drive/00_work/guidance/20200701/01_title/'\n","path='/content/mount/My\\ Drive/00_work/guidance/20200701/02_confirm/'\n","path='/content/mount/My\\ Drive/00_work/guidance/20200701/03_detail/'\n","path='/content/mount/My\\ Drive/00_work/guidance/20200701/04_word/'\n","path='/content/mount/My\\ Drive/00_work/guidance/20200701/05_all/'\n","!python -m bleurt.score \\\n","  -candidate_file={path}\"00_candidate_file.csv\" \\\n","  -reference_file={path}\"00_reference_file_file.csv\" \\\n","  -bleurt_checkpoint=bleurt/bleurt-base-128  \\\n","  -scores_file={path}\"00_base128_score.csv\"\n"," "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b4YYFL5FYJ4-","colab_type":"code","colab":{}},"source":["# 01_title\n","columns='title'\n","path='/content/mount/My Drive/00_work/guidance/20200701/01_title/'\n","df_out=pd.read_csv(path+'00_out_col.csv',sep=',',header=0)\n","df_score=pd.read_csv(path+'00_score.csv',sep=',',header=None)\n","df_score.columns = ['similarity']\n","df=pd.concat([df_out,df_score],axis=1)\n","print(df_out.shape)\n","print(df_score.shape)\n","print(df.shape)\n","# df.to_csv(pathout+\"00_out_\"+col+\"_similarity.csv\", header=None,index=None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TN3xVUS_g1B7","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1C4Siis3g1uV","colab_type":"text"},"source":["# 環境構築\n","```\n","# MeCabのインストール\n","!apt install aptitude swig\n","!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n","!pip install mecab-python3\n","# mecab-ipadic-NEologdのインストール\n","!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git\n","!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -a\n","!pip install unidic-lite\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"eGNG2DzqezHA","colab_type":"text"},"source":["##word2vec"]},{"cell_type":"code","metadata":{"id":"lq3EiMSje1-Q","colab_type":"code","colab":{}},"source":["# %cd '/content/mount/My Drive/00_work/word2vec'\n","#https://drive.google.com/file/d/0ByFQ96A4DgSPUm9wVWRLdm5qbmc/view\n","# !unzip vector_neologd.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e30PB3Lhe6pq","colab_type":"text"},"source":["## install MeCab"]},{"cell_type":"code","metadata":{"id":"dTShKiOPl-1T","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1601442563570,"user_tz":-540,"elapsed":130035,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"2920b2b4-b253-4384-e2c4-7fe8d9e508e4"},"source":["# install MeCab\n","!apt-get -q -y install sudo file mecab libmecab-dev mecab-ipadic-utf8 git curl python-mecab > /dev/null\n","!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git > /dev/null \n","!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n > /dev/null 2>&1\n","!pip install mecab-python3 > /dev/null\n","!pip install unidic-lite > /dev/null\n","! export MECABRC=\"/etc/mecabrc\"\n","!pip install tensorflow_text > /dev/null\n","!pip install jaconv > /dev/null\n","###################ginza\n","# !pip install ginza > /dev/null\n","# !pip install git+https://github.com/boudinfl/pke.git > /dev/null\n","# !python -m nltk.downloader stopwords > /dev/null\n","# !apt install aptitude swig > /dev/null"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Cloning into 'mecab-ipadic-neologd'...\n","remote: Enumerating objects: 75, done.\u001b[K\n","remote: Counting objects: 100% (75/75), done.\u001b[K\n","remote: Compressing objects: 100% (74/74), done.\u001b[K\n","remote: Total 75 (delta 5), reused 54 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (75/75), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"or784F6MRfRD","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1601268041811,"user_tz":-540,"elapsed":3737,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}}},"source":["!pip install tensorflow_text==2.2.0 > /dev/null\n","\n","!pip install tf_sentencepiece > /dev/null"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wwMfVybvpHPd","colab_type":"text"},"source":["##import"]},{"cell_type":"code","metadata":{"id":"jZgIBpbGpJQF","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1601442707756,"user_tz":-540,"elapsed":54625,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}}},"source":["import MeCab  as mc\n","import subprocess\n","import json\n","import pandas as pd\n","import tensorflow_hub as hub\n","import numpy as np\n","import tensorflow_text\n","import readline\n","from gensim.models import word2vec\n","import MeCab\n","import re\n","import warnings\n","from sklearn import preprocessing\n","# import pke\n","# pke.base.ISO_to_language['ja_ginza'] = 'japanese'\n","# import ginza\n","# import nltk\n","import gensim\n","# stopwords = list(ginza.STOP_WORDS)\n","# nltk.corpus.stopwords.words_org = nltk.corpus.stopwords.words\n","# nltk.corpus.stopwords.words = lambda lang : stopwords if lang == 'japanese' else nltk.corpus.stopwords.words_org(lang)\n","cmd='echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"'\n","path = (subprocess.Popen(cmd, stdout=subprocess.PIPE,shell=True).communicate()[0]).decode('utf-8')\n","m=mc.Tagger(\"-d {0}\".format(path))\n","warnings.simplefilter('ignore')\n","import tensorflow_hub as hub\n","embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")\n","pathout='/content/mount/My Drive/00_work/INES_guidance/w2vmodel/ja-gensim.50d.data.model'\n","model_w2v50 = word2vec.Word2Vec.load(pathout) # path of w2v model\n","pathword2vec='/content/mount/My Drive/00_work/fastText_w2v/model.vec'\n","model_w2v300 = gensim.models.KeyedVectors.load_word2vec_format(pathword2vec, binary=False)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"rJh47eJmtyJ2","colab_type":"code","colab":{}},"source":["# %cd '/content/mount/My Drive/00_work'\n","# !unzip guidance_org.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2iHoO0HprKsO","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1601022203433,"user_tz":-540,"elapsed":9670,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}}},"source":["modpath='/content/mount/My Drive/00_work/INES_guidance/w2vmodel/ja-gensim.50d.data.model'\n","sw=synonymswords(modpath)"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"tDK_QbJMdZqO","colab_type":"code","colab":{}},"source":["plst=['放置','単語','場合']\n","nlst=['用法','単語の場合']\n","n=10\n","sw.getsynonyms_wdlist(pwdlst=plst,th=0.8,topn=n)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hdLIpms594JC","colab_type":"text"},"source":["## word2vec import"]},{"cell_type":"code","metadata":{"id":"atND4Tz9oEio","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1601266679423,"user_tz":-540,"elapsed":16510,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}}},"source":["from gensim.models import word2vec\n","import warnings\n","warnings.simplefilter('ignore')\n","pathout='/content/mount/My Drive/00_work/INES_guidance/w2vmodel/ja-gensim.50d.data.model'\n","model_w2v50 = word2vec.Word2Vec.load(pathout) # path of w2v model\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"WctVBhZc9tVg","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1601267355568,"user_tz":-540,"elapsed":6,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}}},"source":["import gensim\n","pathword2vec='/content/mount/My Drive/00_work/fastText_w2v/model.vec'\n","model_w2v300 = gensim.models.KeyedVectors.load_word2vec_format(pathword2vec, binary=False)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"16_1qgm8_nI9","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1601268117248,"user_tz":-540,"elapsed":6,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}}},"source":["import tensorflow_hub as hub\n","import numpy as np\n","import tensorflow_text\n","embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y-K_0Vzw8LzB","colab_type":"code","colab":{}},"source":["model_w2v300.most_similar(positive=['知的','発達','影響'],topn=20)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y-oi_NSiGtPs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1601274445557,"user_tz":-540,"elapsed":659,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"ca5d1d97-46c3-4495-e05c-d2a2d5f19d59"},"source":["words_txt = \"同情引く\"\n","print(m.parse(words_txt))"],"execution_count":97,"outputs":[{"output_type":"stream","text":["同情\t名詞,サ変接続,*,*,*,*,同情,ドウジョウ,ドージョー\n","引く\t動詞,自立,*,*,五段・カ行イ音便,基本形,引く,ヒク,ヒク\n","EOS\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0t6y5VeEIPWS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"status":"ok","timestamp":1601276065046,"user_tz":-540,"elapsed":831,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"9e934230-5953-4964-8b64-81d3003f9d96"},"source":["searchstr='ミュンヒハウゼン症候群'\n","model_w2v50.most_similar(positive=[searchstr],topn=10)"],"execution_count":99,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('糖尿病性ニューロパチー', 0.9011921882629395),\n"," ('結節性硬化症', 0.8835355043411255),\n"," ('代理ミュンヒハウゼン症候群', 0.8818574547767639),\n"," ('傍腫瘍性神経症候群', 0.879774808883667),\n"," ('hypp', 0.8777968883514404),\n"," ('トルーソー', 0.8757524490356445),\n"," ('レベチラセタム', 0.873195469379425),\n"," ('ヘモクロマトーシス', 0.8724767565727234),\n"," ('イフェクサー', 0.8678131103515625),\n"," ('ストロイスラー', 0.8668649196624756)]"]},"metadata":{"tags":[]},"execution_count":99}]},{"cell_type":"code","metadata":{"id":"ilpkR2V5BoMF","colab_type":"code","colab":{}},"source":["from sklearn.metrics.pairwise import cosine_similarity\n","searchstr='硬膜下血腫'\n","searchstr='虚偽'\n","words='知的発達面への影響 低い自己評価'\n","# sims=model_w2v300.most_similar(positive=['知的','発達','影響'],topn=100)\n","sims=model_w2v50.most_similar(positive=[searchstr],negative=[],topn=1000)\n","cos_sim=cosine_similarity(embed(np.array(sims)[:,0]),embed([searchstr]))\n","sims_exp=np.expand_dims(np.array(sims)[:,0],axis=1)\n","use_sim=np.concatenate([cos_sim,sims_exp],axis=1)\n","sorted(use_sim,key=lambda x:x[0], reverse=True)[0:50]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ynf6bb2bfeqr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1601276209707,"user_tz":-540,"elapsed":702,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"98040a9d-3f20-4c0f-9aa0-555d4cc82b82"},"source":["cosine_similarity(embed(['代理によるミュンヒハウゼン症候群']),embed(['虚偽や症状の捏造']))"],"execution_count":104,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.36656892]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":104}]},{"cell_type":"markdown","metadata":{"id":"RPr4ylZC_Exr","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"f8shyQ8B_Dak","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1601268494119,"user_tz":-540,"elapsed":642,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"10a5942b-064f-47a9-a08f-7d984c764af7"},"source":["sims_exp.shape"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(20, 1)"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"AyAqpNGcoQkX","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1601021211993,"user_tz":-540,"elapsed":1145,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}}},"source":["simlst=model_w2v50.most_similar(positive=['搾取'],negative=[],topn=10)\n","th=0.85\n","# retsim=[]\n","# for sim in simlst:\n","#   if sim[1]"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZxNEPzsITmgd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1601021358803,"user_tz":-540,"elapsed":1017,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"316edcf4-adf0-45e8-8c97-7b60188e895a"},"source":["th=0.8\n","list(filter(lambda x: x[1] > th, simlst))"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('抑圧', 0.8276890516281128),\n"," ('収奪', 0.8263579607009888),\n"," ('助長', 0.8150772452354431),\n"," ('差別', 0.8112378120422363),\n"," ('腐敗', 0.8038375377655029),\n"," ('排除', 0.8019343614578247)]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"5wG0YXip3PqC","colab_type":"code","colab":{}},"source":["model_w2v50.most_similar(positive=['性的搾取'],topn=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F97E01HY3U-Q","colab_type":"code","colab":{}},"source":["sample_txt = \"発言し\"\n","print(\"Mecab:\\n\", m.parse(sample_txt))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oIwISH-o30oM","colab_type":"code","colab":{}},"source":["# print(m_cab2.parse('身体的影響'))\n","print(m.parse('母は昼から男のところへ遊びにいき、私は朝まで放置されました。'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WsbbCA2mIr9E","colab_type":"code","colab":{}},"source":["words=['ネグレクト','置き去り','無視','放置','怠る']\n","remark1=['顔','腫れ上がり','額','ケロイド','出来','怠る']\n","remark2=['母','男','遊び','いき','放置']\n","\n","from sklearn.metrics.pairwise import cosine_similarity\n","cosine_similarity(embed(remark1),embed(words))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kUDNAXk7LZti","colab_type":"code","colab":{}},"source":["cosine_similarity(model_w2v50.wv[remark1],model_w2v50.wv[words])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NELV5Ef31WNK","colab_type":"code","colab":{}},"source":["words=['痴愚','知的障害者','知能障害','魯鈍']\n","cosine_similarity(embed(words),embed(words))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ppg9qR9q1y9j","colab_type":"code","colab":{}},"source":["cosine_similarity(model_w2v50.wv[words],model_w2v50.wv[words])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L9es1EAS8tNh","colab_type":"code","colab":{}},"source":["words=['知的発達面への影響','ちてきしょうがい','痴愚','知的発達遅滞','知的発達障がい','知的発達障害','知的発達障害者','知的発達障碍','知的発達障礙','知的障害児','知的障害者','知的障碍','知的障碍者','知的障礙','知的障礙者','知能障害','知障者','精神発達遅滞','精神薄弱児','精薄児','精薄者','魯鈍']\n","remark1=['知的発達面への影響','ちてきしょうがい','痴愚','知的発達遅滞','知的発達障がい','知的発達障害','知的発達障害者','知的発達障碍','知的発達障礙','知的障害児','知的障害者','知的障碍','知的障碍者','知的障礙','知的障礙者','知能障害','知障者','精神発達遅滞','精神薄弱児','精薄児','精薄者','魯鈍']\n","# remark2=['母','男','遊び','いき','放置']\n","from sklearn.metrics.pairwise import cosine_similarity\n","cosine_similarity(embed(words),embed(remark1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vxf6z0p9cLdX","colab_type":"code","colab":{}},"source":["words=['知的発達面への影響','ちてきしょうがい','痴愚','知的発達遅滞','知的発達障がい','知的発達障害','知的発達障害者','知的発達障碍','知的発達障礙','知的障害児','知的障害者','知的障碍','知的障碍者','知的障礙','知的障礙者','知能障害','知障者','精神発達遅滞','精神薄弱児','精薄児','精薄者','魯鈍']\n","for w in words:\n","  if w in model_w2v50.wv:\n","    print(w,\",〇\")\n","  else:\n","    print(w,\",×\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JbiA218fcAFj","colab_type":"code","colab":{}},"source":["words=['知的発達面への影響','ちてきしょうがい','痴愚','知的発達遅滞','知的発達障がい','知的発達障害','知的発達障害者','知的発達障碍','知的発達障礙','知的障害児','知的障害者','知的障碍','知的障碍者','知的障礙','知的障礙者','知能障害','知障者','精神発達遅滞','精神薄弱児','精薄児','精薄者','魯鈍']\n","# remark2=['母','男','遊び','いき','放置']\n","from sklearn.metrics.pairwise import cosine_similarity\n","cosine_similarity(embed(words),embed(words))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jgBxLm4fWLQs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600334980171,"user_tz":-540,"elapsed":1505,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"9b264031-8c48-4655-c075-00317e12ce08"},"source":["remark2=['痴愚','知的障害者','知能障害','魯鈍']\n","from sklearn.metrics.pairwise import cosine_similarity\n","cosine_similarity(embed(words),embed(words))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.58717793"]},"metadata":{"tags":[]},"execution_count":134}]},{"cell_type":"code","metadata":{"id":"1oDf4zyxxsmW","colab_type":"code","colab":{}},"source":["cosine_similarity(model_w2v50.wv[words],model_w2v50.wv[words])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3GeAacdhYKP6","colab_type":"code","colab":{}},"source":["sim_a=[]\n","for w in words:\n","  sim_l=[]\n","  for r in remark2:\n","    sim=w2v.similarity(w,r)\n","    sim_l.append(sim)\n","  sim_a.append(sim_l)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t0cX5SdnYlZr","colab_type":"code","colab":{}},"source":["model_w2v50.most_similar(positive=['性的搾取'],topn=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xu80BdUYy0oe","colab_type":"code","colab":{}},"source":["model_w2v50.most_similar_cosmul(['子供','身体','影響','虐待'],topn=50)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TEYmbsGglg-A","colab_type":"code","colab":{}},"source":["model_w2v50.most_similar(positive=['知的障害者'],topn=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OMs6iz1Sdvwk","colab_type":"code","colab":{}},"source":["import pprint\n","pprint.pprint(model_w2v.most_similar(positive=['女', '国王'], negative=['男']))\n","# pprint.pprint(model_w2v.similarity('国王', '王妃'))\n","# # => 0.74155587641044496\n","# pprint.pprint(model_w2v.similarity('国王', 'ラーメン'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-EWLGRzho3wm","colab_type":"code","colab":{}},"source":["model_w2v.most_similar('中学1年生')#,negative=['身体'])) #'ネグレクト','無視','放置','怠る'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ntkMJPyBY9Iw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":171},"executionInfo":{"status":"error","timestamp":1600999657175,"user_tz":-540,"elapsed":1630,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"dcf07d13-cc84-4263-dda0-1fea3e186793"},"source":["print(model_w2v.most_similar(positive=['影響','身体','虐待']))#,negative=['身体'])) #'ネグレクト','無視','放置','怠る'"],"execution_count":6,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-cb8640dcf626>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_w2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'影響'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'身体'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'虐待'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#,negative=['身体'])) #'ネグレクト','無視','放置','怠る'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'model_w2v' is not defined"]}]},{"cell_type":"code","metadata":{"id":"eOiJ3sNqhDM5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600320730613,"user_tz":-540,"elapsed":1112,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"b16c5008-d0f0-4842-bb07-b92647a2542e"},"source":["model_w2v.get_vector('身体的虐待').shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(300,)"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"ML29Q8znrRqj","colab_type":"code","colab":{}},"source":["model_w2v.most_similar('身体的虐待')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6m8UMeWoria_","colab_type":"code","colab":{}},"source":["!git clone https://github.com/philipperemy/japanese-word-to-vectors.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n1x9lE4PrnN-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600322965947,"user_tz":-540,"elapsed":1402,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"d1b6523f-823c-4d45-85c2-d18e1bdbe3f8"},"source":["%cd japanese-word-to-vectors"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/mount/My Drive/00_work/word2vec/japanese-word-to-vectors\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fys-xbOurtUw","colab_type":"code","colab":{}},"source":["!pip3 install -r requirements.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zdO73__Jw2jR","colab_type":"code","colab":{}},"source":["!bzip2 -d  jawiki_20180420_100d.pkl.bz2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IymjXPO-r1WO","colab_type":"code","colab":{}},"source":["!wget https://dumps.wikimedia.org/jawiki/latest/jawiki-latest-pages-articles.xml.bz2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5HkpV-wWr8f4","colab_type":"code","colab":{}},"source":["!python3 generate_vectors.py "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZrJFWZCesAME","colab_type":"code","colab":{}},"source":["!python3 generate_vectors.py --mecab"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dE-OU9iLw4E6","colab_type":"code","colab":{}},"source":["!wget http://wikipedia2vec.s3.amazonaws.com/models/ja/2018-04-20/jawiki_20180420_100d.pkl.bz2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WfseeDM1zdCf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":230},"executionInfo":{"status":"ok","timestamp":1600325022695,"user_tz":-540,"elapsed":5306,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"37ea8dcd-01e8-426e-98b8-9f80bbacea63"},"source":["!pip install chakin"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting chakin\n","  Downloading https://files.pythonhosted.org/packages/ca/3f/ca2f63451c0ab47970a6ab1d39d96118e70b6e73125529cea767c31368a3/chakin-0.0.8-py3-none-any.whl\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from chakin) (1.15.0)\n","Requirement already satisfied: pandas>=0.20.1 in /usr/local/lib/python3.6/dist-packages (from chakin) (1.0.5)\n","Requirement already satisfied: progressbar2>=3.20.0 in /usr/local/lib/python3.6/dist-packages (from chakin) (3.38.0)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.1->chakin) (1.18.5)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.1->chakin) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.1->chakin) (2.8.1)\n","Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2>=3.20.0->chakin) (2.4.0)\n","Installing collected packages: chakin\n","Successfully installed chakin-0.0.8\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eG27b6Inzgzj","colab_type":"code","colab":{}},"source":["import chakin\n","chakin.search(lang='Japanese')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q5EUOoMBz0jJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600325325951,"user_tz":-540,"elapsed":4748,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"27654b8c-54da-40df-8856-8358a2b56ed4"},"source":["fnmode='/content/mount/My Drive/00_work/word2vec/word2vec.gensim.model'\n","# w2v = gensim.models.KeyedVectors.load_word2vec_format(fnmode, binary=False)\n","w2v = word2vec.Word2Vec.load(fnmode) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:gensim.models.base_any2vec:consider setting layer size to a multiple of 4 for greater performance\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"zcRHt8Q00sLX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"status":"ok","timestamp":1600331180160,"user_tz":-540,"elapsed":968,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"690af42f-e1bc-4420-cf42-08d3f66dd67b"},"source":["w2v.most_similar('身体的虐待')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('近親姦', 0.8238307237625122),\n"," ('性的虐待', 0.8128407597541809),\n"," ('ドメスティックバイオレンス', 0.8069393634796143),\n"," ('性犯罪', 0.8037077188491821),\n"," ('ネグレクト', 0.7936483025550842),\n"," ('児童虐待', 0.7804021239280701),\n"," ('人工妊娠中絶', 0.7800942063331604),\n"," ('医療過誤', 0.7763338685035706),\n"," ('精神疾患', 0.7758262157440186),\n"," ('心的外傷後ストレス障害', 0.7734633684158325)]"]},"metadata":{"tags":[]},"execution_count":108}]},{"cell_type":"code","metadata":{"id":"WRAD4Rv8zqvz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1600325089423,"user_tz":-540,"elapsed":3000,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"92d5222a-4b25-4049-b8a0-6b535b53f27f"},"source":["chakin.download(number=22, save_dir='./') # select fastText(en)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test: 100% ||                                      | Time:  0:00:02  60.7 MiB/s\n"],"name":"stderr"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'./latest-ja-word2vec-gensim-model.zip'"]},"metadata":{"tags":[]},"execution_count":71}]},{"cell_type":"code","metadata":{"id":"3OjZtIwmQ4ai","colab_type":"code","colab":{}},"source":["import tensorflow_hub as hub\n","import tensorflow_text\n","embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IAIvUoRbBYXw","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","fn='/content/mount/My Drive/00_work/USE'\n","model_1=tf.keras.models.load_model(fn)\n","# model_1 = tf.saved_model.load(fn)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZyF5qfeSFAiq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"6e4cd83b-4583-49e2-dc74-0816d2e6b8e4"},"source":["import tensorflow_text as text\n","tokenizer = text.UnicodeScriptTokenizer ()\n","tokens = tokenizer.tokenize(['everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n","print(tokens.to_list())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[b'everything', b'not', b'saved', b'will', b'be', b'lost', b'.'], [b'Sad', b'\\xe2\\x98\\xb9']]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZPM10GjcG1H7","colab_type":"code","colab":{}},"source":["embed.trainable_variables"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R0u2o29yCNch","colab_type":"code","colab":{}},"source":["!wget 'https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3?tf-hub-format=compressed' universal-sentence-encoder-multilingual-large_3.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UjK0fq7Vczft","colab_type":"text"},"source":["# 環境構築 USE"]},{"cell_type":"code","metadata":{"id":"KHU-_hSw7Rr5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1600056867611,"user_tz":-540,"elapsed":119835,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"bb59a250-9500-4175-be70-017875055c08"},"source":["!pip install tensorflow==2.2.0 > /dev/null\n","!pip3 install --quiet seaborn > /dev/null\n","!pip install sentencepiece > /dev/null\n","!pip install tf_sentencepiece > /dev/null\n","!pip install tensorflow_text==2.2.0 > /dev/null\n","!pip install jaconv > /dev/null"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/tensorboard/\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lCi7C_6zdzmY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1600056933437,"user_tz":-540,"elapsed":177449,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"3650799c-bb02-412a-cf69-198275bee593"},"source":["import tensorflow.compat.v1 as tf\n","import tensorflow_hub as hub\n","import numpy as np\n","import tensorflow_text\n","import tf_sentencepiece\n","from sklearn.metrics.pairwise import cosine_similarity\n","embed_ml3 = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")\n","embed_mqa = hub.load('https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3')\n","# Set up graph.\n","g = tf.Graph()\n","with g.as_default():\n","  xling_8_embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-xling-many/1\")\n","  text_input = tf.placeholder(dtype=tf.string, shape=[None])\n","  embedded_text = xling_8_embed(text_input)  \n","  init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n","g.finalize()\n","# Initialize session.\n","session = tf.Session(graph=g)\n","session.run(init_op)\n","\n","def embed_xm1_single(str_input):\n","  ja_result = session.run(embedded_text, feed_dict={text_input: [str_input]})\n","  return ja_result\n","def embed_xm1(strList):\n","  ja_result = session.run(embedded_text, feed_dict={text_input: strList})\n","  return ja_result\n","\n","def embed_qa_q(q):  \n","  question_embeddings = embed_mqa.signatures['question_encoder'](\n","              tf.constant(q))\n","  return question_embeddings['outputs']\n","def embed_qa_r(r,rc):  \n","  response_embeddings = embed_mqa.signatures['response_encoder'](\n","          input=tf.constant(r),\n","          context=tf.constant(rc))\n","  return response_embeddings['outputs']\n","\n","def embed_qa_sim(q,r,rc):  \n","  question_embeddings = embed_mqa.signatures['question_encoder'](\n","              tf.constant(q))\n","  response_embeddings = embed_mqa.signatures['response_encoder'](\n","          input=tf.constant(r),\n","          context=tf.constant(rc))\n","  res=cosine_similarity(question_embeddings['outputs'], response_embeddings['outputs'])\n","  return res\n","def embed_qa_sim_a(q,r,rc):  \n","  question_embeddings1= embed_mqa.signatures['question_encoder'](\n","                tf.constant(q[0:3000]))\n","  question_embeddings2= embed_mqa.signatures['question_encoder'](\n","                tf.constant(q[3000:]))\n","  question_embeddings=np.concatenate([question_embeddings1['outputs'],question_embeddings2['outputs']],axis=0)\n","\n","  response_embeddings = embed_mqa.signatures['response_encoder'](\n","            input=tf.constant(r),\n","            context=tf.constant(rc))\n","\n","  res=cosine_similarity( response_embeddings['outputs'],question_embeddings['outputs'])\n","  return res\n","def embed_qa_sim_b(q,r,rc):  \n","  question_embeddings1= embed_mqa.signatures['question_encoder'](\n","                tf.constant(q[0:3000]))\n","  question_embeddings2= embed_mqa.signatures['question_encoder'](\n","                tf.constant(q[3000:]))\n","  question_embeddings=np.concatenate([question_embeddings1['outputs'],question_embeddings2['outputs']],axis=0)\n","\n","  response_embeddings = embed_mqa.signatures['response_encoder'](\n","            input=tf.constant(r),\n","            context=tf.constant(rc))\n","\n","  res=cosine_similarity( question_embeddings['outputs'],response_embeddings['outputs'])\n","  return res"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"Rzi5TZcOXaDQ","colab_type":"text"},"source":["#新ガイダンスの類語追加方法"]},{"cell_type":"markdown","metadata":{"id":"sVSvcmyhXke_","colab_type":"text"},"source":["## src synonym words"]},{"cell_type":"code","metadata":{"id":"gay1HKD0eNW6","colab_type":"code","colab":{}},"source":["from gensim.models import word2vec\n","import warnings\n","warnings.simplefilter('ignore')\n","class synonymswords():\n","    def __init__(self,modpath):\n","      self.w2v = word2vec.Word2Vec.load(modpath) \n","    def getsynonyms(self, wd,th=0.6,topn=10):\n","      retsim=[]\n","      in_voc_flg=False\n","      if wd in self.w2v.wv:\n","        in_voc_flg=True\n","        simlst=self.w2v.wv.most_similar(positive=[wd],topn=topn)\n","        retsim=list(filter(lambda x: x[1] > th, simlst))\n","      else:\n","        print(\"word %s not in vocabulary\" % (wd))\n","      if in_voc_flg and len(retsim)==0:\n","        print(\"閾値の値を下げてください\")\n","      return retsim\n","    def getsynonyms_wdlist(self, pwdlst,nwdlst=[],th=0.6,topn=10):\n","      retsim=[]\n","      wdln=[]\n","      nwdln=[]\n","      wdlnotexist=[]\n","      in_voc_flg=False\n","      for wd in pwdlst:\n","        if wd in self.w2v.wv:\n","          wdln.append(wd)\n","        else:\n","          wdlnotexist.append(wd)\n","      for wd in nwdlst:\n","        if wd in self.w2v.wv:\n","          nwdln.append(wd)\n","        else:\n","          wdlnotexist.append(wd)\n","      if len(wdlnotexist)>0:\n","          print(\"word %s not in vocabulary\" % (\",\".join(map(str, wdlnotexist))))\n","      if len(wdln)>0:\n","        in_voc_flg=True\n","        simlst=self.w2v.wv.most_similar(positive=wdln,negative=nwdln,topn=topn)\n","        retsim=list(filter(lambda x: x[1] > th, simlst))\n","      if in_voc_flg and len(retsim)==0:\n","        print(\"閾値の値を下げてください\")\n","      return retsim\n","      \n","\n","#modpath='00_work/INES_guidance/w2vmodel/ja-gensim.50d.data.model'\n","#sw=synonymswords(modpath)\n","#sw.getsynonyms('放置',topn=10)\n","#sw.getsynonyms_wdlist(['放置','da放置'],topn=10)\n","\n","'''\n","クラスの使い方\n","１．クラスを作成\n","  クラス名：synonymswords\n","  引数modpath：word2vecモデルのフルパス\n","  サンプル：\n","  modpath='00_work/INES_guidance/w2vmodel/ja-gensim.50d.data.model'\n","  sw=synonymswords(modpath)\n","２．ワードの類語を取得\n","   関数：getsynonyms(self, wd,th=0.6,topn=10):\n","   引数：wd：単語\n","         th：閾値\n","         topn：取得予定の類語個数(入力しなし場合。10です) \n","   サンプル：\n","   wd='放置'\n","   n=10\n","   synonymsList=sw.getsynonyms(wd,topn=n)\n","3．ワードリストの類語を取得\n","   関数：getsynonyms_wdlist(self, pwdlst,nwdlst=[],th=0.6,topn=10):\n","   引数：pwdlst：positive単語リスト\n","                サンプル：['類語','単語','場合']\n","         nwdlst :negative単語(入力しなし場合。空きとします) \n","         th：閾値\n","         topn：取得予定の類語個数(入力しなし場合。10です) \n","   サンプル：\n","   plst=['放置','単語','場合']\n","   nlst=['用法','単語の場合']\n","   n=10\n","   th=0.7\n","   sw.getsynonyms_wdlist(pwdlst=plst,nwdlst=nlst,th=th,topn=n)\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d8qmjJfPX7AL","colab_type":"text"},"source":["## mecab_w2v_use"]},{"cell_type":"code","metadata":{"id":"C6Y6u970YyQf","colab_type":"code","colab":{}},"source":["import MeCab  as mc\n","import subprocess\n","import pandas as pd\n","import tensorflow_hub as hub\n","import numpy as np\n","import tensorflow_text\n","from gensim.models import word2vec\n","import re\n","import gensim\n","import warnings\n","from sklearn.metrics.pairwise import cosine_similarity\n","warnings.simplefilter('ignore')\n","cmd='echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"'\n","path = (subprocess.Popen(cmd, stdout=subprocess.PIPE,shell=True).communicate()[0]).decode('utf-8')\n","m=mc.Tagger(\"-d {0}\".format(path))\n","embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")\n","pathout='/content/mount/My Drive/00_work/INES_guidance/w2vmodel/ja-gensim.50d.data.model'\n","model_w2v50 = word2vec.Word2Vec.load(pathout) # path of w2v model\n","pathword2vec='/content/mount/My Drive/00_work/fastText_w2v/model.vec'\n","model_w2v300 = gensim.models.KeyedVectors.load_word2vec_format(pathword2vec, binary=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XjQTB2wVbTn7","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1601443235679,"user_tz":-540,"elapsed":1068,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}}},"source":["model_w2v=model_w2v50\n","# model_w2v=model_w2v300\n","m=mc.Tagger(\"-d {0}\".format(path))"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"AanqdSMxYA48","colab_type":"code","colab":{}},"source":["import re\n","import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","word_filter=[\n","['動詞','自立'],\n","['名詞','サ変接続'],\n","['名詞','ナイ形容詞語幹'],\n","['名詞','一般'],\n","['名詞','固有名詞'],\n","['名詞','形容動詞語幹'],\n","['形容詞','自立']]\n","word_filter=[\n","['接頭詞'],\n","['動詞'],\n","['名詞'],\n","['形容詞']]\n","def words_filter(lines):\n","  main_words=[]\n","  for line in lines:\n","     items=re.split('[\\t,]',line)\n","     if len(items) > 1:\n","        for f in word_filter:\n","          # if items[1]==f[0] and items[2]==f[1]:\n","          if items[1]==f[0]:\n","              main_words.append(items[0])\n","  return main_words\n","def getWordLst(words):\n","  w_mecb=m.parse(words)\n","  lines=w_mecb.split(sep='\\n')\n","  return words_filter(lines)\n","wordsLst=['代理によるミュンヒハウゼン症候群','症状捏造']\n","nu=10\n","nw=10\n","nwMax=1000\n","# for words in wordsLst:\n","\n","def getUsrWvTopn(words):\n","  wordsLstMe=getWordLst(words)\n","  sysUseTopnLst=[]\n","  sysW2vTopnLst=[]\n","  for word in wordsLstMe:\n","    synonymsWv=model_w2v.wv.most_similar(word,topn=nwMax)\n","    sysWords=np.array(synonymsWv)[:,0]\n","    simLst=cosine_similarity(embed(sysWords),embed([word]))\n","    sysWordsExpdim=np.expand_dims(sysWords,axis=1)\n","    sysUse=np.concatenate([sysWordsExpdim,simLst],axis=1)\n","    sysUseTopn=sorted(sysUse,key=lambda x:x[1], reverse=True)[0:nu]\n","    sysW2vTopn=sorted(synonymsWv,key=lambda x:x[1], reverse=True)[0:nw]\n","    sysW2vTopnLst.append(sysW2vTopn)\n","    sysUseTopnLst.append(sysUseTopn)\n","  return [wordsLstMe,sysW2vTopnLst,sysUseTopnLst]\n","\n","\n","wordsLstMe,sysW2vTopnLst,sysUseTopnLst=getUsrWvTopn(wordsLst[0])\n","def getWordsTopn(lstMe,topnLst):\n","  sysW2vArr=None\n","  for lst in topnLst:\n","    if sysW2vArr is None:\n","      sysW2vArr=lst\n","    else:\n","      sysW2vArr=np.concatenate([sysW2vArr,lst],axis=1)\n","  import pandas  as pd\n","  cols=[]\n","  for col in lstMe:\n","    cols.append(col)\n","    cols.append('確認度')\n","  return pd.DataFrame(sysW2vArr,columns=cols)\n","# getWordsTopn(wordsLstMe,sysW2vTopnLst)\n","# getWordsTopn(wordsLstMe,sysUseTopnLst)\n","# wordsLstMe,sysW2vTopnLst,sysUseTopnLst=getUsrWvTopn('代理ミュンヒハウゼン症候群')\n","# wordsLstMe,sysW2vTopnLst,sysUseTopnLst=getUsrWvTopn(wordsLst[0])\n","# print(\"word2vec\\n\",getWordsTopn(wordsLstMe,sysW2vTopnLst))\n","# print(\"use\\n\",getWordsTopn(wordsLstMe,sysUseTopnLst))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mXmF0-dCcNm9","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1601451732367,"user_tz":-540,"elapsed":6703,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}}},"source":["wordsLst=['身体的虐待','ネグレクト','性的虐待','心理的虐待','身体的影響','知的発達面への影響',\n","          '対人関係の障害','低い自己評価','行動コントロールの問題','多動','心的外傷後ストレス障害',\n","          '偽成熟性',\n","          # '精神的病状',\n","          '硬膜下血腫',\n","          # '乳幼児揺さぶられ症候群',\n","          '代理によるミュンヒハウゼン症候群',\n","          '子ども虐待の発生要因']\n","for word in wordsLst:\n","  wordsLstMe,sysW2vTopnLst,sysUseTopnLst=getUsrWvTopn(word)\n","  df_w=getWordsTopn(wordsLstMe,sysW2vTopnLst)\n","  df_u=getWordsTopn(wordsLstMe,sysUseTopnLst)\n","  fd='/content/mount/My Drive/00_work/guidance/002_output_dt/me_w2v_use/'\n","  df_w.to_csv(fd+word+\"_w.csv\",index=None)\n","  df_u.to_csv(fd+word+\"_u.csv\",index=None)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"z-WL84JZbHEL","colab_type":"code","colab":{}},"source":["wordsLstMe,sysW2vTopnLst,sysUseTopnLst=getUsrWvTopn('知的発達面への影響')\n","print(\"word2vec\\n\",getWordsTopn(wordsLstMe,sysW2vTopnLst)\n","print(\"use\\n\",getWordsTopn(wordsLstMe,sysUseTopnLst))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lV81nMKAeoAg","colab_type":"code","colab":{}},"source":["# model_w2v300.wv['精神的病状']\n","text='ある程度に体が成長した児童では、多少揺すられた程度では、反射的に体をこわばらせるため、そう簡単に怪我をすることはないが、同じことを首が据わっておらず頭蓋骨も隙間の多い新生児で行うと、眼底出血や頭蓋内出血（クモ膜下出血など）・脳挫傷を伴う致命的な怪我を負わせかねない。また身体の組織が成長途上で柔らかく力も弱い幼児でも、過度に揺すられると、程度の差こそあれ問題となる場合もあるとみなされる。'\n","m=mc.Tagger(\"-d {0}\".format(path))\n","lines=m.parse(text)\n","for line in lines.split(sep='\\n'):\n","  print(line)    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P30ivri3iBH5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1601444541402,"user_tz":-540,"elapsed":980,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"15178c6d-3a49-4362-dd78-f7213ae5199b"},"source":["m.parse('偽成熟性')"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'偽\\t接頭詞,名詞接続,*,*,*,*,偽,ニセ,ニセ\\n成熟\\t名詞,サ変接続,*,*,*,*,成熟,セイジュク,セイジュク\\n性\\t名詞,接尾,一般,*,*,*,性,セイ,セイ\\nEOS\\n'"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"ZtDPrUjtxtzE","colab_type":"code","colab":{}},"source":["# !pip install html2text\n","!pip install bs4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MFRvP_7si79a","colab_type":"code","colab":{}},"source":["from urllib.request import urlopen\n","import html2text as h2t\n","from bs4 import BeautifulSoup\n","from urllib.parse import quote\n","import string\n","url='https://ja.wikipedia.org/wiki/揺さぶられっ子症候群'\n","# url='https://renso-ruigo.com/word/身体的虐待'\n","url = quote(url, safe=string.printable)\n","html = urlopen(url, timeout=20).read().decode('utf-8','ignore').replace(u'\\xa9', u'')\n","soup = BeautifulSoup(html)\n","for script in soup([\"script\", \"style\"]):\n","    script.decompose()\n","strips = list(soup.stripped_strings)\n","for text in strips:\n","  # lines=m.parse(text)\n","  # for line in lines.split(sep='\\n'):\n","  #   print(line)   \n","  print(text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_tWFMyc41aZt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1601432851167,"user_tz":-540,"elapsed":567,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"36c944db-0a32-4cf9-e888-d7d8b4cbb2ce"},"source":["'https://ja.wikipedia.org/wiki/揺さぶられっ子症候群'.encode('utf-8')\n","https://ja.wikipedia.org/wiki/%E6%8F%BA%E3%81%95%E3%81%B6%E3%82%89%E3%82%8C%E3%81%A3%E5%AD%90%E7%97%87%E5%80%99%E7%BE%A4"],"execution_count":84,"outputs":[{"output_type":"execute_result","data":{"text/plain":["b'https://ja.wikipedia.org/wiki/\\xe6\\x8f\\xba\\xe3\\x81\\x95\\xe3\\x81\\xb6\\xe3\\x82\\x89\\xe3\\x82\\x8c\\xe3\\x81\\xa3\\xe5\\xad\\x90\\xe7\\x97\\x87\\xe5\\x80\\x99\\xe7\\xbe\\xa4'"]},"metadata":{"tags":[]},"execution_count":84}]},{"cell_type":"code","metadata":{"id":"qbqktdi6tQC4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1601430765524,"user_tz":-540,"elapsed":1300,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"82627d69-8be6-428c-ca43-6195c7a24c18"},"source":["# !curl -o wikipedia_synonym.py https://raw.githubusercontent.com/ikegami-yukino/misc/master/data/wikipedia_synonym.py\n","# !python wikipedia_synonym.py\n","!wc -l wikipedia_synonym.txt"],"execution_count":43,"outputs":[{"output_type":"stream","text":["365585 wikipedia_synonym.txt\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QZxYVZ-69hk7","colab_type":"text"},"source":["#program USE"]},{"cell_type":"markdown","metadata":{"id":"0J2TYh6E3Smk","colab_type":"text"},"source":["## common function"]},{"cell_type":"code","metadata":{"id":"nI1sNRjr3XzJ","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","dtlst=[\n","       [['a',0.8],['b',0.7],['c',0.6],['d',0.3]],\n","       [['b',0.9],['e',0.5],['c',0.4],['a',0.3]],\n","       [['a',0.6],['e',0.3],['f',0.2],['g',0.1]],\n","]\n","dtlst=[\n","       [[0,0.8],[1,0.7],[2,0.6],[3,0.3]],\n","       [[1,0.9],[4,0.5],[2,0.4],[0,0.3]],\n","       [[0,0.6],[4,0.3],[5,0.2],[6,0.1]],\n","]\n","def get_topn_dt(dtlst,n):\n","  dtlstn=[]\n","  for i, dt in enumerate(dtlst):\n","    for dtj in dt:\n","      dtlstn.append(dtj+[i])\n","  df=pd.DataFrame(dtlstn,columns=['docid','p','c'])\n","  dg=df.groupby(['docid'])\n","  dga=dg.agg({'p':'mean','c':'count'})\n","  dga.columns=['p','cnt']\n","  dga_n=dga.sort_values(by=['cnt','p'],ascending=False)\n","  \n","  words=np.array(dga_n.iloc[0:n,:].index)\n","  pm=np.array(dga_n.iloc[0:n,0])\n","  res=[]\n","  for w, p in zip(words,pm):\n","    res.append([w,p])\n","  return res\n","def pre_process_sen(sent):\n","  sps=['…','･･･','※','-']\n","  for s in sps:\n","    sent=sent.replace(s,'')\n","  result = re.sub(r'[・　。、･]+', '', sent)\n","  return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gHFjjY9Q7lDM","colab_type":"code","colab":{}},"source":["n=6\n","dtlstn=[]\n","for i, dt in enumerate(dtlst):\n","  for dtj in dt:\n","    dtlstn.append(dtj+[i])\n","df=pd.DataFrame(dtlstn,columns=['word','p','c'])\n","dg=df.groupby(['word'])\n","dga=dg.agg({'p':'mean','c':'count'})\n","dga.columns=['p','cnt']\n","dga_n=dga.sort_values(by=['cnt','p'],ascending=False)\n","\n","words=np.array(dga_n.iloc[0:n,:].index)\n","pm=np.array(dga_n.iloc[0:n,0])\n","res=[]\n","for w, p in zip(words,pm):\n","  res.append([w,p])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OJEAWOT1VzOw","colab_type":"code","colab":{}},"source":["dtlst_new=dtlst\n","# dtlst_new[0]=dtlst_new*2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mJecSjcRXQWW","colab_type":"code","colab":{}},"source":["dtarr_new=np.array(dtlst[0])\n","dtarr_new[:,1]=dtarr_new[:,1]*2\n","dtlst[0]=list(dtarr_new)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QNnAXXZeZZ2z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1597986801738,"user_tz":-540,"elapsed":1296,"user":{"displayName":"于勝龍","photoUrl":"","userId":"09684187088444140086"}},"outputId":"9b36df47-b847-43c9-8c42-61be459ac208"},"source":["dtlst"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[array([0. , 6.4]), array([1. , 5.6]), array([2. , 4.8]), array([3. , 2.4])],\n"," [[1, 0.9], [4, 0.5], [2, 0.4], [0, 0.3]],\n"," [[0, 0.6], [4, 0.3], [5, 0.2], [6, 0.1]]]"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"markdown","metadata":{"id":"vcTjp_AQhQzL","colab_type":"text"},"source":["## 000_use concat"]},{"cell_type":"code","metadata":{"id":"Ogz6GqcvhUWH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":158},"executionInfo":{"status":"ok","timestamp":1600059278057,"user_tz":-540,"elapsed":1218,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"c248ab3c-b26e-4383-9276-edb561f3c788"},"source":["import tensorflow_hub as hub\n","import tensorflow_text\n","import numpy as np\n","import pandas as pd\n","import json\n","import re\n","from  datetime import datetime as dt\n","from sklearn.metrics.pairwise import cosine_similarity\n","import csv\n","fn_s='/content/mount/My Drive/00_work/guidance/001_input_dt/n_valuation/word_synonyms.csv'\n","with open(fn_s) as f:\n","    reader = csv.reader(f)\n","    word_synonyms_lst = [row for row in reader]\n","col='all'\n","fn_g='/content/mount/My Drive/00_work/guidance/001_input_dt/n_valuation/new_guidance_word_all.json'\n","# fn_r='/content/mount/My Drive/00_work/guidance/001_input_dt/n_valuation/sentence_85.json'\n","fn_r='/content/mount/My Drive/00_work/guidance/001_input_dt/n_valuation/s_85.json'\n","fn_s='/content/mount/My Drive/00_work/guidance/001_input_dt/n_valuation/word_synonyms.csv'\n","json_remark = json.load(open(fn_r, \"r\"))\n","json_guidan = json.load(open(fn_g, \"r\"))\n","\n","results = []\n","remark_list=[]\n","guidan_list=[]\n","guidan_context_list=[]\n","\n","print('remark vec start',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","for remark in json_remark:\n","    clean_sen=pre_process_sen(remark[\"sentence\"])\n","    # clean_sen=remark[\"sentence\"]\n","    remark_list.append(clean_sen)\n","\n","print('remark vec end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","for title in json_guidan:\n","    clean_sen=pre_process_sen(title[col])\n","    # clean_sen=title[col]\n","    guidan_list.append(clean_sen)\n","    # print(clean_sen)\n","    guidan_context_list.append(''.join(map(str,list(reversed(clean_sen)))))\n","    # guidan_context_list.append(''.join(map(str,list(reversed(title['word'])))))\n","    # guidan_context_list.append(clean_sen)\n","print('guidance vec end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","\n","\n","print('sim vector create start',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","qal_vec_r=embed_qa_q(remark_list)\n","qal_vec_g=embed_qa_r(guidan_list,guidan_context_list)\n","ml3_vec_r=embed_ml3(remark_list)\n","ml3_vec_g=embed_ml3(guidan_list)\n","xml_vec_r=embed_xm1(remark_list)\n","xml_vec_g=embed_xm1(guidan_list)\n","print('sim vector create end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","\n","exc='qa'\n","sim_rg_matrix_ml3=cosine_similarity(ml3_vec_r, ml3_vec_g)\n","sim_rg_matrix_xm1=cosine_similarity(xml_vec_r,xml_vec_g)\n","sim_rg_matrix_qa3=cosine_similarity( qal_vec_r,qal_vec_g)\n","sim_rg_matrix=sim_rg_matrix_ml3+sim_rg_matrix_qa3+sim_rg_matrix_xm1\n","sim_rg_matrix=np.divide(sim_rg_matrix,3)\n","def sim_rg_matrix_cmp(v11,v12,v21,v22):\n","  v1=np.concatenate([v11,v12],axis=1)\n","  v2=np.concatenate([v21,v22],axis=1)\n","  # print('v1.shape:',v1.shape)\n","  return cosine_similarity(v1,v2)\n","print('sim caculate end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","sim_dic={\n","'mqa3_xm1':sim_rg_matrix_cmp(qal_vec_r,xml_vec_r,qal_vec_g,xml_vec_g),\n","'mqa3_ml3':sim_rg_matrix_cmp(qal_vec_r,ml3_vec_r,qal_vec_g,ml3_vec_g),\n","'xm1_ml3':sim_rg_matrix_cmp(xml_vec_r,ml3_vec_r,xml_vec_g,ml3_vec_g),\n","'xml':cosine_similarity(xml_vec_r,xml_vec_g),\n","'ml3':cosine_similarity(ml3_vec_r,ml3_vec_g),\n","'qa':cosine_similarity(qal_vec_r,qal_vec_g),\n","}\n","sim_rg_matrix=sim_dic[exc]\n","print('sim sim computer end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","# sim_rg_matrix=np.squeeze(sim_rg_matrix_ml3)\n","for i,remark in enumerate(json_remark):\n","    sim_rg_list=[]\n","    r_doc=remark['sentence']\n","    #word_synonyms_lst\n","    for j, g_doc in enumerate(json_guidan):\n","      keymatch_flg=True\n","      # sim=sim_rg_matrix[i,j]\n","      # sim_rg_list.append([j,sim])\n","\n","      #$$$$ keyworkd match\n","      # keywd_match=word_synonyms_lst[j]\n","      # for w in keywd_match:\n","      #   if re.search(w, r_doc) is not None:\n","      #     keymatch_flg=False\n","      #     sim_rg_list.append([j,1.0])\n","      #     break\n","      #$$$$ keyworkd match\n","\n","      if keymatch_flg:\n","        sim=sim_rg_matrix[i,j]\n","        sim_rg_list.append([j,sim])\n","    sim_rg_list_sort=sorted(sim_rg_list,key=lambda x:x[1], reverse=True)\n","    for sim_rg in sim_rg_list_sort[0:3]:\n","      doc_id=sim_rg[0]\n","      sim_mean=sim_rg[1]\n","      gs_w=json_guidan[doc_id]['word']\n","      gs_s=json_guidan[doc_id][col]\n","      results.append([r_doc,gs_w,gs_s,sim_mean])\n","df_out=pd.DataFrame(results,columns=['sentence','word',col,'similarity'])\n","outdir='/content/mount/My Drive/00_work/guidance/002_output_dt/'\n","df_out.to_csv(outdir+exc+\"_guidance_remark_similarity.csv\",index=None)\n","print('finish out',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["remark vec start 20/09/14 04:54:36\n","remark vec end 20/09/14 04:54:36\n","guidance vec end 20/09/14 04:54:36\n","sim vector create start 20/09/14 04:54:36\n","sim vector create end 20/09/14 04:54:36\n","sim caculate end 20/09/14 04:54:36\n","sim sim computer end 20/09/14 04:54:36\n","finish out 20/09/14 04:54:36\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wjTcXOMJ2oFd","colab_type":"code","colab":{}},"source":["title"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PoKVavKLU4fS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599791171748,"user_tz":-540,"elapsed":655,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"f3fb8361-0604-4dad-f35f-34d6a12ba2e1"},"source":["outdir='/content/mount/My Drive/00_work/guidance/002_output_dt/'\n","df_out.to_csv(outdir+exc+\"_guidance_remark_similarity.csv\",index=None)\n","print('finish out',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["finish out 20/09/11 02:26:11\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7aySTsvd3KQ9","colab_type":"text"},"source":["## 0001_野田さんサンプル"]},{"cell_type":"code","metadata":{"id":"ebNh6RVr9oPR","colab_type":"code","colab":{}},"source":["import tensorflow_hub as hub\n","import tensorflow_text\n","import numpy as np\n","import pandas as pd\n","import json\n","basedir='/content/mount/My Drive/00_work/guidance/20200701/05_all/'\n","json_remark = json.load(open(basedir+\"sentence.json\", \"r\"))\n","json_title = json.load(open(basedir+\"/word_all.json\", \"r\"))\n","\n","results = []\n","remarks = []\n","for remark in json_remark:\n","    remark_vec = embed(remark[\"sentence\"])\n","    for title in json_title:\n","        title_vec = embed(title[\"all\"])\n","        remarks.append(remark[\"sentence\"])\n","        remarks.append(title[\"word\"])\n","        remarks.append(title[\"all\"])\n","        remarks.append(np.inner(remark_vec, title_vec)[0][0])\n","        results.append(remarks)\n","        remarks = []\n","\n","outdir='/content/mount/My Drive/00_work/guidance/output/00_nozawa/'\n","df = pd.DataFrame(results, columns=['sentence','word','all','similarity'])\n","df.to_csv(outdir+\"sentence_word_all_similarity.csv\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uSwggTpXKJye","colab_type":"code","colab":{}},"source":["!pip install tensorflow_text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e1ENwZZuSjMh","colab_type":"text"},"source":["## 003_all modify"]},{"cell_type":"code","metadata":{"id":"m2PjE9LmtM4e","colab_type":"code","colab":{}},"source":["\n","import tensorflow_hub as hub\n","import tensorflow_text\n","fn=\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\"\n","fn='https://tfhub.dev/google/universal-sentence-encoder-multilingual/3'\n","embed = hub.load(fn)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_38qdSqZBJg9","colab_type":"code","colab":{}},"source":["import tensorflow_hub as hub\n","import tensorflow_text\n","import numpy as np\n","import pandas as pd\n","import json\n","from  datetime import datetime as dt\n","from sklearn.metrics.pairwise import cosine_similarity\n","col='all'\n","basedir='/content/mount/My Drive/00_work/guidance/20200701/05_all/'\n","\n","# col='synonyms'\n","# basedir='/content/mount/My Drive/00_work/guidance/20200701/04_word/'\n","\n","fn=\"/word_\"+col+\".json\"\n","json_remark = json.load(open(basedir+\"sentence.json\", \"r\"))\n","json_title = json.load(open(basedir+fn, \"r\"))\n","\n","results = []\n","remark_list=[]\n","guidan_list=[]\n","\n","print('remark vec start',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","for remark in json_remark:\n","    clean_sen=remark[\"sentence\"]\n","    remark_list.append(embed(clean_sen)[0])\n","\n","print('remark vec end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","for title in json_title:\n","    clean_sen=title[col]\n","    guidan_list.append(embed(clean_sen)[0])\n","print('guidance vec end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","\n","sim_rg_matrix_ml3=cosine_similarity( remark_list,guidan_list)\n","\n","# print('sim caculate end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","sim_rg_matrix=np.squeeze(sim_rg_matrix_ml3)\n","for i,remark in enumerate(json_remark):\n","    sim_rg_list=[]\n","    r_doc=remark['sentence']\n","    if i==80:\n","      break\n","    for j, g_doc in enumerate(json_title):\n","      sim=sim_rg_matrix[i,j]\n","      sim_rg_list.append([j,sim])\n","    sim_rg_list_sort=sorted(sim_rg_list,key=lambda x:x[1], reverse=True)\n","    for sim_rg in sim_rg_list_sort[0:3]:\n","      doc_id=sim_rg[0]\n","      sim_mean=sim_rg[1]\n","      gs_w=json_title[doc_id]['word']\n","      gs_s=json_title[doc_id][col]\n","      results.append([r_doc,gs_w,gs_s,sim_mean])\n","df_out=pd.DataFrame(results,columns=['sentence','word',col,'similarity'])\n","outdir='/content/mount/My Drive/00_work/guidance/output/00_USE_TEST/'\n","outdir='/content/'\n","df_out.to_csv(outdir+col+\"_modified_sentence_similarity.csv\")\n","print('finish out',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ETYriPDPPDcS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1597988626431,"user_tz":-540,"elapsed":1164,"user":{"displayName":"于勝龍","photoUrl":"","userId":"09684187088444140086"}},"outputId":"06e4c994-e64c-4d10-acc4-34e15031509b"},"source":["org_str = '顔が腫れ上がり、額にケロイドが出来たのに…'\n","\n","# rev=list(reversed(org_str))\n","''.join(map(str,list(reversed(org_str))))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'…にのた来出がドイロケに額、りが上れ腫が顔'"]},"metadata":{"tags":[]},"execution_count":63}]},{"cell_type":"code","metadata":{"id":"cQNzNthITbOK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1598320215376,"user_tz":-540,"elapsed":1995,"user":{"displayName":"于勝龍","photoUrl":"","userId":"09684187088444140086"}},"outputId":"48ac97f4-f6c1-4a7f-ec9e-f713ba7bb9a6"},"source":["'顔が腫れ上がり、額にケロイドが出来たの…に-'.replace('…','')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'顔が腫れ上がり、額にケロイドが出来たのに-'"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"3-wTAPf4bDCv","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zqlel7oaVU5m","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598321768681,"user_tz":-540,"elapsed":975,"user":{"displayName":"于勝龍","photoUrl":"","userId":"09684187088444140086"}},"outputId":"8eae00db-555e-4f7b-c15c-c09e1fc5f8da"},"source":["\n","text = \"ﾌｪﾗ\"\n","# 'フェラ'\n","import re\n","p = re.compile('[\\uFF01-\\uFF0F\\uFF1A-\\uFF20\\uFF3B-\\uFF40\\uFF5B-\\uFF65\\u3000-\\u303F]+')\n","print(p.fullmatch('!？（）［］｢｣、。「」【】'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YPK0aFi-bSVV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1598322209429,"user_tz":-540,"elapsed":1070,"user":{"displayName":"于勝龍","photoUrl":"","userId":"09684187088444140086"}},"outputId":"cab5e3c3-4174-409a-fc1c-a14ac80beb1e"},"source":["import jaconv\n","jaconv.hira2kata(u'ともえまみ')\n","# => u'トモエマミ'\n","jaconv.hira2hkata(u'ともえまみ')\n","# => u'ﾄﾓｴﾏﾐ'\n","jaconv.kata2hira(u'巴マミ')\n","# => u'巴まみ'\n","jaconv.h2z(u'ﾃｨﾛ･ﾌｨﾅｰﾚ')\n","# => u'ティロ･フィナーレ'\n","jaconv.h2z(u'abc', ascii=True)\n","# => u'ａｂｃ'\n","jaconv.h2z(u'123', digit=True)\n","# => u'１２３'\n","jaconv.h2z(u'ｱabc123', kana=False, digit=True, ascii=True)\n","# => u'ｱａｂｃ１２３'\n","jaconv.z2h(u'ティロ・フィナーレ')\n","# => u'ﾃｨﾛ・ﾌｨﾅｰﾚ'\n","jaconv.z2h(u'ａｂｃ', ascii=True)\n","# => u'abc'\n","jaconv.z2h(u'１２３', digit=True)\n","# => u'123'\n","jaconv.z2h(u'アａｂｃ１２３', kana=False, digit=True, ascii=True)\n","# => u'アabc123'\n","jaconv.normalize(u'ティロ･フィナ〜レ', 'NFKC')\n","# => u'ティロ・フィナーレ'"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'ティロ・フィナーレ'"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"ilY0XJhtbe9s","colab_type":"code","colab":{}},"source":["!pip install jaconv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I2bOX7qdWVmA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1598944505338,"user_tz":-540,"elapsed":955,"user":{"displayName":"于勝龍","photoUrl":"","userId":"09684187088444140086"}},"outputId":"c21c0dfe-87bf-49cb-929c-a77107cd7b1c"},"source":["import jaconv\n","def half2all(text):\n","    return jaconv.h2z(text, kana=True, digit=True, ascii=True)\n","text='ドライブに連れて行かれてはキスされ、ﾌｪﾗもしてと言われました'\n","#,'･･･','※','-'\n","half2all('!!')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'！！'"]},"metadata":{"tags":[]},"execution_count":142}]},{"cell_type":"markdown","metadata":{"id":"JrgzUZufIZNw","colab_type":"text"},"source":["## xm1 ml3 qa count"]},{"cell_type":"code","metadata":{"id":"bFErH_bcIgAN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":242},"executionInfo":{"status":"error","timestamp":1600058270223,"user_tz":-540,"elapsed":1198,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"c862d4fc-78ee-428c-c32c-d44af705c096"},"source":["import tensorflow_hub as hub\n","import tensorflow_text\n","import numpy as np\n","import pandas as pd\n","import json\n","import re\n","from  datetime import datetime as dt\n","import jaconv\n","from sklearn.metrics.pairwise import cosine_similarity\n","def half2all(text):\n","    return jaconv.h2z(text, kana=True, digit=True, ascii=True)\n","def pre_process_sen(sent):\n","  sps=['…','･･･','※','-','　']\n","  result=sent\n","  # # result=result.replace('\\-','')\n","  # result=result.replace('…','。')\n","  # result=result.replace('※','。')\n","  # # for s in sps:\n","  # #   result=result.replace(s,'、')\n","  # # result = re.sub(r'[・　。、･]+', '', sent)\n","\n","  splst=[['…','。'],# 重要\n","        ['･･･','。'],\n","         ]\n","  for sp in splst:\n","    result=result.replace(sp[0],sp[1])\n","  \n","  result=half2all(result)\n","  \n","  splst=[['・','。'], # 重要\n","        ['　','。'], # 重要\n","        # ['！！','！'],\n","        # ['。。','。'],\n","         ]\n","  for sp in splst:\n","    result=result.replace(sp[0],sp[1])\n","  return result\n","# def pre_process_sen(sent):\n","#   return sent\n","col='all'\n","basedir='/content/mount/My Drive/00_work/guidance/20200701/05_all/'\n","\n","# col='synonyms'\n","# basedir='/content/mount/My Drive/00_work/guidance/20200701/04_word/'\n","\n","fn=\"/word_\"+col+\".json\"\n","json_remark = json.load(open(basedir+\"sentence.json\", \"r\"))\n","json_title = json.load(open(basedir+fn, \"r\"))\n","\n","results = []\n","remark_list=[]\n","guidan_list=[]\n","guidan_context_list=[]\n","\n","print('remark vec start',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","for remark in json_remark:\n","    clean_sen=pre_process_sen(remark[\"sentence\"])\n","    # clean_sen=remark[\"sentence\"]\n","    remark_list.append(clean_sen)\n","\n","print('remark vec end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","for title in json_title:\n","    clean_sen=pre_process_sen(title[col])\n","    # clean_sen=title[col]\n","    guidan_list.append(clean_sen)\n","    guidan_context_list.append(''.join(map(str,list(reversed(clean_sen)))))\n","    # guidan_context_list.append('')\n","print('guidance vec end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","\n","sim_rg_matrix_ml3=cosine_similarity( embed_ml3(remark_list), embed_ml3(guidan_list))\n","sim_rg_matrix_xm1=cosine_similarity( embed_xm1(remark_list), embed_xm1(guidan_list))\n","sim_rg_matrix_qa3=embed_qa_sim( remark_list, guidan_list,guidan_context_list)\n","# sim_rg_matrix_qa32=sim_rg_matrix_qa3*2\n","# sim_rg_matrix_ml3_xml=sim_rg_matrix_ml3+sim_rg_matrix_xm1\n","print('sim caculate end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","# sim_rg_matrix=np.squeeze(sim_rg_matrix_ml3)\n","cnt_out=6\n","for i,remark in enumerate(json_remark):\n","    sim_rg_list=[]\n","    r_doc=remark['sentence']\n","    if i==80:\n","      break\n","    def get_remark_guidan_sim(i,n,sim_mat):\n","      sim_rgs=[]\n","      for j, g_doc in enumerate(json_title):\n","        sim=sim_mat[i,j]\n","        sim_rgs.append([j,sim])\n","      sim_rgs_sort=sorted(sim_rgs,key=lambda x:x[1], reverse=True)\n","      return sim_rgs_sort[0:n]\n","    sim_rg_topn_ml3=get_remark_guidan_sim(i,cnt_out,sim_rg_matrix_ml3)\n","    sim_rg_topn_xm1=get_remark_guidan_sim(i,cnt_out,sim_rg_matrix_xm1)\n","    sim_rg_topn_qa3=get_remark_guidan_sim(i,cnt_out,sim_rg_matrix_qa3)\n","    # sim_rg_topn_qa32=get_remark_guidan_sim(i,cnt_out,sim_rg_matrix_qa32)\n","    # sim_rg_topn_ml3xm1=get_remark_guidan_sim(i,cnt_out,sim_rg_matrix_ml3_xml)\n","    sim_rg_list=[sim_rg_topn_ml3+sim_rg_topn_xm1,sim_rg_topn_qa3*2]\n","    sim_topn=get_topn_dt(sim_rg_list,3)\n","    for sim_rg in sim_topn:\n","      doc_id=sim_rg[0]\n","      sim_mean=sim_rg[1]\n","      gs_w=json_title[doc_id]['word']\n","      gs_s=json_title[doc_id][col]\n","      results.append([r_doc,gs_w,gs_s,sim_mean])\n","df_out=pd.DataFrame(results,columns=['sentence','word',col,'similarity'])\n","outdir='/content/mount/My Drive/00_work/guidance/output/00_USE_TEST/'\n","df_out.to_csv(outdir+str(cnt_out)+col+\"_use_cnt_sentence_similarity.csv\",index=None)\n","print('finish out',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-9e1a248a58dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/word_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mjson_remark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasedir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"sentence.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mjson_title\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasedir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/mount/My Drive/00_work/guidance/20200701/05_all/sentence.json'"]}]},{"cell_type":"code","metadata":{"id":"A-Geylxb1yE5","colab_type":"code","colab":{}},"source":["# sim_rg_topn_ml3=get_remark_guidan_sim(i,cnt_out,sim_rg_matrix_ml3)\n","# sim_rg_topn_xm1=get_remark_guidan_sim(i,cnt_out,sim_rg_matrix_xm1)\n","# sim_rg_topn_qa3=get_remark_guidan_sim(i,cnt_out,sim_rg_matrix_qa3)\n","n=3\n","dtlst=[sim_rg_topn_ml3+sim_rg_topn_xm1,sim_rg_topn_qa3*2]\n","dtlstn=[]\n","for i, dt in enumerate(dtlst):\n","  for dtj in dt:\n","    dtlstn.append(dtj+[i])\n","df=pd.DataFrame(dtlstn,columns=['docid','p','c'])\n","dg=df.groupby(['docid'])\n","dga=dg.agg({'p':'mean','c':'count'})\n","dga.columns=['p','cnt']\n","dga_n=dga.sort_values(by=['cnt','p'],ascending=False)\n","\n","words=np.array(dga_n.iloc[0:n,:].index)\n","pm=np.array(dga_n.iloc[0:n,0])\n","res=[]\n","for w, p in zip(words,pm):\n","  res.append([w,p])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4lP6BQqa2KbP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":483},"executionInfo":{"status":"ok","timestamp":1598338080029,"user_tz":-540,"elapsed":1493,"user":{"displayName":"于勝龍","photoUrl":"","userId":"09684187088444140086"}},"outputId":"175a98e9-164c-4449-a288-7a04f90bdcaf"},"source":["dga_n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>p</th>\n","      <th>cnt</th>\n","    </tr>\n","    <tr>\n","      <th>docid</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>48</th>\n","      <td>0.504416</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>100</th>\n","      <td>0.405053</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>101</th>\n","      <td>0.392775</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>136</th>\n","      <td>0.524527</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>160</th>\n","      <td>0.407480</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>90</th>\n","      <td>0.384447</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.377822</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>134</th>\n","      <td>0.592628</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>133</th>\n","      <td>0.562642</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>157</th>\n","      <td>0.557960</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>279</th>\n","      <td>0.556805</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>60</th>\n","      <td>0.384881</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>0.383522</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              p  cnt\n","docid               \n","48     0.504416    4\n","100    0.405053    3\n","101    0.392775    3\n","136    0.524527    2\n","160    0.407480    2\n","90     0.384447    2\n","3      0.377822    2\n","134    0.592628    1\n","133    0.562642    1\n","157    0.557960    1\n","279    0.556805    1\n","60     0.384881    1\n","99     0.383522    1"]},"metadata":{"tags":[]},"execution_count":80}]},{"cell_type":"markdown","metadata":{"id":"2a8heO2SXzj8","colab_type":"text"},"source":["## 099_abuse 纏め"]},{"cell_type":"code","metadata":{"id":"jLiG-wtIX4EH","colab_type":"code","colab":{}},"source":["import tensorflow_hub as hub\n","import tensorflow_text\n","import numpy as np\n","import pandas as pd\n","import json\n","import re\n","from  datetime import datetime as dt\n","import jaconv\n","from sklearn.metrics.pairwise import cosine_similarity\n","def half2all(text):\n","    return jaconv.h2z(text, kana=True, digit=True, ascii=True)\n","def pre_process_sen(sent):\n","  sps=['…','･･･','※','-','　']\n","  result=sent\n","  # # result=result.replace('\\-','')\n","  # result=result.replace('…','。')\n","  # result=result.replace('※','。')\n","  # # for s in sps:\n","  # #   result=result.replace(s,'、')\n","  # # result = re.sub(r'[・　。、･]+', '', sent)\n","\n","  splst=[['…','。'],# 重要\n","        ['･･･','。'],\n","         ]\n","  for sp in splst:\n","    result=result.replace(sp[0],sp[1])\n","  \n","  result=half2all(result)\n","  \n","  splst=[['・','。'], # 重要\n","        # ['　','。'], # 重要\n","        ['　',''], # 重要\n","        # ['！！','！'],\n","        # ['。。','。'],\n","         ]\n","  for sp in splst:\n","    result=result.replace(sp[0],sp[1])\n","  return result\n","# def pre_process_sen(sent):\n","#   return sent\n","col='abuse_all'\n","basedir='/content/mount/My Drive/00_work/guidance/001_input_dt/abousedt/'\n","df_remark= pd.read_csv(basedir+\"sentence.csv\", sep='\\t' , header=0)\n","df_title= pd.read_csv(basedir+\"abuse.csv\",  sep='\\t', header=0)\n","\n","results = []\n","remark_list=[]\n","guidan_list=[]\n","guidan_context_list=[]\n","lst_remark=df_remark.values.tolist()\n","lst_title=df_title.values.tolist()\n","lst_title=['挨拶','通知','依頼','質問','苦情','誘い','返事','励まし']\n","print('remark vec start',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","for remark in lst_remark:\n","    # clean_sen=pre_process_sen(remark[0])\n","    clean_sen=remark[0]\n","    remark_list.append(clean_sen)\n","\n","print('remark vec end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","for title in lst_title:\n","    # clean_sen=pre_process_sen(title[1])\n","    clean_sen=title\n","    guidan_list.append(clean_sen)\n","    guidan_context_list.append(''.join(map(str,list(reversed(clean_sen)))))\n","print('guidance vec end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","remark_vec1=embed_ml3(remark_list[0:3000])\n","remark_vec2=embed_ml3(remark_list[3000:])\n","remark_vec=np.concatenate([remark_vec1,remark_vec2],axis=0)\n","\n","sim_rg_matrix_ml3=cosine_similarity( embed_ml3(guidan_list),remark_vec)\n","sim_rg_matrix_xm1=cosine_similarity(  embed_xm1(guidan_list),embed_xm1(remark_list))\n","sim_rg_matrix_qa3=embed_qa_sim_a( remark_list, guidan_list,guidan_context_list)\n","print('sim caculate end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","# sim_rg_matrix=np.squeeze(sim_rg_matrix_ml3)\n","cnt_out=6\n","\n","# for i,remark in enumerate(df_remark.values.tolist()):\n","#     sim_rg_list=[]\n","#     r_doc=remark\n","#     def get_remark_guidan_sim(i,n,sim_mat):\n","#       sim_rgs=[]\n","#       for j, g_doc in enumerate(df_title.values.tolist()):\n","for i,remark in enumerate(lst_title):\n","    sim_rg_list=[]\n","    r_doc=remark\n","    def get_remark_guidan_sim(i,n,sim_mat):\n","      sim_rgs=[]\n","      for j, g_doc in enumerate(lst_remark):\n","        sim=sim_mat[i,j]\n","        sim_rgs.append([j,sim])\n","      sim_rgs_sort=sorted(sim_rgs,key=lambda x:x[1], reverse=True)\n","      return sim_rgs_sort[0:n]\n","    sim_rg_topn_ml3=get_remark_guidan_sim(i,cnt_out,sim_rg_matrix_ml3)\n","    sim_rg_topn_xm1=get_remark_guidan_sim(i,cnt_out,sim_rg_matrix_xm1)\n","    sim_rg_topn_qa3=get_remark_guidan_sim(i,cnt_out,sim_rg_matrix_qa3)\n","    # sim_rg_topn_qa32=get_remark_guidan_sim(i,cnt_out,sim_rg_matrix_qa32)\n","    # sim_rg_topn_ml3xm1=get_remark_guidan_sim(i,cnt_out,sim_rg_matrix_ml3_xml)\n","    sim_rg_list=[sim_rg_topn_ml3+sim_rg_topn_xm1,sim_rg_topn_qa3*2]\n","    sim_topn=get_topn_dt(sim_rg_list,int(cnt_out/2))\n","    for sim_rg in sim_topn:\n","      doc_id=sim_rg[0]\n","      sim_mean=sim_rg[1]\n","      # gs_w=lst_remark[doc_id][0]\n","      gs_s=lst_remark[doc_id][0]\n","      results.append([r_doc[0],r_doc[1],gs_s,sim_mean])\n","df_out=pd.DataFrame(results,columns=['sentence','word',col,'similarity'])\n","outdir='/content/mount/My Drive/00_work/guidance/002_output_dt/'\n","df_out.to_csv(outdir+str(cnt_out)+col+\"_abuse_sentence_similarity.csv\",index=None)\n","print('finish out',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HEA9KoykAIf6","colab_type":"text"},"source":["## 挨拶"]},{"cell_type":"code","metadata":{"id":"gXoE23RqALN_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"status":"ok","timestamp":1599805172789,"user_tz":-540,"elapsed":6296,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"677a9cd7-ac46-46c7-86ec-51b210d61fde"},"source":["import tensorflow_hub as hub\n","import tensorflow_text\n","import numpy as np\n","import pandas as pd\n","import json\n","import re\n","from  datetime import datetime as dt\n","import jaconv\n","from sklearn.metrics.pairwise import cosine_similarity\n","def half2all(text):\n","    return jaconv.h2z(text, kana=True, digit=True, ascii=True)\n","def pre_process_sen(sent):\n","  sps=['…','･･･','※','-','　']\n","  result=sent\n","  # # result=result.replace('\\-','')\n","  # result=result.replace('…','。')\n","  # result=result.replace('※','。')\n","  # # for s in sps:\n","  # #   result=result.replace(s,'、')\n","  # # result = re.sub(r'[・　。、･]+', '', sent)\n","\n","  splst=[['…','。'],# 重要\n","        ['･･･','。'],\n","         ]\n","  for sp in splst:\n","    result=result.replace(sp[0],sp[1])\n","  \n","  result=half2all(result)\n","  \n","  splst=[['・','。'], # 重要\n","        # ['　','。'], # 重要\n","        ['　',''], # 重要\n","        # ['！！','！'],\n","        # ['。。','。'],\n","         ]\n","  for sp in splst:\n","    result=result.replace(sp[0],sp[1])\n","  return result\n","# def pre_process_sen(sent):\n","#   return sent\n","col='abuse_all'\n","basedir='/content/mount/My Drive/00_work/guidance/001_input_dt/abousedt/'\n","df_remark= pd.read_csv(basedir+\"sentence.csv\", sep='\\t' , header=0)\n","df_title= pd.read_csv(basedir+\"abuse.csv\",  sep='\\t', header=0)\n","\n","results = []\n","remark_list=[]\n","guidan_list=[]\n","guidan_context_list=[]\n","lst_remark=df_remark.values.tolist()\n","lst_title=df_title.values.tolist()\n","lst_title=['会釈、敬礼、頭語、ご挨拶、目礼、辞儀、式礼、御辞儀、お辞儀、御挨拶',\n","           '通知','依頼','質問','苦情','誘い','返事',\n","           '励まし、声援、鞭撻、ご鞭撻、奨励、薦め、勧奨、励み、勧め、振興、助勢、御鞭撻、振起、督励、鼓舞激励、鼓吹、激励、鼓舞']\n","print('remark vec start',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","for remark in lst_remark:\n","    # clean_sen=pre_process_sen(remark[0])\n","    clean_sen=remark[0]\n","    remark_list.append(clean_sen)\n","\n","print('remark vec end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","for title in lst_title:\n","    # clean_sen=pre_process_sen(title[1])\n","    clean_sen=title\n","    guidan_list.append(clean_sen)\n","    guidan_context_list.append(''.join(map(str,list(reversed(clean_sen)))))\n","print('guidance vec end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","remark_vec1=embed_ml3(remark_list[0:3000])\n","remark_vec2=embed_ml3(remark_list[3000:])\n","remark_vec=np.concatenate([remark_vec1,remark_vec2],axis=0)\n","\n","sim_rg_matrix_ml3=cosine_similarity( remark_vec,embed_ml3(guidan_list))\n","sim_rg_matrix_xm1=cosine_similarity(  embed_xm1(guidan_list),embed_xm1(remark_list))\n","# sim_rg_matrix_qa3=embed_qa_sim_b( remark_list, guidan_list,guidan_context_list)\n","print('sim caculate end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","# sim_rg_matrix=np.squeeze(sim_rg_matrix_ml3)\n","cnt_out=6\n","for i,remark in enumerate(lst_remark):\n","    sim_rg_list=[]\n","    r_doc=remark\n","    def get_remark_guidan_sim(i,n,sim_mat):\n","      sim_rgs=[]\n","      for j, g_doc in enumerate(lst_title):\n","        sim=sim_mat[i,j]\n","        sim_rgs.append([j,sim])\n","      sim_rgs_sort=sorted(sim_rgs,key=lambda x:x[1], reverse=True)\n","      return sim_rgs_sort[0:n]\n","    sim_rg_topn_ml3=get_remark_guidan_sim(i,cnt_out,sim_rg_matrix_ml3)\n","\n","    sim_topn=sorted(sim_rg_topn_ml3,key=lambda x:x[1], reverse=True)\n","    for sim_rg in sim_topn[0:1]:\n","      doc_id=sim_rg[0]\n","      sim_mean=sim_rg[1]\n","      gs_s=lst_title[doc_id]\n","      results.append([r_doc[0],gs_s,sim_mean])\n","df_out=pd.DataFrame(results,columns=['sentence','word','similarity'])\n","outdir='/content/mount/My Drive/00_work/guidance/002_output_dt/'\n","df_out.to_csv(outdir+str(cnt_out)+col+\"_abuse_sentence_similarity.csv\",index=None)\n","print('finish out',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["remark vec start 20/09/11 06:19:26\n","remark vec end 20/09/11 06:19:26\n","guidance vec end 20/09/11 06:19:26\n","sim caculate end 20/09/11 06:19:32\n","finish out 20/09/11 06:19:32\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r9GJFNewzgFF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":178},"executionInfo":{"status":"ok","timestamp":1599804950711,"user_tz":-540,"elapsed":602,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"5d1efc67-20f3-46e7-efc0-5f7b21e01cbc"},"source":["# embed_qa_sim_b( remark_list, guidan_list,guidan_context_list)\n","guidan_context_list"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['拶挨御 ・ 儀辞お ・ 儀辞御 ・ 礼式 ・ 儀辞 ・ 礼目 ・ 拶挨ご ・ 語頭 ・ 礼敬 ・ 釈会',\n"," '知通',\n"," '頼依',\n"," '問質',\n"," '情苦',\n"," 'い誘',\n"," '事返',\n"," '舞鼓 ・ 励激 ・ 吹鼓 ・ 励激舞鼓 ・ 励督 ・ 起振 ・ 撻鞭御 ・ 勢助 ・ 興振 ・ め勧 ・ み励 ・ 奨勧 ・ め薦 ・ 励奨 ・ 撻鞭ご ・ 撻鞭 ・ 援声・しま励']"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"EQVN-2Xl7p7o","colab_type":"text"},"source":["## xm1 ml3 plus"]},{"cell_type":"code","metadata":{"id":"h2ej5r5Z7yKU","colab_type":"code","colab":{}},"source":["import tensorflow_hub as hub\n","import tensorflow_text\n","import numpy as np\n","import pandas as pd\n","import json\n","import re\n","from  datetime import datetime as dt\n","from sklearn.metrics.pairwise import cosine_similarity\n","col='all'\n","basedir='/content/mount/My Drive/00_work/guidance/20200701/05_all/'\n","\n","# col='synonyms'\n","# basedir='/content/mount/My Drive/00_work/guidance/20200701/04_word/'\n","\n","fn=\"/word_\"+col+\".json\"\n","json_remark = json.load(open(basedir+\"sentence.json\", \"r\"))\n","json_title = json.load(open(basedir+fn, \"r\"))\n","\n","results = []\n","remark_list=[]\n","guidan_list=[]\n","guidan_context_list=[]\n","\n","print('remark vec start',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","for remark in json_remark:\n","    # clean_sen=pre_process_sen(remark[\"sentence\"])\n","    clean_sen=remark[\"sentence\"]\n","    remark_list.append(clean_sen)\n","\n","print('remark vec end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","for title in json_title:\n","    # clean_sen=pre_process_sen(title[col])\n","    clean_sen=title[col]\n","    guidan_list.append(clean_sen)\n","    guidan_context_list.append(''.join(map(str,list(reversed(clean_sen)))))\n","print('guidance vec end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","\n","sim_rg_matrix_ml3=cosine_similarity( embed_ml3(remark_list), embed_ml3(guidan_list))\n","sim_rg_matrix_xm1=cosine_similarity( embed_xm1(remark_list), embed_xm1(guidan_list))\n","sim_rg_matrix_qa3=embed_qa_sim( remark_list, guidan_list,guidan_context_list)\n","sim_rg_matrix_qa3=sim_rg_matrix_qa3*2\n","sim_rg_matrix=(sim_rg_matrix_ml3+sim_rg_matrix_xm1+sim_rg_matrix_qa3)/4\n","\n","print('sim caculate end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","# sim_rg_matrix=np.squeeze(sim_rg_matrix_ml3)\n","for i,remark in enumerate(json_remark):\n","    sim_rg_list=[]\n","    r_doc=remark['sentence']\n","    if i==80:\n","      break\n","    for j, g_doc in enumerate(json_title):\n","      sim=sim_rg_matrix[i,j]\n","      sim_rg_list.append([j,sim])\n","    sim_rg_list_sort=sorted(sim_rg_list,key=lambda x:x[1], reverse=True)\n","    for sim_rg in sim_rg_list_sort[0:3]:\n","      doc_id=sim_rg[0]\n","      sim_mean=sim_rg[1]\n","      gs_w=json_title[doc_id]['word']\n","      gs_s=json_title[doc_id][col]\n","      results.append([r_doc,gs_w,gs_s,sim_mean])\n","df_out=pd.DataFrame(results,columns=['sentence','word',col,'similarity'])\n","outdir='/content/mount/My Drive/00_work/guidance/output/00_USE_TEST/'\n","df_out.to_csv(outdir+col+\"_xm1_ml3_modified_sentence_similarity.csv\")\n","print('finish out',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ls587HCJ86NN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597985874114,"user_tz":-540,"elapsed":1181,"user":{"displayName":"于勝龍","photoUrl":"","userId":"09684187088444140086"}},"outputId":"ffce5af4-f2da-4ef6-9abb-940e413ecc1c"},"source":["(sim_rg_matrix_qa3*2)[0][0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.35334775"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"zqaMcV8D_L2M","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"cbfd258f-54ff-4d61-b3a9-add8156ee0de"},"source":["sim_rg_matrix_xm1[0][0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.25507325"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"fW16me4O_UQa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"215bea35-b0d4-42b7-c090-ff08fdaf858e"},"source":["(sim_rg_matrix_xm1+sim_rg_matrix_ml3)[0][0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.34305263"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"QY6FyUbBMrvN","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PpowzbLhSmJB","colab_type":"code","colab":{}},"source":["import tensorflow_hub as hub\n","import tensorflow_text\n","import numpy as np\n","import pandas as pd\n","import json\n","from  datetime import datetime as dt\n","from sklearn.metrics.pairwise import cosine_similarity\n","col='all'\n","basedir='/content/mount/My Drive/00_work/guidance/20200701/05_all/'\n","\n","col='synonyms'\n","basedir='/content/mount/My Drive/00_work/guidance/20200701/04_word/'\n","# embed_qa_sim\n","\n","fn=\"/word_\"+col+\".json\"\n","json_remark = json.load(open(basedir+\"sentence.json\", \"r\"))\n","json_title = json.load(open(basedir+fn, \"r\"))\n","\n","results = []\n","remark_vec_list=[]\n","guidan_vec_list=[]\n","\n","\n","print('remark vec start',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","for remark in json_remark:\n","    # clean_sen=pre_process_sen(remark[\"sentence\"])\n","    clean_sen=remark[\"sentence\"]\n","    remark_vec = embed(clean_sen)\n","    remark_vec_list.append(remark_vec[0])\n","    # print(remark_vec[0].shape)\n","print('remark vec end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","for title in json_title:\n","    # clean_sen=pre_process_sen(title[col])\n","    clean_sen=title[col]\n","    title_vec = embed(clean_sen)\n","    guidan_vec_list.append(title_vec[0])\n","print('guidance vec end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","# sim_rg_matrix_org=np.inner(remark_vec_list, guidan_vec_list)\n","sim_rg_matrix_org=cosine_similarity( remark_vec_list, guidan_vec_list)\n","# embed_ml3\n","# embed_xm1\n","print('sim caculate end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","sim_rg_matrix=np.squeeze(sim_rg_matrix_org)\n","for i,remark in enumerate(json_remark):\n","    sim_rg_list=[]\n","    r_doc=remark['sentence']\n","    if i==80:\n","      break\n","    for j, g_doc in enumerate(json_title):\n","      sim=sim_rg_matrix[i,j]\n","      sim_rg_list.append([j,sim])\n","    sim_rg_list_sort=sorted(sim_rg_list,key=lambda x:x[1], reverse=True)\n","    for sim_rg in sim_rg_list_sort[0:3]:\n","      doc_id=sim_rg[0]\n","      sim_mean=sim_rg[1]\n","      gs_w=json_title[doc_id]['word']\n","      gs_s=json_title[doc_id][col]\n","      results.append([r_doc,gs_w,gs_s,sim_mean])\n","df_out=pd.DataFrame(results,columns=['sentence','word',col,'similarity'])\n","outdir='/content/mount/My Drive/00_work/guidance/output/00_nozawa/'\n","df_out.to_csv(outdir+col+\"_trans_inner_modified_sentence_word_synonyms_similarity.csv\")\n","\n","\n","print('finish out',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pYNHn-wjMszw","colab_type":"text"},"source":["## 人力抽出_通告事例"]},{"cell_type":"code","metadata":{"id":"Drx_0tJ3Mw42","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":108},"outputId":"b9eaecea-7f8b-4259-a14f-32d292ce366c"},"source":["import tensorflow_hub as hub\n","# import tensorflow_text\n","import numpy as np\n","import pandas as pd\n","import json\n","from  datetime import datetime as dt\n","\n","# col='all'\n","# basedir='/content/mount/My Drive/00_work/guidance/20200701/05_all/'\n","\n","# # col='synonyms'\n","# # basedir='/content/mount/My Drive/00_work/guidance/20200701/04_word/'\n","\n","\n","# fn=\"/word_\"+col+\".json\"\n","# json_remark = json.load(open(basedir+\"sentence.json\", \"r\"))\n","# json_title = json.load(open(basedir+fn, \"r\"))\n","\n","def get_remark_dt2(fn_in):\n","  col='all'\n","  basedir='/content/mount/My Drive/00_work/guidance/20200701/05_all/'\n","\n","  # col='synonyms'\n","  # basedir='/content/mount/My Drive/00_work/guidance/20200701/04_word/'\n","\n","  fn_remark='/content/mount/My Drive/00_work/guidance/remak/'+fn_in\n","  json_remark = json.load(open(fn_remark, \"r\"))\n","  fn=\"/word_\"+col+\".json\"\n","  json_title = json.load(open(basedir+fn, \"r\"))\n","  return json_remark,json_title\n","col='all'\n","fn='notification_case_sentence'\n","# fn='human_power_extraction_sentence'\n","json_remark,json_title=get_remark_dt2(fn+'.json')\n","results = []\n","remark_vec_list=[]\n","guidan_vec_list=[]\n","print('remark vec start',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","for remark in json_remark:\n","    remark_vec = embed(remark[\"sentence\"])\n","    remark_vec_list.append(remark_vec[0])\n","    # print(remark_vec[0].shape)\n","print('remark vec end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","for title in json_title:\n","    title_vec = embed(title[col])\n","    guidan_vec_list.append(title_vec[0])\n","print('guidance vec end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","sim_rg_matrix_org=np.inner(remark_vec_list, guidan_vec_list)\n","# sim_rg_matrix_org=cosine_similarity( remark_vec_list, guidan_vec_list)\n","print('sim caculate end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","sim_rg_matrix=np.squeeze(sim_rg_matrix_org)\n","for i,remark in enumerate(json_remark):\n","    sim_rg_list=[]\n","    r_doc=remark['sentence']\n","    for j, g_doc in enumerate(json_title):\n","      sim=sim_rg_matrix[i,j]\n","      sim_rg_list.append([j,sim])\n","    sim_rg_list_sort=sorted(sim_rg_list,key=lambda x:x[1], reverse=True)\n","    for sim_rg in sim_rg_list_sort[0:3]:\n","      doc_id=sim_rg[0]\n","      sim_mean=sim_rg[1]\n","      gs_w=json_title[doc_id]['word']\n","      gs_s=json_title[doc_id][col]\n","      results.append([r_doc,gs_w,gs_s,sim_mean])\n","df_out=pd.DataFrame(results,columns=['sentence','word',col,'similarity'])\n","outdir='/content/mount/My Drive/00_work/guidance/output/00_nozawa/'\n","df_out.to_csv(outdir+fn+\"_3333_inner_modified_sentence_word_synonyms_similarity.csv\")\n","\n","\n","print('finish out',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["remark vec start 20/08/13 00:48:03\n","remark vec end 20/08/13 00:48:03\n","guidance vec end 20/08/13 00:48:07\n","sim caculate end 20/08/13 00:48:30\n","finish out 20/08/13 00:48:30\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kmtNtxchl0ew","colab_type":"text"},"source":["## perm simi test"]},{"cell_type":"code","metadata":{"id":"RgFe5Mjrl337","colab_type":"code","colab":{}},"source":["import tensorflow_hub as hub\n","# import tensorflow_text\n","import numpy as np\n","import pandas as pd\n","import json\n","from  datetime import datetime as dt\n","from sklearn.metrics.pairwise import cosine_similarity\n","col='all'\n","basedir='/content/mount/My Drive/00_work/guidance/20200701/05_all/'\n","\n","# col='synonyms'\n","# basedir='/content/mount/My Drive/00_work/guidance/20200701/04_word/'\n","fn=\"/word_\"+col+\".json\"\n","json_remark = json.load(open(basedir+\"sentence.json\", \"r\"))\n","json_title = json.load(open(basedir+fn, \"r\"))\n","nsen=5\n","results = []\n","print('sim caculate end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","# sim_rg_matrix=np.squeeze(sim_rg_matrix_org)\n","for i,remark in enumerate(json_remark):\n","    sim_rg_list=[]\n","    r_doc=remark['sentence']\n","    print(i,r_doc)\n","    if i ==80:\n","      break\n","    for j, g_doc in enumerate(json_title):\n","      # sim=sim_rg_matrix[i,j]\n","      str_sen1=r_doc\n","      str_sen2=g_doc[col]\n","      sim=cosine_similarity_perm(str_sen1,str_sen2,nsen)\n","      sim_rg_list.append([j,sim])\n","    sim_rg_list_sort=sorted(sim_rg_list,key=lambda x:x[1], reverse=True)\n","    # print(sim_rg_list_sort)\n","    # break\n","    for sim_rg in sim_rg_list_sort[0:3]:\n","      doc_id=sim_rg[0]\n","      sim_mean=sim_rg[1]\n","      gs_w=json_title[doc_id]['word']\n","      gs_s=json_title[doc_id][col]\n","      results.append([r_doc,gs_w,gs_s,sim_mean])\n","    #   print(r_doc,gs_w,gs_s,sim_mean)\n","    # break\n","df_out=pd.DataFrame(results,columns=['sentence','word',col,'similarity'])\n","outdir='/content/mount/My Drive/00_work/guidance/output/00_nozawa/'\n","df_out.to_csv(outdir+col+\"_perm_pre_similarity.csv\")\n","print('finish out',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mGyC6v7g0usy","colab_type":"text"},"source":["## use qa tcd"]},{"cell_type":"code","metadata":{"id":"ls2qnUDP0v78","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"fcdfbb68-43a0-4e19-ea51-af7e30bf5fef"},"source":["import tensorflow_hub as hub\n","import tensorflow_text\n","import numpy as np\n","import pandas as pd\n","import json\n","from  datetime import datetime as dt\n","from sklearn.metrics.pairwise import cosine_similarity\n","col='tcd'\n","basedir='/content/mount/My Drive/00_work/guidance/20200701/05_all/'\n","\n","# col='synonyms'\n","# basedir='/content/mount/My Drive/00_work/guidance/20200701/04_word/'\n","\n","\n","# fn=\"/word_\"+col+\".json\"\n","# json_remark = json.load(open(basedir+\"sentence.json\", \"r\"))\n","# json_title = json.load(open(basedir+fn, \"r\"))\n","\n","fn_tcd='/content/mount/My Drive/00_work/guidance/20200701/07_tcd/word_tcd.json'\n","json_title = json.load(open(fn_tcd, \"r\"))\n","cols=[ 'title','confirmation','detail','synonyms']\n","# cols=[ 'title','confirmation','detail']#,'synonyms']\n","results = []\n","remark_list=[]\n","guidan_list=[]\n","guidan_context_list=[]\n","\n","\n","print('remark vec start',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","for remark in json_remark:\n","    # clean_sen=pre_process_sen(remark[\"sentence\"])\n","    clean_sen=remark[\"sentence\"]\n","    remark_list.append(clean_sen)\n","print('remark vec end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","for tcd in json_title:\n","    guidan_list.append(tcd['tcd'])\n","    guidan_context_list.append(tcd['synonyms'])\n","\n","print('guidance vec end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","# sim_rg_matrix_org=np.inner(remark_vec_list, guidan_vec_list)\n","sim_rg_matrix_org=embed_qa_sim( remark_list, guidan_list,guidan_context_list)\n","print('sim caculate end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","sim_rg_matrix=np.squeeze(sim_rg_matrix_org)\n","for i,remark in enumerate(json_remark):\n","    sim_rg_list=[]\n","    r_doc=remark['sentence']\n","    if i==80:\n","      break\n","    for j, g_doc in enumerate(json_title):\n","      sim=sim_rg_matrix[i,j]\n","      sim_rg_list.append([j,sim])\n","    sim_rg_list_sort=sorted(sim_rg_list,key=lambda x:x[1], reverse=True)\n","    for sim_rg in sim_rg_list_sort[0:3]:\n","      doc_id=sim_rg[0]\n","      sim_mean=sim_rg[1]\n","      gs_w=json_title[doc_id]['word']\n","      gs_s=json_title[doc_id][col]\n","      results.append([r_doc,gs_w,gs_s,sim_mean])\n","df_out=pd.DataFrame(results,columns=['sentence','word',col,'similarity'])\n","outdir='/content/mount/My Drive/00_work/guidance/output/00_nozawa/'\n","df_out.to_csv(outdir+col+\"_tcd_embed_qa_sentence_similarity.csv\")\n","\n","\n","print('finish out',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["remark vec start 20/08/19 05:12:22\n","remark vec end 20/08/19 05:12:22\n","guidance vec end 20/08/19 05:12:22\n","sim caculate end 20/08/19 05:12:24\n","finish out 20/08/19 05:12:24\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aocDlGdSw7J8","colab_type":"text"},"source":["## use qa"]},{"cell_type":"markdown","metadata":{"id":"VbRz8hIH0sra","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"q1BHa-kjxAOS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1599183742431,"user_tz":-540,"elapsed":1704,"user":{"displayName":"于勝龍","photoUrl":"","userId":"09684187088444140086"}},"outputId":"d1e43eea-2f9b-4a17-92a3-f8c2690d1bbd"},"source":["import tensorflow_hub as hub\n","import tensorflow_text\n","import numpy as np\n","import pandas as pd\n","import json\n","from  datetime import datetime as dt\n","from sklearn.metrics.pairwise import cosine_similarity\n","col='all'\n","def pre_process_sen(sent):\n","  # sps=['…','･･･','※','-']\n","  sps=['…','･･･','-']\n","  result=sent\n","  for s in sps:\n","    result=result.replace(s,'')\n","  # result = re.sub(r'[・　。、･]+', '', sent)\n","  result = re.sub(r'[・･\\n]+', '。', result)\n","  return result\n","basedir='/content/mount/My Drive/00_work/guidance/20200701/05_all/'\n","# col='synonyms'\n","# basedir='/content/mount/My Drive/00_work/guidance/20200701/04_word/'\n","fn=\"/word_\"+col+\".json\"\n","json_remark = json.load(open(basedir+\"sentence.json\", \"r\"))\n","json_title = json.load(open(basedir+fn, \"r\"))\n","\n","results = []\n","remark_list=[]\n","guidan_list=[]\n","guidan_context_list=[]\n","print('remark vec start',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","for remark in json_remark:\n","    clean_sen=pre_process_sen(remark[\"sentence\"])\n","    # clean_sen=remark[\"sentence\"]\n","    remark_list.append(clean_sen)\n","print('remark vec end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","for title in json_title:\n","    # clean_sen=title[col]\n","    clean_sen=pre_process_sen(title[col])\n","    guidan_list.append(clean_sen)\n","    # guidan_context_list.append(''.join(map(str,list(reversed(clean_sen)))))\n","    guidan_context_list.append('')\n","\n","print('guidance vec end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","sim_rg_matrix_org=embed_qa_sim( remark_list, guidan_list,guidan_context_list)\n","print('sim caculate end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","sim_rg_matrix=np.squeeze(sim_rg_matrix_org)\n","for i,remark in enumerate(json_remark):\n","    sim_rg_list=[]\n","    r_doc=remark['sentence']\n","    if i==80:\n","      break\n","    for j, g_doc in enumerate(json_title):\n","      sim=sim_rg_matrix[i,j]\n","      sim_rg_list.append([j,sim])\n","    sim_rg_list_sort=sorted(sim_rg_list,key=lambda x:x[1], reverse=True)\n","    for sim_rg in sim_rg_list_sort[0:3]:\n","      doc_id=sim_rg[0]\n","      sim_mean=sim_rg[1]\n","      gs_w=json_title[doc_id]['word']\n","      gs_s=json_title[doc_id][col]\n","      results.append([r_doc,gs_w,gs_s,sim_mean])\n","df_out=pd.DataFrame(results,columns=['sentence','word',col,'similarity'])\n","outdir='/content/mount/My Drive/00_work/guidance/output/00_USE_TEST/'\n","# outdir='/content/'\n","df_out.to_csv(outdir+col+\"_embed_qa_sentence_similarity.csv\")\n","print('finish out',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["remark vec start 20/09/04 01:42:20\n","remark vec end 20/09/04 01:42:20\n","guidance vec end 20/09/04 01:42:20\n","sim caculate end 20/09/04 01:42:22\n","finish out 20/09/04 01:42:22\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SLA_FsRFx2YG","colab_type":"code","colab":{}},"source":["df_out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ad2lu-ZgT5p0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"c11b3c6c-59e5-4ad5-d44e-ae0e56c359b7"},"source":["dt.utcnow().strftime( '%y/%m/%d %H:%M:%S')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'20/08/04 02:37:28'"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"oXe4RckODrA1","colab_type":"code","colab":{}},"source":["import tensorflow_hub as hub\n","import tensorflow_text\n","import numpy as np\n","import pandas as pd\n","import json\n","basedir='/content/mount/My Drive/00_work/guidance/20200701/04_word/'\n","json_remark = json.load(open(basedir+\"sentence.json\", \"r\"))\n","json_title = json.load(open(basedir+\"/word_synonyms.json\", \"r\"))\n","\n","results = []\n","remarks = []\n","for remark in json_remark:\n","    remark_vec = embed(remark[\"sentence\"])\n","    for title in json_title:\n","        title_vec = embed(title[\"synonyms\"])\n","        remarks.append(remark[\"sentence\"])\n","        remarks.append(title[\"word\"])\n","        remarks.append(title[\"synonyms\"])\n","        sim=np.inner(remark_vec, title_vec)[0][0]\n","        remarks.append(np.round(sim,3))\n","        results.append(remarks)\n","        remarks = []\n","\n","outdir='/content/mount/My Drive/00_work/guidance/output/00_nozawa/'\n","df = pd.DataFrame(results, columns=['sentence','word','synonyms','similarity'])\n","df.to_csv(outdir+\"sentence_word_synonyms_similarity.csv\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uf94AHN_Np43","colab_type":"text"},"source":["## np.inner test"]},{"cell_type":"code","metadata":{"id":"oqDFUMihP4pf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a10410c9-d576-4299-8f1f-b4690c28aa64"},"source":["remark_vec=embed(['彼女はペンパイナッポーアッポーペ'])\n","title_vec=embed(['彼女はペンパイナッポーアッポーペ','ポーアッポーペ'])\n","np.inner(remark_vec, title_vec)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1.0000002, 0.4617161]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"fPtCmYlJI36M","colab_type":"code","colab":{}},"source":["!pip install ginza\n","!pip install git+https://github.com/boudinfl/pke.git\n","!python -m nltk.downloader stopwords"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zk0yM_1b8Ohn","colab_type":"text"},"source":["# word2vector test"]},{"cell_type":"code","metadata":{"id":"Th-7dT8nmO58","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"status":"ok","timestamp":1600325954275,"user_tz":-540,"elapsed":1818,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"3a8d8bbd-85db-42ea-ab7e-efa4e23b9815"},"source":[""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mecab:\n"," 溺れ\tオボレ\tオボレル\t溺れる\t動詞-一般\t下一段-ラ行\t連用形-一般\t0\n","て\tテ\tテ\tて\t助詞-接続助詞\t\t\t\n","EOS\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LGUsoAUn4GsT","colab_type":"text"},"source":["## key word analysis"]},{"cell_type":"code","metadata":{"id":"zLd_QeThhY_r","colab_type":"code","colab":{}},"source":["import MeCab  as mc\n","import subprocess\n","import json\n","import pandas as pd\n","cmd='echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"'\n","path = (subprocess.Popen(cmd, stdout=subprocess.PIPE,\n","                           shell=True).communicate()[0]).decode('utf-8')\n","m=mc.Tagger(\"-d {0}\".format(path))\n","# m=mc.Tagger(\"-Owakati\")\n","\n","\n","def extract_words(col,path,outputdir,sen_flg=False):\n","  if sen_flg:\n","    json_remark = json.load(open('/content/mount/My Drive/00_work/guidance/20200701/04_word/sentence.json', \"r\"))\n","    col='sentence'\n","  else:\n","    json_remark = json.load(open(path+\"word_\"+col+\".json\", \"r\"))\n","  # json_remark = json.load(open(fp, \"r\"))\n","  remarks=[]\n","  remarks_mecab=[]\n","  for remark in json_remark:\n","    remarks.append(remark[col])\n","  for remark in remarks:\n","    lines=m.parse(remark).split('\\n')\n","    for line in lines:\n","      items = re.split('[\\t]',line)\n","      if len(items) > 1:\n","        subItems= items[1].split(',')\n","        wv_flg='Y'\n","        if items[0] not in model_w2v:\n","          wv_flg='N'\n","        remarks_mecab.append([items[0]]+[wv_flg]+subItems)\n","  df = pd.DataFrame(remarks_mecab)\n","  df.to_csv(outputdir+col+\"_words.csv\", header=None,index=None)\n","# fp='/content/mount/My Drive/00_work/guidance/20200701/04_word/sentence.json'\n","# outpath='/content/mount/My Drive/00_work/guidance/words_analysis/remarks_mecab.csv'\n","# # pathout+\"word_\"+col+\".json\"\n","# extract_words(fp,outpath)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WWLnzFz3Lhuo","colab_type":"code","colab":{}},"source":["\n","outputdir='/content/mount/My Drive/00_work/guidance/words_analysis/'\n","# 01_title\n","columns='title'\n","path='/content/mount/My Drive/00_work/guidance/20200701/01_title/'\n","extract_words(columns,path,outputdir,sen_flg=True)\n","extract_words(columns,path,outputdir)\n","\n","#02_confirm\n","columns='confirmation'\n","path='/content/mount/My Drive/00_work/guidance/20200701/02_confirm/'\n","extract_words(columns,path,outputdir)\n","\n","#03_detail\n","columns='detail'\n","path='/content/mount/My Drive/00_work/guidance/20200701/03_detail/'\n","extract_words(columns,path,outputdir)\n","\n","#04_word\n","columns='synonyms'\n","path='/content/mount/My Drive/00_work/guidance/20200701/04_word/'\n","extract_words(columns,path,outputdir)\n","\n","\n","\n","#05_all\n","columns='all'\n","path='/content/mount/My Drive/00_work/guidance/20200701/05_all/'\n","extract_words(columns,path,outputdir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2R83TbpPJF2-","colab_type":"code","colab":{}},"source":["%mkdir '/content/mount/My Drive/00_work/guidance/words_analysis'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6u2zD3HQ4RBR","colab_type":"text"},"source":["## word vector key word"]},{"cell_type":"code","metadata":{"id":"uUfMDJoK72GM","colab_type":"code","colab":{}},"source":["import MeCab  as mc\n","import subprocess\n","import json\n","import pandas as pd\n","\n","filterList=[\n","['動詞','自立'],\n","['名詞','サ変接続'],\n","['名詞','ナイ形容詞語幹'],\n","['名詞','一般'],\n","['名詞','固有名詞'],\n","['名詞','形容動詞語幹'],\n","['形容詞','自立']]\n","cmd='echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"'\n","path = (subprocess.Popen(cmd, stdout=subprocess.PIPE,\n","                           shell=True).communicate()[0]).decode('utf-8')\n","m=mc.Tagger(\"-d {0}\".format(path))\n","pathout='/content/mount/My Drive/00_work/guidance/20200701/04_word/'\n","json_remark = json.load(open(pathout+\"sentence.json\", \"r\"))\n","remarks=[]\n","remarks_mecab=[]\n","remarks_keys=[]\n","for remark in json_remark:\n","  remarks.append(remark[\"sentence\"])\n","\n","for remark in remarks:\n","  lines=m.parse(remark).split('\\n')\n","  remarks_key=[]\n","  remarks_key.append(remark)\n","  for line in lines:\n","    items = re.split('[\\t]',line)\n","    if len(items) > 1:\n","      subItems= items[1].split(',')\n","      for f in filterList:\n","        if subItems[0]==f[0] and subItems[1]==f[1]:\n","            remarks_key.append(items[0])\n","  print(remarks_key)\n","# df = pd.DataFrame(remarks_mecab)\n","# pathout='/content/mount/My Drive/00_work/guidance/remarks_keys.csv'\n","# df.to_csv(pathout, header=None,index=None)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"446x5Co2ay4g","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"nRrejBf1BRkt","colab_type":"text"},"source":["## word2vec"]},{"cell_type":"code","metadata":{"id":"j3vSj37ZhC9J","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"3252a91d-f9b4-4c4a-cf5a-32a0f12c89b8"},"source":["from gensim.models import word2vec\n","import subprocess\n","import MeCab as mc\n","def get_words(text):\n","    cmd='echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"'\n","    path = (subprocess.Popen(cmd, stdout=subprocess.PIPE,\n","                              shell=True).communicate()[0]).decode('utf-8')\n","    mt=mc.Tagger(\"-d {0}\".format(path))\n","    mt.parse('')\n","\n","    parsed = mt.parseToNode(text)\n","    components = []\n","\n","    while parsed:\n","        components.append(parsed.surface)\n","        parsed = parsed.next\n","    components = [i for i in components if i !='']\n","    return components\n","pathout='/content/mount/My Drive/00_work/guidance/w2vmodel/ja-gensim.50d.data.model'\n","model_w2v = word2vec.Word2Vec.load(pathout) # path of w2v model\n","text = \"天気がいいから散歩しましょう\"\n","word_list = get_words(text)\n","wv=model_w2v[word_list[0]]\n","print(word_list)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['天気', 'が', 'いい', 'から', '散歩', 'し', 'ましょ', 'う']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"geQRqgF7ls5Z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"b9b86e14-a1ca-4e74-c20e-47c352e67273"},"source":["text = \"うつ鬱抑鬱\"\n","get_words(text)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['うつ', '鬱', '抑鬱']"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"R5Azk1dl_UUD","colab_type":"text"},"source":["# USE univeral sentence "]},{"cell_type":"markdown","metadata":{"id":"D_5NPh5u8cNe","colab_type":"text"},"source":["## USE test"]},{"cell_type":"code","metadata":{"id":"os_-Uq4JWqTx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"outputId":"3103de63-eb99-4159-cfe0-63138a04c872"},"source":["import tensorflow_hub as hub\n","import numpy as np\n","import tensorflow_text\n","# Some texts of different lengths.\n","english_sentences = [\"dog\", \"Puppies are nice.\", \"I enjoy taking long walks along the beach with my dog.\"]\n","italian_sentences = [\"cane\", \"I cuccioli sono carini.\", \"Mi piace fare lunghe passeggiate lungo la spiaggia con il mio cane.\"]\n","japanese_sentences = [\"犬\", \"子犬はいいです\", \"私は犬と一緒にビーチを散歩するのが好きです\"]\n","\n","embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")\n","\n","# Compute embeddings.\n","en_result = embed(english_sentences)\n","it_result = embed(italian_sentences)\n","ja_result = embed(japanese_sentences)\n","\n","# Compute similarity matrix. Higher score indicates greater similarity.\n","similarity_matrix_it = np.inner(en_result, it_result)\n","similarity_matrix_ja = np.inner(en_result, ja_result)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:5 out of the last 7 calls to <function recreate_function.<locals>.restored_function_body at 0x7f4119e9dc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:5 out of the last 7 calls to <function recreate_function.<locals>.restored_function_body at 0x7f4119e9dc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:6 out of the last 8 calls to <function recreate_function.<locals>.restored_function_body at 0x7f411c7ad620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:6 out of the last 8 calls to <function recreate_function.<locals>.restored_function_body at 0x7f411c7ad620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:7 out of the last 9 calls to <function recreate_function.<locals>.restored_function_body at 0x7f411d9501e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:7 out of the last 9 calls to <function recreate_function.<locals>.restored_function_body at 0x7f411d9501e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"VsiQOTFz5UeR","colab_type":"code","colab":{}},"source":["import tensorflow_hub as hub\n","import numpy as np\n","import tensorflow_text\n","embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s0sSYaon5bHc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"218f313e-5a33-4d43-9d9c-3566c5a88587"},"source":[""],"execution_count":null,"outputs":[{"output_type":"stream","text":["(1, 512)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"z9EGCZSRbllh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"6c9ddc0f-9b90-4ced-8811-e91f7ea333b5"},"source":["from sklearn import preprocessing\n","def minmax_scale(x):\n","  return preprocessing.minmax_scale(x, axis=1)\n","english_sentences = \"Puppies are nice.\"\n","japanese_sentences = \"子犬はいいです\"\n","# Compute embeddings.\n","en_result = embed(english_sentences)\n","ja_result = embed(japanese_sentences)\n","\n","ja_result=np.squeeze(ja_result)\n","en_result=np.squeeze(en_result)\n","print(ja_result.shape,wv.shape)\n","ja_result_n=np.concatenate([minmax_scale(ja_result),minmax_scale(wv)],axis=0)\n","en_result_n=np.concatenate([minmax_scale(en_result),minmax_scale(wv)],axis=0)\n","# similarity_matrix_ja_n = np.inner(ja_result_n, en_result_n)\n","# print(similarity_matrix_ja_n)\n","print(ja_result_n.shape)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(512,) (50,)\n","(562,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rzQjj7jaRGig","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"1a60765b-96cf-4098-b41b-6efec7ab8cfc"},"source":["print(ja_result.shape,wv.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(1, 512) (50,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_uWzzDiF692C","colab_type":"text"},"source":["## 類似度計算"]},{"cell_type":"code","metadata":{"id":"HI153RrQk--4","colab_type":"code","colab":{}},"source":["# Pearson Correlation　最大値を採用　●\n","def pearson_coef_use_mat(x,y):\n","  ret=[]\n","  for xs in x:\n","    rety=[]\n","    for ys in y:\n","      x_=xs-np.mean(xs)\n","      y_=ys-np.mean(ys)\n","      d1=np.dot(x_,y_)/(np.linalg.norm(x_)*np.linalg.norm(y_))\n","      rety.append(d1)\n","    ret.append(rety)\n","  return ret\n","#Euclidean Distance 最小値を採用　● 1-x\n","def euclidean_distance_use_mat(x,y):\n","  ret=[]\n","  for xs in x:\n","    rety=[]\n","    for ys in y:\n","      d1=np.sqrt(np.sum(np.square(xs-ys)))\n","      rety.append(d1)\n","    ret.append(rety)\n","  return ret\n","#Cosine Similarity　●\n","def cosine_use_mat(x,y):\n","  ret=[]\n","  for xs in x:\n","    rety=[]\n","    for ys in y:\n","      d1=np.dot(xs,ys)/(np.linalg.norm(xs)*np.linalg.norm(ys))\n","      rety.append(d1)\n","    ret.append(rety)\n","  return ret\n","# Jaccard Similarity ×\n","def jaccard_use_mat(x,y):\n","  ret=[]\n","  for xs in x:\n","    rety=[]\n","    for ys in y:\n","      up=np.double(np.bitwise_and((xs != ys),np.bitwise_or(xs != 0, ys != 0)).sum())\n","      down=np.double(np.bitwise_or(xs != 0, ys != 0).sum())\n","      d1=(up/down)\n","      rety.append(d1)\n","    ret.append(rety)\n","  return ret\n","# Pearson Correlation　最大値を採用　●\n","def pearson_coef_use(x,y):\n","  x_=x-np.mean(x)\n","  y_=y-np.mean(y)\n","  return np.dot(x_,y_)/(np.linalg.norm(x_)*np.linalg.norm(y_))\n","#Euclidean Distance 最小値を採用　● 1-x\n","def euclidean_distance_use(x,y):\n","  return np.sqrt(np.sum(np.square(x-y)))\n","#Cosine Similarity　●\n","def cosine_use(xs,ys):\n","  return np.dot(xs,ys)/(np.linalg.norm(xs)*np.linalg.norm(ys))\n","# Jaccard Similarity ×\n","def jaccard_use(xs,ys):\n","  up=np.double(np.bitwise_and((xs != ys),np.bitwise_or(xs != 0, ys != 0)).sum())\n","  down=np.double(np.bitwise_or(xs != 0, ys != 0).sum())\n","  d1=(up/down)\n","  return d1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UlB-uveZ_hLZ","colab_type":"code","colab":{}},"source":["print(pearson_coef_use(en_result_n,ja_result_n))\n","print(euclidean_distance_use(en_result_n,ja_result_n))\n","print(cosine_use(en_result_n,ja_result_n))\n","print(jaccard_use(en_result_n,ja_result_n))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C0KekAdD6nT_","colab_type":"text"},"source":["## USE + w2v import"]},{"cell_type":"code","metadata":{"id":"4Bqdgl0J-OxA","colab_type":"code","colab":{}},"source":["import MeCab  as mc\n","import subprocess\n","import json\n","import pandas as pd\n","import tensorflow_hub as hub\n","import numpy as np\n","import tensorflow_text\n","import readline\n","from gensim.models import word2vec\n","import MeCab\n","import re\n","import warnings\n","from sklearn import preprocessing\n","import pke\n","pke.base.ISO_to_language['ja_ginza'] = 'japanese'\n","import ginza\n","import nltk\n","stopwords = list(ginza.STOP_WORDS)\n","nltk.corpus.stopwords.words_org = nltk.corpus.stopwords.words\n","nltk.corpus.stopwords.words = lambda lang : stopwords if lang == 'japanese' else nltk.corpus.stopwords.words_org(lang)\n","cmd='echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"'\n","path = (subprocess.Popen(cmd, stdout=subprocess.PIPE,shell=True).communicate()[0]).decode('utf-8')\n","m=mc.Tagger(\"-d {0}\".format(path))\n","warnings.simplefilter('ignore')\n","embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")\n","pathout='/content/mount/My Drive/00_work/guidance/w2vmodel/ja-gensim.50d.data.model'\n","model_w2v = word2vec.Word2Vec.load(pathout) # path of w2v model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DUjmq-MJ9Oze","colab_type":"code","colab":{}},"source":["# model_w2v = word2vec.Word2Vec.load(pathout) # path of w2v model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jgkAz6C24KE7","colab_type":"text"},"source":["## USE + w2v body"]},{"cell_type":"code","metadata":{"id":"7i6RMvwJ6sRh","colab_type":"code","colab":{}},"source":["filterList=[\n","['動詞','自立'],\n","['名詞','サ変接続'],\n","['名詞','ナイ形容詞語幹'],\n","['名詞','一般'],\n","['名詞','固有名詞'],\n","['名詞','形容動詞語幹'],\n","['形容詞','自立']]\n","def get_key_word(remark):  \n","  lines=m.parse(remark).split('\\n')\n","  remarks_key=[]\n","  for line in lines:\n","    items = re.split('[\\t]',line)\n","    if len(items) > 1:\n","      subItems= items[1].split(',')\n","      for f in filterList:\n","        if subItems[0]==f[0] and subItems[1]==f[1] and check_by_regex(items[0]):\n","            remarks_key.append(items[0])\n","  res=[]\n","  # delete duplicate data\n","  [res.append(x) for x in remarks_key if x not in res]\n","  # print(len(res),remark)\n","  return np.array(res)\n","cmd='echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"'\n","path = (subprocess.Popen(cmd, stdout=subprocess.PIPE,\n","                           shell=True).communicate()[0]).decode('utf-8')\n","m=mc.Tagger(\"-d {0}\".format(path))\n","pathout='/content/mount/My Drive/00_work/guidance/20200701/04_word/'\n","json_remark = json.load(open(pathout+\"sentence.json\", \"r\"))\n","remarks=[]\n","remarks_mecab=[]\n","remarks_keys=[]\n","def extract_keywords(text,cnt=15):\n","  # extractor = pke.unsupervised.MultipartiteRank()\n","  extractor = pke.unsupervised.PositionRank() #ok\n","  extractor.load_document(input=text, language='ja_ginza', normalization=None)\n","  #次にフレーズ候補を形成する品詞を spaCy の定数で指定します。以下は名詞、固有名詞、形容詞、数を指定しています。\n","  extractor.candidate_selection(pos={'NOUN', 'PROPN', 'ADJ', 'VERB'})\n","  # extractor.candidate_weighting(threshold=0.74, method='average', alpha=1.1)\n","  extractor.candidate_weighting()\n","\n","  extra=extractor.get_n_best(n=cnt)\n","  wordList=[]\n","  for word in extra:\n","    wordList.extend(word[0].split())\n","  res=[]\n","  # delete duplicate data\n","  [res.append(x) for x in wordList if x not in res]\n","  # print(res)\n","  return res\n","def mt_min_max_normal(mt):\n","  mt=np.array(mt,dtype=np.float32)\n","  mt_max=np.max(mt)\n","  mt_min=np.min(mt)\n","  # mt_mean=np.mean(mt)\n","  # mt_res=(mt-mt_mean)/(mt_max-mt_min)\n","  mt_res=(mt-mt_min)/(mt_max-mt_min)\n","  return list(mt_res)\n","def minmax_scale(x):\n","  return preprocessing.minmax_scale(x)\n","\n","# 正規表現で調べる方法：データをクレンジング\n","def check_by_regex(s):\n","  ret= bool(re.search(r\"[a-zA-Z]\", s)) or \\\n","           bool(re.search(r\"[0-9]\", s))\n","  return not ret\n","def w2v_keywords(words,cnt=10):\n","  ret_vec=[]\n","  eCnt=0\n","  i=-1\n","  for i,wd in zip(range(cnt),words):\n","    if wd in model_w2v:\n","      wv=model_w2v[wd]\n","      ret_vec=np.concatenate([ret_vec,wv],axis=0)\n","    else:\n","      eCnt=eCnt+1\n","  for j in range(i-eCnt+1,cnt):\n","    ret_vec=np.concatenate([ret_vec,np.zeros(50)],axis=0)\n","  return ret_vec\n","\n","def create_use_w2v_vec(in_sentence,keyCnt=10):\n","    keywords=get_key_word(in_sentence)\n","    # print(len(keywords),in_sentence)\n","    keywords_c=''.join(map(str, keywords))\n","    use_vec=embed(keywords_c)\n","    use_vec=np.squeeze(use_vec)\n","    w2v_vec=w2v_keywords(keywords,keyCnt)\n","    # keywords_vec=np.concatenate([minmax_scale(use_vec),minmax_scale(w2v_vec)],axis=0)\n","    keywords_vec=np.concatenate([use_vec,w2v_vec],axis=0)\n","    return keywords_vec\n","\n","def create_single_use_w2v_vec(in_sentence,index,flg):\n","  w2v_vec=[]\n","  keywords=get_key_word(in_sentence)\n","  keywords_gi=' '.join(map(str, keywords))\n"," \n","  if len(keywords)>10:\n","     print(in_sentence)\n","     keywords_ginza=extract_keywords(in_sentence,30)\n","     print('keywords_ginza：',len(keywords),len(keywords_ginza))\n","  else:\n","    keywords_ginza=keywords\n","  keywords_c=''.join(map(str, keywords))\n","  use_vec=embed(keywords_c)\n","  use_vec=np.squeeze(use_vec)\n","  ret_use=[use_vec,index,flg]\n","  # w2v_vec=w2v_keywords2(keywords)  \n","  for wd in keywords_ginza:\n","    if wd in model_w2v:\n","      wv=model_w2v[wd]\n","      w2v_vec.append([wv,index,flg])\n","  if len(w2v_vec)==0:\n","    w2v_vec.append([np.zeros(50),index,flg])\n","  \n","  return ret_use,w2v_vec\n","def w2v_concat(vecs,cnt=10):\n","  ret_vec=[]\n","  i=-1\n","  for i,wv in zip(range(cnt),vecs):\n","      ret_vec=np.concatenate([ret_vec,wv],axis=0)\n","  for j in range(i+1,cnt):\n","    ret_vec=np.concatenate([ret_vec,np.zeros(50)],axis=0)\n","  return ret_vec\n","def vec_nor_concat(w2v_vecs,use_vecs,cnt=10):\n","  uvList=np.array(use_vecs)[:,0]\n","  use_vec_cat=mt_min_max_normal(list(uvList))\n","\n","  w2v_vec_cat=[]\n","  wvList=np.array(w2v_vecs)[:,0]\n","  wvList_nor=mt_min_max_normal(list(wvList))\n","  # print(wvList_nor[10])\n","  df=pd.DataFrame(w2v_vecs)\n","  df[0]=list(wvList_nor)\n","  df.columns=['vec','index','cls']\n","  gdf=df.groupby(['cls','index'])\n","  for ig in gdf:\n","    vec_cat=w2v_concat(ig[1]['vec'],cnt)\n","    w2v_vec_cat.append(vec_cat)\n","    # print(ig[0])\n","  \n","  return use_vec_cat,w2v_vec_cat\n","def similarity_cal(col,pathout,keyCnt=10):  \n","  json_remark = json.load(open(pathout+\"sentence.json\", \"r\"))\n","  json_title = json.load(open(pathout+\"word_\"+col+\".json\", \"r\"))\n","  outFile=[]\n","  pearsonf_use_outfile=[]\n","  euclidea_use_outfile=[]\n","  cosinedi_use_outfile=[]\n","  \n","  use_vecs=[]\n","  w2v_vecs=[]\n","  cnt_r=len(json_remark)\n","  for i,remark in zip(range(cnt_r),json_remark):\n","    use_vec,w2v_vec=create_single_use_w2v_vec(remark['sentence'],i,0)\n","    use_vecs.append(use_vec)\n","    w2v_vecs.extend(w2v_vec)  \n","  cnt_t=len(json_title)\n","  for i, title in  zip(range(cnt_t),json_title): \n","    guidance=title['word']+\"。\"+title[col]\n","    use_vec,w2v_vec=create_single_use_w2v_vec(guidance,i,1)\n","    use_vecs.append(use_vec)\n","    w2v_vecs.extend(w2v_vec) \n","  use_vec_cat,w2v_vec_cat=vec_nor_concat(w2v_vecs,use_vecs,keyCnt)\n","\n","  vec_cat=np.concatenate([use_vec_cat,w2v_vec_cat],axis=1)\n","  remark_vecs=vec_cat[0:cnt_r]\n","  guianc_vecs=vec_cat[cnt_r:]\n","  for remark,remark_vec in zip(json_remark,remark_vecs):\n","    triFil=[]\n","    for title , guidanc_vec in zip(json_title,guianc_vecs):\n","        # print(remark_vec.shape,remark_vec)\n","        # print(guidanc_vec.shape,guidanc_vec)\n","        # return\n","        p_use=pearson_coef_use(remark_vec,guidanc_vec)\n","        e_use=euclidean_distance_use(remark_vec,guidanc_vec)\n","        e_use=1-np.log10(e_use)\n","        c_use=cosine_use(remark_vec,guidanc_vec)\n","        # j_use=jaccard_use(remark_vec,guidanc_vec)\n","        # print(np.round(p_use,3),np.round(e_use,3),np.round(c_use,3))\n","        triFil.append([remark[\"sentence\"],title['word'],title[col],p_use,e_use,c_use])\n","        outFile.append([remark[\"sentence\"],title['word'],title[col],p_use,e_use,c_use])\n","    # return triFil\n","\n","    # pearson_coef_use\n","    # triFil.sort(key=lambda x:x[3],reverse=True)\n","    # triFilT=np.array(triFil)\n","    # # print(triFilT[0:3,[0,1,2,3]])    \n","    # pearsonf_use_outfile.extend(triFilT[0:3,[0,1,2,3]])\n","    \n","    def getmax3(dtList,i):\n","      dtList.sort(key=lambda x:x[i],reverse=True)\n","      triFilT=np.array(dtList)\n","      return triFilT[0:3,[0,1,2,i]]\n","    \n","    # pearson_coef_use\n","    i=3\n","    pearsonf_use_outfile.extend(getmax3(triFil,i))\n","    # euclidean_distance_use\n","    i=4    \n","    euclidea_use_outfile.extend(getmax3(triFil,i))\n","\n","    # cosine_distance_use\n","    i=5\n","    cosinedi_use_outfile.extend(getmax3(triFil,i))\n","\n","\n","  df = pd.DataFrame(pearsonf_use_outfile, columns=['sentence','word',col,'pearson'])\n","  df.to_csv(outputdir+\"pearson\"+\"_similarity_\"+col+\".csv\",index=None)\n","\n","  df = pd.DataFrame(euclidea_use_outfile, columns=['sentence','word',col,'euclidean'])\n","  df.to_csv(outputdir+\"euclidean\"+\"_similarity_\"+col+\".csv\",index=None)\n","\n","  df = pd.DataFrame(cosinedi_use_outfile, columns=['sentence','word',col,'cosine'])\n","  df.to_csv(outputdir+\"cosine\"+\"_similarity_\"+col+\".csv\",index=None)\n","\n","  df = pd.DataFrame(outFile, columns=['sentence','word',col,'pearson','euclidean','cosine'])\n","  df.to_csv(outputdir+\"00_\"+col+\"_similarity_\"+\"raw_all.csv\",index=None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bo-bs2pQ9sh2","colab_type":"code","colab":{}},"source":["# 01_title\n","import re\n","import warnings\n","warnings.simplefilter('ignore')\n","columns='all'\n","outputdir='/content/mount/My Drive/00_work/guidance/output/'\n","path='/content/mount/My Drive/00_work/guidance/20200701/05_all/'\n","triFile=similarity_cal(columns,path,30)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hx1MnX5OUZAE","colab_type":"code","colab":{}},"source":["triFile.sort(key=lambda x:x[3],reverse=True)\n","triFileT=np.array(triFile)\n","print(triFileT[0:3,[0,1,2,3]])    \n","# pearsonf_use_outfile.extend(triFileT[0:3,[0,1,2,3]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cwIrEeNFLg69","colab_type":"code","colab":{}},"source":["%mkdir '/content/mount/My Drive/00_work/guidance/output20'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R4Ar3ci633cS","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DohD94DL3-oS","colab_type":"text"},"source":["## USE+W2V run"]},{"cell_type":"code","metadata":{"id":"DdkPuX7_IAjd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"dd5e58a7-dc43-43ea-89e1-44c25eddfc57"},"source":["# 01_title\n","keycnt=10\n","outputdir='/content/mount/My\\ Drive/00_work/guidance/output_ginza_'+str(keycnt)+'/'\n","%mkdir {outputdir}\n","outputdir='/content/mount/My Drive/00_work/guidance/output_ginza_'+str(keycnt)+'/'\n","\n","\n","#04_word\n","columns='synonyms'\n","path='/content/mount/My Drive/00_work/guidance/20200701/04_word/'\n","similarity_cal(columns,path,keycnt)\n","\n","columns='title'\n","path='/content/mount/My Drive/00_work/guidance/20200701/01_title/'\n","similarity_cal(columns,path,keycnt)\n","#02_confirm\n","columns='confirmation'\n","path='/content/mount/My Drive/00_work/guidance/20200701/02_confirm/'\n","similarity_cal(columns,path,keycnt)\n","\n","#03_detail\n","columns='detail'\n","path='/content/mount/My Drive/00_work/guidance/20200701/03_detail/'\n","similarity_cal(columns,path,keycnt)\n","\n","\n","#05_all\n","columns='all'\n","path='/content/mount/My Drive/00_work/guidance/20200701/05_all/'\n","similarity_cal(columns,path,keycnt)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["発端は母が第三者に身体を許してたことからはじまり小学校を入学して直ぐから去年の夏頃から二カ月の間まで長い間当たり前の様に第三者から身体を弄られ性行為をしないと言葉による暴言や去年の夏の終わりから首の付け根を抑えこまれ息が出来なくなり死ぬかもしれない暴力を受けました\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 20 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 28 21\n","またいつものように急に殴りにきて、私に馬乗りになり、両手を捕まれ叩かれてたときに、やっぱり鼻血が出て、いつもは横に流れるんですけど、そのとき抵抗をしてたので口元に流れてき、母の手を一瞬ほどいて鼻血をトレーナーで拭いてしまったら、そのとき赤とグレーの色のトレーナーで、拭いたところがなぜか運悪くグレーのとこで、まぁ服を汚したらど突かれるんですけど、なぜグレーに汚す!とよけいキレられ、殴られ続けてぐったり\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 18 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 34 18\n","そしたら急に父の顔色が変わり、私の頭にオムライスをかけ、まだ熱をもっているフライパンで私を叩いたのです\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 6 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 11 6\n","暴力やネグレクトはなかったものの、支配的な接し方をされたり、理不尽に当たられたり、無視されたり、学業でしか評価してもらえなかったりといったような子供時代でした\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 8 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 14 11\n","また、ネグレクトで常にカビの生えた生乾きの下着や服を着ており、お風呂は週一、ゴミ屋敷の中でコンビニ弁当を食べたりと、精神的、身体的に毎日ボロボロでした\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 13 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 14 16\n","食事はまともに食べさせてもらえず、痩せすぎて、近所ではあの家の子供は上の子の方は太ってるのに下の子は痩せすぎてるし、しょっちゅう叫び声がする(暴力されるから)虐待だと言っているそうだとクラスメイトに教えてもらった\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 12 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 20 12\n","食事も別、母や兄弟に話しかけても無視されあまりにも辛く家に居場所がなく小学生で自殺を企てた事もあります\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 8 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 15 10\n","小学高学年になってから、父親は、私がお風呂に入っているのを覗きにきて、性器を見せるようにいうようになりました\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 4 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 12 7\n","祖母はよく私の部屋にノックなしで入ってきて、私が薬を塗るため服を脱いでいた事があったのですが、「あの子おっぱい丸出しでいたよ」と父と祖父もいるリビングでわざわざ言いました\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 10 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 20 14\n","裸になった父親が抱きついてきて勃起しているものを私に押し付けてきたり、トイレに入るとどんどんドアを叩き、トイレを使わせないなど、思春期の私はすごく辛く怖い思いをしました\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 6 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 17 6\n","スカートの脚をじろじろ見てくる、両親のセックスの場面を見せようとする、風呂から上がるタイミングで脱衣所の洗濯機置き場に度々いる\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 9 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 15 10\n","この病は育児への理想と現実のギャップから母親がノイローゼになってしまったり、育児による辛さや不安をまわりに吐き出せないままストレスを抱えてしまうことからなると考えられています\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 11 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 16 13\n","(おもに父)母は転職癖がある父のかわりにフルで外で働いていたため私の養育はおもに父と祖母がすることになりました\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 9 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 13 10\n","暴力やネグレクトはなかったものの、支配的な接し方をされたり、理不尽に当たられたり、無視されたり、学業でしか評価してもらえなかったりといったような子供時代でした\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 8 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 14 11\n","弟は小学生のころにアスペルガー症候群と発覚し、他人とは違う行動をするため、学校から電話がかかってき、それを聞いた母は毎日怒鳴っています\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 11 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 15 12\n","DV保護法、その支援策は「もっともだ」と思っていましたが、実際自分が当事者になると、自分が全く反論も証明もする機会を与えられず、DV・虐待の加害者に…\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 11 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 14 13\n","怒鳴。怒鳴大声怒声怒鳴り声泣き声罵声罵倒罵るにらむ馬鹿アホのろま死ね殺すぞうざい消えろボケ叫言葉の暴力\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 3 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 20 6\n","暴力。暴力暴行殴噛む監禁刺投げ絞め引っ張ビンタ平手手を上げ窒息蹴\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 5 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 14 9\n","発端は母が第三者に身体を許してたことからはじまり小学校を入学して直ぐから去年の夏頃から二カ月の間まで長い間当たり前の様に第三者から身体を弄られ性行為をしないと言葉による暴言や去年の夏の終わりから首の付け根を抑えこまれ息が出来なくなり死ぬかもしれない暴力を受けました\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 20 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 28 21\n","またいつものように急に殴りにきて、私に馬乗りになり、両手を捕まれ叩かれてたときに、やっぱり鼻血が出て、いつもは横に流れるんですけど、そのとき抵抗をしてたので口元に流れてき、母の手を一瞬ほどいて鼻血をトレーナーで拭いてしまったら、そのとき赤とグレーの色のトレーナーで、拭いたところがなぜか運悪くグレーのとこで、まぁ服を汚したらど突かれるんですけど、なぜグレーに汚す!とよけいキレられ、殴られ続けてぐったり\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 18 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 34 18\n","そしたら急に父の顔色が変わり、私の頭にオムライスをかけ、まだ熱をもっているフライパンで私を叩いたのです\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 6 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 11 6\n","暴力やネグレクトはなかったものの、支配的な接し方をされたり、理不尽に当たられたり、無視されたり、学業でしか評価してもらえなかったりといったような子供時代でした\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 8 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 14 11\n","また、ネグレクトで常にカビの生えた生乾きの下着や服を着ており、お風呂は週一、ゴミ屋敷の中でコンビニ弁当を食べたりと、精神的、身体的に毎日ボロボロでした\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 13 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 14 16\n","食事はまともに食べさせてもらえず、痩せすぎて、近所ではあの家の子供は上の子の方は太ってるのに下の子は痩せすぎてるし、しょっちゅう叫び声がする(暴力されるから)虐待だと言っているそうだとクラスメイトに教えてもらった\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 12 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 20 12\n","食事も別、母や兄弟に話しかけても無視されあまりにも辛く家に居場所がなく小学生で自殺を企てた事もあります\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 8 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 15 10\n","小学高学年になってから、父親は、私がお風呂に入っているのを覗きにきて、性器を見せるようにいうようになりました\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 4 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 12 7\n","祖母はよく私の部屋にノックなしで入ってきて、私が薬を塗るため服を脱いでいた事があったのですが、「あの子おっぱい丸出しでいたよ」と父と祖父もいるリビングでわざわざ言いました\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 10 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 20 14\n","裸になった父親が抱きついてきて勃起しているものを私に押し付けてきたり、トイレに入るとどんどんドアを叩き、トイレを使わせないなど、思春期の私はすごく辛く怖い思いをしました\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 6 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 17 6\n","スカートの脚をじろじろ見てくる、両親のセックスの場面を見せようとする、風呂から上がるタイミングで脱衣所の洗濯機置き場に度々いる\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 9 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 15 10\n","この病は育児への理想と現実のギャップから母親がノイローゼになってしまったり、育児による辛さや不安をまわりに吐き出せないままストレスを抱えてしまうことからなると考えられています\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 11 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 16 13\n","(おもに父)母は転職癖がある父のかわりにフルで外で働いていたため私の養育はおもに父と祖母がすることになりました\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 9 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 13 10\n","暴力やネグレクトはなかったものの、支配的な接し方をされたり、理不尽に当たられたり、無視されたり、学業でしか評価してもらえなかったりといったような子供時代でした\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 8 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 14 11\n","弟は小学生のころにアスペルガー症候群と発覚し、他人とは違う行動をするため、学校から電話がかかってき、それを聞いた母は毎日怒鳴っています\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 11 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 15 12\n","DV保護法、その支援策は「もっともだ」と思っていましたが、実際自分が当事者になると、自分が全く反論も証明もする機会を与えられず、DV・虐待の加害者に…\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 11 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 14 13\n","アザ。受傷部位、受傷頻度※受傷理由頭部へのあざの有無※入院加療が必要な外傷の有無\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 8 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 12 11\n","痣。受傷部位、受傷頻度※受傷理由頭部へのあざの有無※入院加療が必要な外傷の有無\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 8 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 12 11\n","首。受傷の程度、頻度、児童・保護者への確認※入院加療が必要な外傷の有無\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 10 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 12 11\n","怪我。受傷部位、受傷の程度、児童・保護者への確認※入院加療が必要な外傷の有無\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 10 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 12 11\n","ケガ。受傷部位、受傷の程度、児童・保護者への確認※入院加療が必要な外傷の有無\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 10 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 12 11\n","けが。受傷部位、受傷の程度、児童・保護者への確認※入院加療が必要な外傷の有無\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 10 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 12 11\n","やけど。最重要入院を必要とする頭部・腹部以外の外傷・火傷がある重度通院を必要とする頭部・腹部以外の外傷・火傷がある中度通院を必要とするほどではないが、治療が必要な外傷・火傷がある\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 9 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 15 12\n","火傷。最重要入院を必要とする頭部・腹部以外の外傷・火傷がある重度通院を必要とする頭部・腹部以外の外傷・火傷がある中度通院を必要とするほどではないが、治療が必要な外傷・火傷がある\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 8 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 14 11\n","発育。発育不全（身長・体重）障害（身体・発達・知的）手帳の有無、慢性の持病（アトピー。喘息）、発達的遅れ、極小未熟児など。虐待の結果からくる胃痛、頭痛など。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 19 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 21 20\n","しがみつき。不安・恐れ。欝的な症状、暗い表情、執拗なスキンシップや、しがみつき。極端に大人の顔をみる。大人を恐れる。笑わない。表情が乏しい。視線が合いにくい。抜毛。睡眠リズムがとれない。自傷行為。バンギング。よく寝る（逃避的）など\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 15 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 26 18\n","おむつかぶれ。おむつかぶれがひどい。身体や衣類の汚れ。風呂にはいらないため異臭。季節に合わない洋服を着させられている。ものが揃わない、保育所休ませがち。健診未受診、予防接種未受診、虫歯が多い。など\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 14 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 21 17\n","多動。感情の起伏が大きい。激しい癇癪を起こす。落ち着きがない。多動。注意をひく行動をする。攻撃的態度。遺尿。過食。異食。性化行動。火遊び。徘徊。万引き。虚言。非行・家出など。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 18 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 25 21\n","自殺。自殺直前のサイン・自殺のほのめかし・自殺計画の具体化・行動、性格、身なりの突然の変化・自傷行為・怪我を繰り返す傾向・アルコールや薬物の乱用・重要な人の最近の自殺・別れの用意（整理整頓大切なものをあげる）・最近の喪失体験・家出\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 24 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 27 27\n","発達障害。発達障害のサイン・おとなしく遊ぶことが難しい、じっとしていられずいつも活動する・しゃべりすぎる・順番を待つのが難しい・他人の会話やゲームに割り込む・学校の勉強でうっかりミスが多い・課題や遊びなどの活動に集中し続けることができない・話しかけられていても聞いていないように見える・やるべきことを最後までやりとげない・課題や作業の段取りが下手・整理整頓が苦手・宿題のように集中力が必要なことを避ける・忘れ物や紛失が多い・気が散りやすい\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 26 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 42 27\n","食べ物。ネグレクトの可能性・出されたおやつを奪い取るようにして食べる。・食べるときにガツガツと必死に食べる。・他の子どもの残りももらって食べる。・長期間、空腹が続くと胃痛が起きて食べたくても食べられない場合がある。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 11 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 19 11\n","PTSD。PTSDでよく見られる症状・突然、つらい記憶がよみがえる・常に神経が張りつめている・記憶を呼び起こす状況や場面を避ける・感覚が麻痺する・いつまでも症状が続く\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 9 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 16 9\n","妊娠。・産婦人科の受診状況確認・パートナーや家族の認識の確認・出産又は中絶の意向の確認・性暴力の可能性\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 12 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 13 14\n","うつ。鬱の症状・食欲がない・体がだるい・疲れやすい・気分が重い・イライラする・集中力がない\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 9 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 12 9\n","産後うつ。産後うつの兆候と症状・気分が落ち込む・赤ちゃんを可愛いと感じない・食欲がなくなる・食べ過ぎてしまう・寝付けなくなる・ひどく疲れて元気がなくなる・以前は楽しんでいたものが楽しめなくなる・イライラしたり、すぐ怒ったりする・良いママではないと不安になる・無気力、羞恥心、罪悪感、自分に価値がないと思う感情に陥る・集中力がなくなる・物事への対応力が落ちる・これらの症状のいずれかが2週間以上も続いている・症状が悪化している、改善していない\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 20 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 40 23\n","薬物。・使い始めた頃より薬の量が増えている・感情的に周囲の人に接することがある・生活に支障が出ている・薬物を使用したことを後悔してもやめられない\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 9 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 17 9\n","てんかん。・発作の持続時間・意識障害の有無・けいれんの部位・顔色・唇の色・唾液がでていたか・呼吸が早くなかったか・脈が早くなかったか\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 13 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 15 15\n","セルフネグレクト。・精神疾患、発達障害に関連する可能性・保護者のストレスは家族や子どもへの暴力として現れる。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 9 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 12 10\n","アスペルガー症候群。アスペルガー症候群・言語や知能の発達に遅れがない・特徴として「ひとり遊びを好む・人とするごっこ遊びが広がりにくい・同じ遊びを繰り返す傾向が強い・行動がパターン化し融通がきかない・他の子どもにあまり関心がない・集団で遊ばない」など\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 19 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 27 21\n","チック。チックのサイン・顔面や東部の繰り返す動き（まばたき・顔をしかめる）・首、肩、胴体繰り返す動き（首を振る、肩をすくめる）・腕や手足の繰り返す動き(繰り返し何かを触る、飛び跳ねる)\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 12 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 19 12\n","吃音。吃音の病状・音のくりかえし（連発）、例：「か、か、からす」・引き伸ばし（伸発）、例：「かーーらす」・ことばを出せずに間があいてしまう（難発、ブロック）、例：「・・・・からす」\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 14 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 16 16\n","自閉症。自閉症の症状・目が合わない・他の子に関心がない・言葉が遅い・一人遊びが多い・指さしをしない・人のまねをしない・名前を呼んでも振り向かない・表情が乏しい・落ち着きがない・かんしゃくが強い\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 15 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 24 15\n","お金。各制度・手当の受給状況の確認・児童手当・児童扶養手当・特別児童扶養手当・障害児福祉手当・寡婦控除・寡夫控除・生活保護・奨学金\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 13 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 15 16\n","シングルマザー。寡婦控除の支給要件(1)　夫と死別し又は夫と離婚した後婚姻をしていない人や夫の生死が明らかでない一定の人(2)　扶養親族である子がいる人(特定の寡婦)(3)　合計所得金額が500万円以下であること。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 15 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 22 19\n","シングルファーザー。寡夫控除の支給要件(1) 合計所得金額が500万円以下であること。(2) 妻と死別し、若しくは妻と離婚した後婚姻をしていないこと又は妻の生死が明らかでない一定の人であること。(3) 生計を一にする子がいること。 この場合の子は、総所得金額等が38万円以下（令和２年分以後は48万円以下）で、他の人の同一生計配偶者や扶養親族になっていない人に限られる。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 20 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 27 27\n","児童扶養手当。 次の①～⑨のいずれかに該当する児童を監護している父、母又は養育者に支給される･父母が婚姻を解消した児童･父又は母が死亡した児童･父又は母が一定程度の障害の状態にある児童･父又は母が生死不明の児童･父又は母から1年以上遺棄されている児童･父又は母が裁判所からのＤＶ保護命令を受けた児童（平成24年８月から）･父又は母が1年以上拘禁されている児童･婚姻によらないで生まれた児童･遺児などで父母がいるかいないかが明らかでない児童\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 20 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 35 24\n","障害児福祉手当。精神又は身体に重度の障害を有するため、日常生活において常時の介護を必要とする状態にある在宅の20歳未満の者に支給される\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 12 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 16 15\n","洋服。・着ているものが毎日同じ。・着ているもののサイズが大きすぎ、小さすぎ。・食べこぼしの跡、袖や襟が垢で黒い。・年齢に関わらず紙おむつをしている。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 12 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 16 13\n","代理ミュンヒアウゼン症候群。・親が内服している薬などを服用させて難病にかかっている、・歩行できないなどの擬似疾患を演出する。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 5 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 17 8\n","内出血。・腹部や太腿内側などの柔らかい組織にある傷は虐待が疑われる。・首に内出血がある場合は首を絞められた可能性を疑う。線上の出血などはその可能性が高い。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 12 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 18 15\n","骨折。虐待の重要な所見。発生状況が保護者の説明と合わない時は強く虐待が疑われる。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 8 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 11 9\n","性的虐待。・子どもが年齢不相応な性的行動をとっている場合、性暴力が潜んでいることを疑うことも必要。・性的虐待の場合には、妊娠の有無、性器の診察や性感染症も検査が必要。性器の外傷や性感染症の存在は性的虐待を強く示唆する。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 15 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 22 17\n","盗み。・生活困窮の可能性を示唆する。・児童のいじめ問題、非行に関連した強要などと関連する可能性を示唆する。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 7 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 12 9\n","ペアレントトレーニング。子育てに取り組むご両親（養育者）が、その役割を積極的に引き受けていくことができるよう、親（養育者）と子どもを支援するために開発されたプログラム\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 10 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 16 13\n","児童福祉司。児童福祉司の任用資格が必要。 この資格は大学で心理学、教育学、社会学のいずれかを専修して卒業後、厚生労働省の定める福祉施設などで1年以上実務経験を積むほか、都道府県知事指定の養成機関を卒業する。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 14 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 23 21\n","借金。・債務整理（借金問題）についての相談先 日本司法支援センター（法テラス）、 日本弁護士連合会等・ヤミ金融についての通報・相談先警察、 消費者生活センター、 日本弁護士連合等・登録貸金業者にかかる苦情・相談先日本貸金業協会、 国民生活センター\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 12 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 22 21\n","児童心理治療施設。軽度の情緒障害を有する児童を、短期間入所または、保護者の下から通わせて、情緒障害を治療し、また退所した者について相談その他の援助を行い自立のための援助を行う施設。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 14 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 20 16\n","パーソナリティ障害。・見捨てられる体験を避けようとする懸命の努力。・理想化と過小評価との両極端を揺れ動く不安定な対人関係。・同一性障害(自己像や自己感覚の不安定さ)。・衝動性によって自己を傷つける可能性のある、浪費薬物常用といった行動。・自殺の脅かし、自傷行為の繰り返し。・著明な感情的な不安定さ。・慢性的な空虚感、退屈。・不適切で激しい怒り。・一過性の妄想的念慮もしくは重症の解離症状。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 25 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 44 37\n","水ぶくれ。受傷部位、受傷頻度※受傷理由入院加療が必要な外傷の有無\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 5 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 11 6\n","ドローン。飛行禁止空域・空港等の周辺の空域・地表又は水面から150メートル以上の高さの空域・人口集中地区等の上空（都内では、ほとんどの地域が該当。）\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 14 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 13 18\n","出会い系サイト。・児童（18歳未満者をいう。）は使ってはいけない。・次のような内容を書き込むことは禁止。　下記(1)から(4)までの書き込みをした者は、　大人でも児童でも処罰の対象(100万円以下の罰金)。　(1)児童に性交等をもちかけること　(2)人に、児童との性交等をもちかけること　(3)対償を示して、児童に性交等以外の異性交際をもちかけること　(4)対償を受けることを示して、人に児童との性交等以外の異性交際をもちかけること　(5)性交等や対償供与が含まれない、児童に係る異性交際を持ちかけること\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 21 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 27 27\n","痴漢。痴漢となる具体的な行為　・衣服や身体に直接触れる。　・背後から密着し、身体や股間を執拗に押しつける。　・エスカレーターや階段で、スカート内を盗撮しようとする。　・スカートなどの衣服を切り裂く(器物損壊罪)　・衣服に精液等を付着させる(器物損壊罪)　・公衆の面前で陰部等を露出する(公然わいせつ罪)　・つきまとい、のぞき(軽犯罪法違反)\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 17 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 32 23\n","児童ポルノ。該当する罪　・児童ポルノを提供、製造等をする行為　・児童ポルノを公然陳列する行為　・児童ポルノを提供目的で所持する行為　・自己の性的好奇心を満たす目的で児童ポルノを所持する行為　・盗撮により児童ポルノを製造する行為\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 10 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 14 12\n","発端は母が第三者に身体を許してたことからはじまり小学校を入学して直ぐから去年の夏頃から二カ月の間まで長い間当たり前の様に第三者から身体を弄られ性行為をしないと言葉による暴言や去年の夏の終わりから首の付け根を抑えこまれ息が出来なくなり死ぬかもしれない暴力を受けました\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 20 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 28 21\n","またいつものように急に殴りにきて、私に馬乗りになり、両手を捕まれ叩かれてたときに、やっぱり鼻血が出て、いつもは横に流れるんですけど、そのとき抵抗をしてたので口元に流れてき、母の手を一瞬ほどいて鼻血をトレーナーで拭いてしまったら、そのとき赤とグレーの色のトレーナーで、拭いたところがなぜか運悪くグレーのとこで、まぁ服を汚したらど突かれるんですけど、なぜグレーに汚す!とよけいキレられ、殴られ続けてぐったり\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 18 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 34 18\n","そしたら急に父の顔色が変わり、私の頭にオムライスをかけ、まだ熱をもっているフライパンで私を叩いたのです\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 6 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 11 6\n","暴力やネグレクトはなかったものの、支配的な接し方をされたり、理不尽に当たられたり、無視されたり、学業でしか評価してもらえなかったりといったような子供時代でした\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 8 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 14 11\n","また、ネグレクトで常にカビの生えた生乾きの下着や服を着ており、お風呂は週一、ゴミ屋敷の中でコンビニ弁当を食べたりと、精神的、身体的に毎日ボロボロでした\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 13 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 14 16\n","食事はまともに食べさせてもらえず、痩せすぎて、近所ではあの家の子供は上の子の方は太ってるのに下の子は痩せすぎてるし、しょっちゅう叫び声がする(暴力されるから)虐待だと言っているそうだとクラスメイトに教えてもらった\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 12 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 20 12\n","食事も別、母や兄弟に話しかけても無視されあまりにも辛く家に居場所がなく小学生で自殺を企てた事もあります\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 8 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 15 10\n","小学高学年になってから、父親は、私がお風呂に入っているのを覗きにきて、性器を見せるようにいうようになりました\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 4 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 12 7\n","祖母はよく私の部屋にノックなしで入ってきて、私が薬を塗るため服を脱いでいた事があったのですが、「あの子おっぱい丸出しでいたよ」と父と祖父もいるリビングでわざわざ言いました\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 10 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 20 14\n","裸になった父親が抱きついてきて勃起しているものを私に押し付けてきたり、トイレに入るとどんどんドアを叩き、トイレを使わせないなど、思春期の私はすごく辛く怖い思いをしました\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 6 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 17 6\n","スカートの脚をじろじろ見てくる、両親のセックスの場面を見せようとする、風呂から上がるタイミングで脱衣所の洗濯機置き場に度々いる\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 9 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 15 10\n","この病は育児への理想と現実のギャップから母親がノイローゼになってしまったり、育児による辛さや不安をまわりに吐き出せないままストレスを抱えてしまうことからなると考えられています\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 11 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 16 13\n","(おもに父)母は転職癖がある父のかわりにフルで外で働いていたため私の養育はおもに父と祖母がすることになりました\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 9 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 13 10\n","暴力やネグレクトはなかったものの、支配的な接し方をされたり、理不尽に当たられたり、無視されたり、学業でしか評価してもらえなかったりといったような子供時代でした\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 8 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 14 11\n","弟は小学生のころにアスペルガー症候群と発覚し、他人とは違う行動をするため、学校から電話がかかってき、それを聞いた母は毎日怒鳴っています\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 11 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 15 12\n","DV保護法、その支援策は「もっともだ」と思っていましたが、実際自分が当事者になると、自分が全く反論も証明もする機会を与えられず、DV・虐待の加害者に…\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 11 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 14 13\n","おむつかぶれ。子どもの衣食住が満足でない。非衛生状態のまま放っておかれている。医療的な放置、監護が十分されていない。放置すると子どもの安全が損なわれると考えられる\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 8 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 16 11\n","けいれん。基礎疾患のない低身長、低体重、低栄養などの医学的所見はネグレクトを疑わせる。表情の欠如などの他の症状がある場合には特に強く疑わなければならない。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 12 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 16 17\n","食べ物。小児の痙攣は、熱性けいれんが最多である。特に乳幼児では、発熱に引き続く熱性けいれんがしばしば見られる。熱性けいれんは6か月  5歳頃に多く、短時間の発作である場合がほとんどである。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 13 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 11 15\n","揺さぶらっ子症候群。ネグレクトを受けている子どもは常に飢餓状態になり、食行動異常が発生する場合がある。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 7 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 14 10\n","アルコール。実際には虐待ではなく、家庭内の事故や突発的な脳出血などの場合もあり、虐待と判定される場合もあると言われている。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 8 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 11 9\n","PTSD。第一に重視されるべきことは、虐待行為の有無と虐待環境か否かであり、子どもの保護である。暴力や暴言、家の中で暴れる等の状況がある場合は、児童福祉法第29条、第33条、第28条、児童虐待防止法第８条、第９条及び第10条の規定に基づき緊急介入を行う。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 21 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 24 23\n","ギャンブル。病理性の高さから支援は困難を極めることも多い。保健、医療、福祉、司法、警察、教育機関との密な連携が不可欠である。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 12 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 16 14\n","不登校。・おねしょのしつけと称して虐待をおこなう事例があり。・おねしょは発達障害の可能性も考慮。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 8 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 11 9\n","引きこもり。文部科学省は不登校を以下の7つのタイプに分類。1. 学校生活上の影響2. あそび・非行3. 無気力4. 不安など情緒的混乱　(1)分離不安によるもの　(2)息切れによるもの　(3)甘やかされによるもの　(4)生活基盤の不安定によるもの5. 意図的な拒否6. 複合不登校状態が継続している理由が複合していて、いずれが主であるかを決めがたい型。いじめと家庭環境など、いくつかの要因が重なって学校に行けなくなった場合がこのタイプ。7. その他\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 27 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 37 31\n","セルフネグレクト。・服薬のルール・発作時の対処方法乳幼児期から高齢期まで、全ての年代で発病します、３歳以下の発病が最も多く、80％は18歳以前に発病すると言われている。最近の傾向では、人口の高齢化に伴い、脳血管障害などが原因となる高齢者の発病が増えている。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 19 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 23 26\n","チック。「LD （Learning Disability）」　全般的な知的発達には問題がないのに、読む、書く、計算するなど特定の事柄のみがとりわけ難しい状態を言う。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 8 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 15 11\n","施設入所。アスペルガー症候群の夫または妻と情緒的な相互関係が築けないために配偶者やパートナーに生じる、身体的・精神的症状。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 11 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 14 16\n","シングルファーザー。寡婦控除夫と死別もしくは離婚した後に婚姻していない、または夫の生死が明らかではない者が」所得税や住民税の控除を受けられる制度\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 11 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 15 12\n","生活保護。寡夫控除妻と死別もしくは離婚した後に婚姻していない、または妻の生死が明らかではない者が」所得税や住民税の控除を受けられる制度\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 11 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 15 12\n","生活扶助。生活に困窮する方に対し、自立を助長することを目的とした制度下記種類に分かれる・生活扶助・住宅扶助・教育扶助・医療扶助・介護扶助・出産扶助・生業扶助・葬祭扶助\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 14 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 20 17\n","特別児童扶養手当。父母の離婚などで、父又は母と生計を同じくしていない児童の養育者へ支給される制度\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 9 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 13 11\n","障害児福祉手当。20歳未満で精神又は身体に障害を有する児童を家庭で監護、養育している父母等に支給される手当\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 10 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 14 13\n","代理ミュンヒアウゼン症候群。ネグレクトを受けている子どもは衣類の状態の不衛生が多く観られる。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 5 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 11 7\n","瘡蓋。発達障害など注意の配分が上手くできない人は、たくさんの感覚を同時に処理することが苦手な場合がある。同時にふたつのことができなかったり、考え事をしていると指示を聞きそびれてしまうことがある。また、注意がそれやすい人は、新しい刺激があるとそれに気が向いてしまい、今していることを忘れてしまうことがある。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 16 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 24 17\n","ダニ。保護者の発達障害、また子どもの部屋がゴミだらけになっているなど様々な状況があるので注意が必要。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 8 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 12 9\n","発端は母が第三者に身体を許してたことからはじまり小学校を入学して直ぐから去年の夏頃から二カ月の間まで長い間当たり前の様に第三者から身体を弄られ性行為をしないと言葉による暴言や去年の夏の終わりから首の付け根を抑えこまれ息が出来なくなり死ぬかもしれない暴力を受けました\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 20 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 28 21\n","またいつものように急に殴りにきて、私に馬乗りになり、両手を捕まれ叩かれてたときに、やっぱり鼻血が出て、いつもは横に流れるんですけど、そのとき抵抗をしてたので口元に流れてき、母の手を一瞬ほどいて鼻血をトレーナーで拭いてしまったら、そのとき赤とグレーの色のトレーナーで、拭いたところがなぜか運悪くグレーのとこで、まぁ服を汚したらど突かれるんですけど、なぜグレーに汚す!とよけいキレられ、殴られ続けてぐったり\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 18 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 34 18\n","そしたら急に父の顔色が変わり、私の頭にオムライスをかけ、まだ熱をもっているフライパンで私を叩いたのです\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 6 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 11 6\n","暴力やネグレクトはなかったものの、支配的な接し方をされたり、理不尽に当たられたり、無視されたり、学業でしか評価してもらえなかったりといったような子供時代でした\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 8 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 14 11\n","また、ネグレクトで常にカビの生えた生乾きの下着や服を着ており、お風呂は週一、ゴミ屋敷の中でコンビニ弁当を食べたりと、精神的、身体的に毎日ボロボロでした\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 13 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 14 16\n","食事はまともに食べさせてもらえず、痩せすぎて、近所ではあの家の子供は上の子の方は太ってるのに下の子は痩せすぎてるし、しょっちゅう叫び声がする(暴力されるから)虐待だと言っているそうだとクラスメイトに教えてもらった\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 12 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 20 12\n","食事も別、母や兄弟に話しかけても無視されあまりにも辛く家に居場所がなく小学生で自殺を企てた事もあります\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 8 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 15 10\n","小学高学年になってから、父親は、私がお風呂に入っているのを覗きにきて、性器を見せるようにいうようになりました\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 4 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 12 7\n","祖母はよく私の部屋にノックなしで入ってきて、私が薬を塗るため服を脱いでいた事があったのですが、「あの子おっぱい丸出しでいたよ」と父と祖父もいるリビングでわざわざ言いました\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 10 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 20 14\n","裸になった父親が抱きついてきて勃起しているものを私に押し付けてきたり、トイレに入るとどんどんドアを叩き、トイレを使わせないなど、思春期の私はすごく辛く怖い思いをしました\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 6 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 17 6\n","スカートの脚をじろじろ見てくる、両親のセックスの場面を見せようとする、風呂から上がるタイミングで脱衣所の洗濯機置き場に度々いる\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 9 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 15 10\n","この病は育児への理想と現実のギャップから母親がノイローゼになってしまったり、育児による辛さや不安をまわりに吐き出せないままストレスを抱えてしまうことからなると考えられています\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 11 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 16 13\n","(おもに父)母は転職癖がある父のかわりにフルで外で働いていたため私の養育はおもに父と祖母がすることになりました\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 9 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 13 10\n","暴力やネグレクトはなかったものの、支配的な接し方をされたり、理不尽に当たられたり、無視されたり、学業でしか評価してもらえなかったりといったような子供時代でした\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 8 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 14 11\n","弟は小学生のころにアスペルガー症候群と発覚し、他人とは違う行動をするため、学校から電話がかかってき、それを聞いた母は毎日怒鳴っています\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 11 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 15 12\n","DV保護法、その支援策は「もっともだ」と思っていましたが、実際自分が当事者になると、自分が全く反論も証明もする機会を与えられず、DV・虐待の加害者に…\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 11 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 14 13\n","自殺。子ども時代に受けた虐待が精神的後遺症 （トラウマ）となって残り、青年期 ・成人期になってからいろいろな問題を引き起こすことは少なくない。これまでの報告からは、うつ症状や自殺企図、アルコール・薬物依存を有する男女では、一般の人よりも虐待された経験を持つことが多いということが分かっている。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 18 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 31 23\n","発達障害。発達障害は、生まれつき脳の発達が通常と違っているために、幼児のうちから症状が現れ、通常の育児ではうまくいかないことがある。成長するにつれ、自分自身のもつ不得手な部分に気づき、生きにくさを感じることがあるかもしれない。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 14 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 23 17\n","けいれん。痙攣とは、不随意に筋肉が激しく収縮することによって起こる発作。痙攣のパターンは多種多様であるが、大きく全身性の場合と体の一部分である場合とに分けることができる。 痙攣を新規に発症した場合には、医療機関を受診することが重要。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 13 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 24 15\n","揺さぶらっ子症候群。揺さぶられっ子症候群（SBS）とは、概ね生後6か月以内の新生児や乳児の体を、過度に揺することで発生する内出血などの外傷。児童虐待ともなりうるもので、乳児揺さぶり症候群ないし乳幼児揺さぶられ症候群ともいう。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 14 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 19 18\n","アルコール。アルコール依存症とは、たばこや薬物等と同じ依存症候群の一つである。アルコールを抑制するコントロールが効かなくなり、脅迫的に飲酒する以外では、酒を手に入れるための行動が大半を占めるようになる。アルコール依存症の家族とともに生活する子どもは、暴れる保護者や殴られる保護者を常時見ているか、あるいは自分も殴られている場合がある。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 21 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 30 24\n","PTSD。心的外傷後ストレス障害。強烈なショック体験、強い精神的ストレスが、こころのダメージとなって、時間がたってからも、その経験に対して強い恐怖を感じるもの。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 10 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 13 13\n","一時保護。虐待されている子どもを保護者から引き離して、生命、身体の安全を確保すること。同意が得られない場合でも、必要に応じて児童相談所の権限で保護することができる。保護の期間は、２ヶ月を超えてはならないとされているが、必要な場合は延長できる。しかし保護者の意思に反する場合は家庭三番所の承認を得る（2017年児童福祉法改正）。原則的に児童相談所の一時保護所を利用する。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 19 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 31 21\n","妊娠。未成年の妊娠は、妊娠の結果、低学歴、無職となり、貧困に陥るケースはよく目にする。不定期な妊婦健診や母体の未熟さゆえに低体重、未熟での出産のリスクが高まり、さらに発達障害、ヘルスリスク、虐待のリスクも飛躍的に高まることが分かっている。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 18 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 24 22\n","自傷。幼少時期に安心して生活することができず、いつも不安や恐怖に脅え、自分を大切な存在であると感じることができずに育ってしまったことで、自己尊重感が築けず、対人関係の築き方にも障害を来たしてしまいがちになる。その為、何とか青年期 ・成人期まで生きてきたとしても、抑うつに陥りやすかったり、ささいなことで不安を強めたり、無気力や自己嫌悪から自傷、自殺企図などを示す場合がある。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 20 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 36 26\n","うつ。子ども時代に受けた虐待が精神的後遺症 （トラウマ）となって残り、青年期 ・成人期になってからいろいろな問題を引き起こすことは少なくない。うつ症状や自殺企図、アルコール・薬物依存を有する男女では、一般の人よりも虐待された経験を持つことが多いということが分かっている。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 17 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 30 22\n","薬物。薬物依存症による幻覚、妄想が、殺人や放火等の凶悪犯罪や、交通事故を引き起こす等周囲の人や社会に対しても取り返しのつかない被害を及ぼすことがある。その環境が、弱者であり無力な子どもにとっては、大変危険な、しかも人権を侵害する場になる可能性が高いことを認識し、薬物依存症への対応の際に、子どもの存在が確認できれば、常に虐待対応を念頭に入れた介入が必要となる。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 28 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 41 30\n","おねしょ。原因は心理的ストレス、膀胱や腎臓の器質的な異常などもあるが、自閉症や発達障害、知的能力障害をもつ子どもは、おねしょをする可能性が高い傾向にある。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 12 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 20 17\n","不登校。「不登校児童生徒」とは「何らかの 心理的、情緒的、身体的あるいは社会的要因・背景により、 登校しないあるいはしたくともできない状況にあるために年間 30日以上欠席した者のうち、病気や経済的な理由による者を 除いたもの」と定義しています。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 16 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 23 20\n","引きこもり。機能不全家族で育った子供は、まず親との人間関係作りに失敗しており、人間関係の基礎が人間不信になっている場合がある。良い子をただ演じている事があり、いじめがきっかけで引きこもりが発生するケースが多い。ひどい場合は解離性障害を発生する。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 16 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 24 22\n","てんかん。種々の成因によってもたらされる慢性の脳疾患であって、大脳ニューロンの過剰な発射に由来する反復性の発作（てんかん発作）を特徴とし、それにさまざまな臨床症状及び検査所見が伴う。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 12 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 22 16\n","セルフネグレクト。種々の成因によってもたらされる慢性の脳疾患であって、大脳ニューロンの過剰な発射に由来する反復性の発作（てんかん発作）を特徴とし、それにさまざまな臨床症状及び検査所見が伴う。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 12 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 23 18\n","アスペルガー症候群。アスペルガー症候群は、広い意味での「自閉症」のひとつのタイプ。アスペルガー症候群は、自閉症の3つの特徴のうち「対人関係の障害」と「パターン化した興味や活動」の2つの特徴を有し、コミュニケーションの目立った障害がないとされている障害。言葉の発達の遅れがないというところが自閉症と異なるところである。知的発達に遅れのある人はほとんどいない。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 20 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 26 24\n","学習障害。「LD （Learning Disability）」　全般的な知的発達には問題がないのに、読む、書く、計算するなど特定の事柄のみがとりわけ難しい状態を言う。有病率は、確認の方法にもよるが2〜10%と見積もられており、読みの困難については、男性が女性より数倍多いと報告されている。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 17 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 27 22\n","チック。急に出現する運動や音声が、繰り返し、不随意に出現する疾患で、比較的よく見られる疾患。原因はわかってはいないが、家族内の発症が多かったり、注意欠陥・多動性障害、強迫性障害に合併することが知られている。ストレスや疲労などで症状が出やすくなることがある。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 16 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 26 20\n","吃音。語頭音を繰り返す（「わ、わ、わたし」）など、話し言葉が滑らかに出ない発話障害。具体的な発生原因は不明。子どもの約５%で発生し、２歳から５歳位の幼児期に始まる子がほとんど。男子に多い。成長と共に改善し、成人後は人口の１％弱となる。人前で話せないことから、仕事や日常生活に大きな支障をきたし、社会に出て行くことが出来ない等の深刻な問題を抱えてしまう人もいる。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 28 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 39 33\n","自閉症。自閉症は「1. 対人関係の障害」「2. コミュニケーションの障害」「3. パターン化した興味や活動」の3つの特徴をもつ障害で、生後まもなくから明らかになる。最近では症状が軽い人たちまで含めて、自閉症スペクトラム障害という呼び方もされている。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 15 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 21 20\n","施設入所。一時保護中の支援方針の選択肢としての施設入所。乳児院、児童養護施設、児童自立支援施設等。原則として保護者の同意、得られない場合は家庭裁判所の承認を得る。措置期間は2年を超えないこと。家庭裁判所の承認で期間更新も可能。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 16 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 21 22\n","里親委託。一時保護中の支援方針の選択肢としての里親への委託。保護者（又は家庭裁判所）の同意が必要。里親は4人以下、「小規模住居方児童養育事業（ファミリーホーム）」は5人以上養育できる。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 12 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 18 16\n","親権。民法第818条「成年に足しない子は、父母の親権に服する」内容は、監護教育権（第820条）、居所指定権（第821条）、懲戒権（第822条）、職業許可権（第823条）、財産管理権及び代表権（第824条）\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 14 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 18 19\n","親権停止。父母に親権の執行が困難、不適当であり、子の利益を害する時に請求できる（民法第834条の2）。原則2年以外。親権停止、親権消失の審判を請求できる。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 13 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 15 15\n","未成年後見制度。後見人が親権者と同一の権利義務で監護・財産管理をおこなう（民法第838条）。法改正（第840条）により法人、また複数の後見人の選任が可能となった。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 13 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 16 16\n","生活扶助。生活保護の種類の一つ。日常生活に必要な費用（食費・被服費・光熱費等）に対する扶助。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 10 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 11 12\n","医療扶助。生活保護の種類の一つ。医療サービスの費用に対する扶助。費用は直接医療機関へ支払(本人負担なし)\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 10 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 12 11\n","介護扶助。生活保護の種類の一つ。介護サービスの費用に対する扶助。費用は直接介護事業者へ支払(本人負担なし)\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 10 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 13 11\n","生業扶助。生活保護の種類の一つ。就労に必要な技能の修得等にかかる費用に対する扶助。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 9 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 11 10\n","代理ミュンヒアウゼン症候群。養護者が元気な子どもを病人にして周囲の気を引こうとする精神疾患。病気やけがを子どもに代理させ、看病する健気な自分を演じて自己満足するというもの。命を脅かす重篤な障害に繋がることもある。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 16 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 26 19\n","性的虐待。性的虐待は、子どもに深刻な精神的問題や行動上の問題を生じさせる危険性が高いと考えられており、早急かつ適切な対応が必要となる。適切な対応を講ずるためには、子どもと虐待を加えていると考えられる保護者との分離が原則となる。　子どもから性的虐待の開示がなされた場合であっても、虐待者とされた保護者がその事実を認めることは少ない。また、子どもの行動や周辺的な状況で性的虐待の疑いを持たれた場合であっても、被害を受けていると考えられる子ども自身がその被害を否認することもある。このように、性的虐待はその事実の確認が非常に困難な場合が少なくなく、それだけに、対応する側に高度な専門性が要求されることになる。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 26 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 44 27\n","三歳児検診。　3歳児の健診は、軽度精神発達遅滞、軽度脳性まひ、斜視、視力障害、難聴などの異常の芽を発見し、早期の治療に結びつけること。そして、言語や認知の発達などの子どもの素因と親からのかかわりなどの環境要因の双方に注目して、広汎性発達障害や注意欠陥・多動性障害など社会性の障害につながる状態への早期の支援や健康な生活習慣の獲得につなげること。さらには、疾病を持つ子どもとその家族、不適切な養育に陥る要因を持った家族に子育て支援の視点でかかわりを持つためのチャンスとするなどの意義がある。　3歳児健診は、3歳0か月から4歳未満のいずれの時期に実施してもよいが、実際は3歳0か月～3歳4か月頃までとする考え方と、3歳6か月以降とする考え方がある。前者は3歳という発達の節目での疾病の早期発見に重点を置き、後者は視覚検査や言語発達などのスクリーニングの精度に重点を置く考え方である。両者とも長所、短所があるので、短所を補う対応に十分配慮したうえで健診時期を決定する（\n","keywords_ginza： 76 36\n","一歳六ヵ月健診。1歳6か月は、発達においては脳幹支配から大脳支配が優位となる時期である。歩行ができ、意味のある単語が言え、他の子どもに興味を示したり、親と一緒に遊ぶことができる。　1歳6か月児の健診は、それ以前に疑われていた中等度以上の発達上の問題を確認するとともに、軽度の精神発達遅滞や脳性まひ、視覚の障害や難聴などの異常の芽を発見し、早期の治療に結びつけること。言語や認知の発達などの子どもの素因と親からのかかわりなどの環境要因の双方に注目して、広汎性発達障害や注意欠陥・多動性障害など社会性の障害につながる状態への早期の支援や健康な生活習慣の獲得につなげること。　さらには、疾病を持つ子どもとその家族、不適切な養育に陥る要因を持った家族に子育て支援の視点でかかわりを持つためのチャンスとするなどの意義がある。\n","keywords_ginza： 73 37\n","ペアレントトレーニング。子育てに取り組むご両親（養育者）が、その役割を積極的に引き受けていくことができるよう、親（養育者）と子どもを支援する。　両親（養育者）は子どもを育てていく経験を通して、親（養育者）として成長していく。その中で、子育ての先輩や支援者の手助けを借りることも役に立つ。現在多くのペアレントプログラムが、支援者および子育ての先輩により実施されている。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 17 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 27 20\n","多重人格。性的虐待がトラウマ性の体験となったり、その後遺症と思われる症状や行動 　（解離性障害、PTSD、抑うつ症状、衝動性のコントロール不全、性化行動、性的逸脱行動など）が認められる場合には、精神科の治療や心理的ケアが必要となる。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 18 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 23 24\n","MSW。医療ソーシャルワーカー社会福祉の立場から患者さんやその家族の方々の抱える経済的・心理的・社会的問題の解決、調整を援助し、社会復帰の促進を図る\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 10 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 18 13\n","過呼吸。自分が意図することなく発作的に呼吸が速くなり、それを止めることができないために血液が過度にアルカリ性に傾き、全身のさまざまな症状を示す症候群。思春期～２０歳代の女性に多くみられる。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 13 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 24 14\n","アダルトチルドレン。子どものとき、こころを傷つけるような言動や暴力のある家庭、子どもらしく自由に振る舞えなかった家庭で育ったことで大人になった今も、こころや人間関係に障害を持つようになった人たちを指す。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 14 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 18 16\n","シラミ。アタマジラミは季節に関係なく被害が起きる。特に保育園児・幼稚園児や小学校低学年に多く見られる。頭髪から落ちたり離れたりした、成虫・幼虫が他の人にうつる。集団で遊んだり、一緒に寝たりといった、接触によりタオルや寝具などを介してうつる。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 18 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 30 21\n","ネズミ。・ねずみの身体にはたくさんの病原菌を含む細菌が付着しており、糞や尿を通じてばらまかれる。イエダニやノミが寄生している。・ネズミに噛まれると、「アナフィラキシーショック」を起こすことがあり得る。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 11 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 18 11\n","リケッチア感染症。主にネズミ、ノミが媒介する発疹熱リケッチアによっておこる感染症で、発熱、頭痛、発疹、関節痛などの症状を認める。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 10 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 15 11\n","バウムテスト。1本の実がなる木を書いてその中に投影される情報を専門家が読み解いていく心理検査。自由に書いてもらった「一本の木」から、　・全体的印象　・樹木の形態　・鉛筆の動き　・樹木の位置の4つの側面から、60項目あまり(全体的所見、風景および付属物、地平、根元、根、幹、枝、冠、果実･花･葉など)から判断し、その人の持つものの考え方、思考のくせ、言葉で表現しにくい内面の気持ち、深層心理などを知るために役立てる。また紙の上に描く木の位置や空間スペース、地面との関係などから、家族関係や環境や世界とのつながりなどを判断することができます。\n","keywords_ginza： 60 35\n","児童福祉司。児童福祉司（じどうふくしし）は、児童相談所に置かなければならない職員で、児童相談所長が定める担当区域により、児童の保護その他児童の福祉に関する事項について相談に応じ、専門的技術に基づいて必要な指導を行うケースワーカー（病気や非行その他の障害等により、社会生活への適応に困難な者又は適応に失敗した者に対して社会的援助活動を行う者）の一種である。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 24 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 33 31\n","児童心理司。児童心理司（じどうしんりし）は、児童相談所において心理学の専門的学識に基づく心理判定業務に携わる職員。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 7 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 13 12\n","借金。債務整理には、主に次の方法がある。 (1)任意整理　 (2)破産手続 (3)個人再生手続 (4)特定調停\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 8 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 11 12\n","義父。義理の親からの虐待は、心理的、経済的な面から外部に発信されづらい場合がある。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 7 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 11 8\n","ノーバディズ・パーフェクト。カナダ発の就学前の子どもを育てる親支援プログラム。０～５歳の子どもの親がグループの中で互いの体験や不安を話しあうことによって、子育てのスキルを高め、自信を取り戻す。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 15 given)\n"],"name":"stderr"},{"output_type":"stream","text":["keywords_ginza： 18 18\n","プレイセラピー。子どもとセラピスト（治療者）の適切で特別な対人関係の中で、安全な環境と遊び道具を使って、子どもが自分の気持ちや考えや行動を表現したり探索したりするのを、プレイセラピストという大人が促進し手伝う。\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Not enough candidates to choose from (30 requested, 14 given)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"rusq_-yMkiC7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"67d7dd03-3970-4c1d-8358-42f85af88c9b"},"source":["def cosine_use(xs,ys):\n","  return np.dot(xs,ys)/(np.linalg.norm(xs, ord=2)*np.linalg.norm(ys, ord=2))\n","remark_vec =[-0.1,0.2,0.3,0.3,0.2,0.2,0.2,0.1,-0.6]\n","guidanc_vec=[0.1,0.1,-0.3,-0.3,0.2,0.2,0.2,0.6,0.1]\n","c=cosine_use(remark_vec,guidanc_vec)\n","print(c)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["-0.07093804422989565\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nEWORSvKpIn7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c26e6188-8fe8-4843-b722-8433fdd6d807"},"source":["remark_vec =[1,2,3]\n","print(np.linalg.norm(remark_vec, ord=2))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["3.7416573867739413\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EiqObIUVuxAM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"60ca4257-6b01-4ed3-e717-426fa1f079b5"},"source":["w2v_vec=[[1,2,3],\n","         [3,2,6],\n","         [1,5,3]]\n","# print(preprocessing.minmax_scale(w2v_vec, axis=1))\n","print(preprocessing.minmax_scale(w2v_vec, axis=0))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[0.   0.5  1.  ]\n"," [0.25 0.   1.  ]\n"," [0.   1.   0.5 ]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CncK_oExv2uR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"e8d2cd46-c63a-45c9-a8bd-9b2c1af9be08"},"source":["\n","print(preprocessing.minmax_scale(w2v_vec, axis=))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[0.   0.5  1.  ]\n"," [0.25 0.   1.  ]\n"," [0.   1.   0.5 ]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wxvykda4g718","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"15eb35a0-013b-4098-e58d-060bda7eec43"},"source":["import numpy as np\n","\n","def mt_min_max_normal(mt):\n","  mt=np.array(mt,dtype=np.float32)\n","  mt_max=np.max(mt)\n","  mt_min=np.min(mt)\n","  mt_res=(mt-mt_min)/(mt_max-mt_min)\n","  return list(mt_res)\n","a=[[2,3],[1,2]]\n","b=[[1,1,2],[2,1,3]]\n","a=np.array(a)\n","b=np.array(b)\n","na = np.empty((2,1,))\n","na[:] = np.nan\n","an=np.concatenate([a,na],axis=1)\n","print(a.shape,b.shape,an.shape)\n","\n","c=np.concatenate([b,an],axis=0)\n","# print(mt_min_max_normal(c))\n","print(np.min(c))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(2, 2) (2, 3) (2, 3)\n","nan\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"n0S9odsoCBw4","colab_type":"text"},"source":["## use test"]},{"cell_type":"code","metadata":{"id":"SR0-3xdCCCW8","colab_type":"code","colab":{}},"source":["  col='title'\n","  pathout='/content/mount/My Drive/00_work/guidance/20200701/01_title/'\n","  json_remark = json.load(open(pathout+\"sentence.json\", \"r\"))\n","  json_title = json.load(open(pathout+\"word_\"+col+\".json\", \"r\"))\n","  \n","  remark_vecs=[]\n","  guianc_vecs=[]\n","  use_vecs=[]\n","  w2v_vecs=[]\n","  cnt=len(json_remark)\n","  for i,remark in zip(range(cnt),json_remark):\n","    use_vec,w2v_vec=create_single_use_w2v_vec(remark['sentence'],i,0)\n","    use_vecs.append(use_vec)\n","    w2v_vecs.extend(w2v_vec)  \n","  cnt=len(json_title)\n","  for i, title in  zip(range(cnt),json_title): \n","    guidance=title['word']+\"。\"+title[col]\n","    use_vec,w2v_vec=create_single_use_w2v_vec(guidance,i,1)\n","    use_vecs.append(use_vec)\n","    w2v_vecs.extend(w2v_vec)  \n","  # guianc_vecs=mt_min_max_normal(guianc_vecs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2iaOCknYJMNj","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","def mt_min_max_normal(mt):\n","  mt=np.array(mt,dtype=np.float32)\n","  mt_max=np.max(mt)\n","  mt_min=np.min(mt)\n","  mt_mean=np.mean(mt)\n","  mt_res=(mt-mt_mean)/(mt_max-mt_min)\n","  return list(mt_res)\n","def w2v_concat(vecs,cnt=10):\n","  ret_vec=[]\n","  i=-1\n","  for i,wv in zip(range(cnt),vecs):\n","      ret_vec=np.concatenate([ret_vec,wv],axis=0)\n","  for j in range(i+1,cnt):\n","    ret_vec=np.concatenate([ret_vec,np.zeros(50)],axis=0)\n","  return ret_vec\n","def vec_nor_concat(w2v_vecs,use_vecs,cnt=10):\n","  uvList=np.array(use_vecs)[:,0]\n","  use_vec_cat=mt_min_max_normal(list(uvList))\n","\n","  w2v_vec_cat=[]\n","  wvList=np.array(w2v_vecs)[:,0]\n","  wvList_nor=mt_min_max_normal(list(wvList))\n","  # print(wvList_nor[10])\n","  df=pd.DataFrame(w2v_vecs)\n","  df[0]=list(wvList_nor)\n","  df.columns=['vec','index','cls']\n","  gdf=df.groupby(['cls','index'])\n","  for ig in gdf:\n","    vec_cat=w2v_concat(ig[1]['vec'],cnt)\n","    w2v_vec_cat.append(vec_cat)\n","    # print(ig[0])\n","  \n","  return use_vec_cat,w2v_vec_cat\n","use_vec_cat,w2v_vec_cat=vec_nor_concat(w2v_vecs,use_vecs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZlyJalWzVAqN","colab_type":"code","colab":{}},"source":["print(len(use_vec_cat),len(w2v_vec_cat))\n","vec_cat=np.concatenate([use_vec_cat,w2v_vec_cat],axis=1)\n","print(vec_cat[110:113])\n","print(vec_cat[100:].shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qxMn7RQ1HMCT","colab_type":"code","colab":{}},"source":["!pip install multi-rake"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1-Sjfif6GLfs","colab_type":"code","colab":{}},"source":["from gensim.summarization import keywords\n","text_en = (\n","    '実際には虐待ではなく、家庭内の事故や突発的な脳出血などの場合もあり、虐待と判定される場合もあると言われている。'\n","    '実際には虐待ではなく、家庭内の事故や突発的な脳出血などの場合もあり、虐待と判定される場合もあると言われている。'\n","    '実際には虐待ではなく、家庭内の事故や突発的な脳出血などの場合もあり、虐待と判定される場合もあると言われている。')\n","\n","print(keywords(text_en,words = 10,scores = True, lemmatize = True))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LiodiUsJI9CF","colab_type":"code","colab":{}},"source":["!pip install neologdn\n","!pip install -U spacy\n","!pip install \"https://github.com/megagonlabs/ginza/releases/download/v1.0.2/ja_ginza_nopn-1.0.2.tgz\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AC5KakYtHUSt","colab_type":"code","colab":{}},"source":["import neologdn\n","import spacy\n","nlp = spacy.load('ja_ginza_nopn')\n","\n","def extract_words(sentence):\n","    docs = nlp(sentence)\t\t\n","    words = set(str(w) for w in docs.noun_chunks)\n","    words.union(str(w) for w in docs.ents)\n","    return words\n","\n","text = \"発達障害。発達障害発達障害のサイン・おとなしく遊ぶことが難しい、じっとしていられずいつも活動する・しゃべりすぎる・順番を待つのが難しい・他人の会話やゲームに割り込む・学校の勉強でうっかりミスが多い・課題や遊びなどの活動に集中し続けることができない・話しかけられていても聞いていないように見える・やるべきことを最後までやりとげない・課題や作業の段取りが下手・整理整頓が苦手・宿題のように集中力が必要なことを避ける・忘れ物や紛失が多い・気が散りやすい発達障害は、生まれつき脳の発達が通常と違っているために、幼児のうちから症状が現れ、通常の育児ではうまくいかないことがある。成長するにつれ、自分自身のもつ不得手な部分に気づき、生きにくさを感じることがあるかもしれない。注意欠如多動性障害ADHDアスペルガーアスペ\"\n","extract_words(neologdn.normalize(text))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bToRBScwIgf9","colab_type":"code","colab":{}},"source":["import spacy\n"," \n","def main():\n","  nlp = spacy.load('ja_ginza')\n","  text='依存構造解析の実験を行っています。'\n","  text='顔が腫れ上がり、額にケロイドが出来たのに…'\n","  doc = nlp(text)\n","  for sent in doc.sents:\n","    for token in sent:\n","      print(token.i, token.orth_, token.lemma_, token.pos_, token.dep_, token.head.i)\n","    print('EOS')\n"," \n","# 直接実行されたときはmain関数を呼び出す\n","if __name__ == \"__main__\":\n","  main()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IIyvIq2URgf9","colab_type":"code","colab":{}},"source":["w='ゴミ屋敷'\n","if w  in model_w2v:\n","   print(model_w2v[w])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DwJKSsUqRGHD","colab_type":"text"},"source":["# ginza\n","https://www.ogis-ri.co.jp/otc/hiroba/technical/similar-document-search/part5.html"]},{"cell_type":"markdown","metadata":{"id":"BuVT4o7zQD_a","colab_type":"text"},"source":["## install\n","```#  > /dev/null\n","!pip install spacy==2.2.3\n","!pip install neologdn\n","!pip install \"https://github.com/megagonlabs/ginza/releases/download/v1.0.2/ja_ginza_nopn-1.0.2.tgz\"\n","```"]},{"cell_type":"code","metadata":{"id":"gk6KMJmYRKDJ","colab_type":"code","colab":{}},"source":["!pip install ginza\n","!pip install git+https://github.com/boudinfl/pke.git\n","!python -m nltk.downloader stopwords"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UqG6wNNxPP49","colab_type":"code","colab":{}},"source":["!pip install sudachipy sudachidict_core"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NHFh15afPZuW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"027ccf9a-e52c-47d0-83f2-f1a08b45f436"},"source":["!echo \"空缶空罐空きカン\" | sudachipy -a"],"execution_count":null,"outputs":[{"output_type":"stream","text":["空缶\t名詞,普通名詞,一般,*,*,*\t空き缶\t空缶\tアキカン\t0\n","空罐\t名詞,普通名詞,一般,*,*,*\t空き缶\t空罐\tアキカン\t0\n","空きカン\t名詞,普通名詞,一般,*,*,*\t空き缶\t空きカン\tアキカン\t0\n","EOS\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Wa1BtSbaQFUn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"ab64a9c3-4600-4f90-92fe-5f7bc7417ee8"},"source":["!echo \"外国人参政権\" | sudachipy -m A"],"execution_count":null,"outputs":[{"output_type":"stream","text":["外国\t名詞,普通名詞,一般,*,*,*\t外国\n","人\t接尾辞,名詞的,一般,*,*,*\t人\n","参政\t名詞,普通名詞,一般,*,*,*\t参政\n","権\t接尾辞,名詞的,一般,*,*,*\t権\n","EOS\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"q_PW9vlcIxSd","colab_type":"text"},"source":["##test"]},{"cell_type":"code","metadata":{"id":"MAjBl4LVRk6t","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"ce98850a-c2ee-4119-be12-99f75c9e66d9"},"source":["import pke\n","pke.base.ISO_to_language['ja_ginza'] = 'japanese'\n","import ginza\n","import nltk\n","import string\n","\n","# from nltk.corpus import stopwords\n","# stoplist = list(string.punctuation)\n","# stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n","# stoplist += stopwords.words('english')\n","\n","stopwords = list(ginza.STOP_WORDS)\n","nltk.corpus.stopwords.words_org = nltk.corpus.stopwords.words\n","nltk.corpus.stopwords.words = lambda lang : stopwords if lang == 'japanese' else nltk.corpus.stopwords.words_org(lang)\n","\n","text = \"発達障害。発達障害発達障害のサイン・おとなしく遊ぶことが難しい、じっとしていられずいつも活動する・しゃべりすぎる・順番を待つのが難しい・他人の会話やゲームに割り込む・学校の勉強でうっかりミスが多い・課題や遊びなどの活動に集中し続けることができない・話しかけられていても聞いていないように見える・やるべきことを最後までやりとげない・課題や作業の段取りが下手・整理整頓が苦手・宿題のように集中力が必要なことを避ける・忘れ物や紛失が多い・気が散りやすい発達障害は、生まれつき脳の発達が通常と違っているために、幼児のうちから症状が現れ、通常の育児ではうまくいかないことがある。成長するにつれ、自分自身のもつ不得手な部分に気づき、生きにくさを感じることがあるかもしれない。注意欠如多動性障害ADHDアスペルガーアスペ\"\n","def extract_keywords(text,cnt=30):\n","  extractor = pke.unsupervised.MultipartiteRank() #ok\n","  # extractor = pke.unsupervised.TextRank()\n","  # extractor = pke.unsupervised.YAKE()\n","  # extractor = pke.supervised.TopicCoRank()\n","  # extractor = pke.unsupervised.PositionRank() #ok\n","  # extractor = pke.unsupervised.TfIdf() # ok\n","  # extractor = pke.unsupervised.FirstPhrases() # ok\n","\n","  extractor.load_document(input=text, language='ja_ginza', normalization=None)\n","  #次にフレーズ候補を形成する品詞を spaCy の定数で指定します。以下は名詞、固有名詞、形容詞、数を指定しています。\n","  extractor.candidate_selection(pos={'NOUN', 'VERB',  'ADJ','PROPN'}) #,'PUNCT',\n","  # extractor.candidate_selection(pos={'ADJ','ADP','ADV','AUX','CCONJ','DET','INTJ','NOUN','NUM','PART','PRON','PROPN','PUNCT','SCONJ','SYM','VERB','X'})\n","  # extractor.candidate_selection(pos={'NOUN', 'PROPN',  'VERB','ADJ','ADP','ADV','AUX','CCONJ','DET','INTJ','NUM','PART','PROPN','SCONJ','SYM','VERB','X'})\n","  # extractor.candidate_weighting(threshold=0.74, method='average', alpha=1.1)\n","  extractor.candidate_weighting()\n","  extra=extractor.get_n_best(n=cnt)\n","  wordList=[]\n","  for word in extra:\n","    wordList.extend(word[0].split())\n","  res=[]\n","  # delete duplicate data\n","  [res.append(x) for x in wordList if x not in res]\n","  return res,extra\n","# text='食事はまともに食べさせてもらえず、痩せすぎて、近所ではあの家の子供は上の子の方は太ってるのに下の子は痩せすぎてるし、しょっちゅう叫び声がする(暴力されるから)虐待だと言っているそうだとクラスメイトに教えてもらった'\n","text='観護措置。観護措置少年を少年鑑別所に送致し，一定期間そこに収容すること。目的は、少年鑑別所で少年の心身の状況等の検査をする必要がある場合，少年が調査，審判などに出頭しない恐れのある場合、暴走族等の外部からの悪影響から保護する必要がある場合にとられる。 法律上は「審判を行うため必要があるとき」とされており，具体的な事案に応じて裁判官が決める。不服があるときは家庭裁判所に対して異議申立てが可能。'\n","# text='スクールカウンセラー。スクールカウンセラー教育機関において心理相談業務に従事する心理職専門家の職業。児童・生徒・学生の不登校や、校内・学内での種々の問題行動などの対応に当たる。広義：　小学校、中学校、高等学校、大学等の教育機関の相談室などに勤務する心理職を指す。狭義：　文部科学省の任用規程のスクールカウンセラー資格保持者。法的身分は地方自治体・教育委員会からの任用を受けた特別職。SCスクールアドバイザー'\n","text=['顔が腫れ上がり、額にケロイドが出来たのに…',\n","'この間なんか頭を窓にぶつけられ、腕につめをたてられて、血がでました',\n","'それが青あざになって血が出てるの知ってる?',\n","'昨日も顔を殴られて眉毛のしたが赤くなり唇が切れました',\n","'叩かれすぎて頬も腫れ、鼻血も垂らしました',\n","'手首に水ぶくれができて体中が燃えるように熱いんです!!',\n","'リスカもいっぱいしてます',\n","'最近は痣が一ヶ月以上残るほど叩いてきます・・・',\n","'眼球の出血が引かず、一週間眼帯をして中学校へ通った期間があります',\n","'四度ほど、首を絞められました',\n","'噛んだ跡がミミズバレのように腫れて、その後内出血をしたような痣になるんです',\n","'私が首に切り傷を入れた時',\n","'発端は母が第三者に身体を許してたことからはじまり小学校を入学して直ぐから去年の夏頃から二カ月の間まで長い間当たり前の様に第三者から身体を弄られ性行為をしないと言葉による暴言や去年の夏の終わりから首の付け根を抑えこまれ息が出来なくなり死ぬかもしれない暴力を受けました',\n","'鼻血が出るまで殴られ続け妹は殴られて鼓膜が破れ、母は骨折しました',\n","'またいつものように急に殴りにきて、私に馬乗りになり、両手を捕まれ叩かれてたときに、やっぱり鼻血が出て、いつもは横に流れるんですけど、そのとき抵抗をしてたので口元に流れてき、母の手を一瞬ほどいて鼻血をトレーナーで拭いてしまったら、そのとき赤とグレーの色のトレーナーで、拭いたところがなぜか運悪くグレーのとこで、まぁ服を汚したらど突かれるんですけど、なぜグレーに汚す!とよけいキレられ、殴られ続けてぐったり',\n","'頭も、割れたかと思うくらい痛くて、殺されると思いました',\n","'それは腕を切るや血が出ることではなく、髪の毛を抜くことでした',\n","'そしたら急に父の顔色が変わり、私の頭にオムライスをかけ、まだ熱をもっているフライパンで私を叩いたのです',\n","'髪を引っ張っては殴ったりお腹を思いっきり蹴られます',\n","'ポットの熱湯を腕に流されたり、包丁で頭を切られたり･･･',\n","'ネグレクトを受けて育ちました',\n","'子どもを連れて実家に帰っても、パートナーとパチンコへ行ってしまったり',\n","'母親が子供の頃、私の祖母が病気のため、長時間留守番をさせ、ご飯を作らなかったなどと話しています',\n","'そんな母親は、生後数ヶ月の弟を産むと、私達を残しあっさり浮気相手の所に行ってしまいました',\n","'女の子に産んでおきながら、女の子らしい服や物を与えない',\n","'もちろん私服など無い',\n","'暴力やネグレクトはなかったものの、支配的な接し方をされたり、理不尽に当たられたり、無視されたり、学業でしか評価してもらえなかったりといったような子供時代でした',\n","'また、ネグレクトで常にカビの生えた生乾きの下着や服を着ており、お風呂は週一、ゴミ屋敷の中でコンビニ弁当を食べたりと、精神的、身体的に毎日ボロボロでした',\n","'父が居ない夕飯は、兄だけ呼ばれ二人で食べる',\n","'もちろん食事なんてくれません',\n","'竹刀で叩かれたり、ネグレクトもありました',\n","'食事も父のだけを一生懸命作る人でした',\n","'お金を私達に渡すとパチンコ三昧の日々',\n","'無関心無視するなら産んでほしくなかった',\n","'食事はまともに食べさせてもらえず、痩せすぎて、近所ではあの家の子供は上の子の方は太ってるのに下の子は痩せすぎてるし、しょっちゅう叫び声がする(暴力されるから)虐待だと言っているそうだとクラスメイトに教えてもらった',\n","'そしたらもう今現在、無視or怒鳴るのどちらかでしか私には接してくれなくなりました',\n","'毎晩起こしたけいれんを放置したのは、一人で生きて行けるようになる教育だったそうです',\n","'そんな環境か、両親は不安定でわたしは育児放棄も受けて育ったようでした',\n","'食事も別、母や兄弟に話しかけても無視されあまりにも辛く家に居場所がなく小学生で自殺を企てた事もあります',\n","'我が家は貧乏ではなかったのに学校で使う裁縫道具(2000円ぐらい)を買ってもらえませんでした',\n","'小学高学年になってから、父親は、私がお風呂に入っているのを覗きにきて、性器を見せるようにいうようになりました',\n","'パパがを陰部を出し娘に押しつけているのか・・・・陰部を出しているのは確かです',\n","'また、父は姉に性的いたずらをしていました',\n","'裸にさせられて、胸を鷲掴みされました',\n","'小学生のとき夜いきなり布団に潜ってきてむねを触られたり下着の上から下半身を触られたりしました',\n","'父親には小さい頃、セクハラされてた',\n","'よく風呂場で縛られてレイプされてた',\n","'そしたら義父が…ゆりちゃんのおっぱい結構でかいねって言ってきて…無視してたら触ってきました',\n","'祖母はよく私の部屋にノックなしで入ってきて、私が薬を塗るため服を脱いでいた事があったのですが、「あの子おっぱい丸出しでいたよ」と父と祖父もいるリビングでわざわざ言いました',\n","'娘に対して性的な目を向けているのが残念でなりません',\n","'中学3年からは兄が性器をあててきて、気持ちわるくてたまりませんでした',\n","'裸になった父親が抱きついてきて勃起しているものを私に押し付けてきたり、トイレに入るとどんどんドアを叩き、トイレを使わせないなど、思春期の私はすごく辛く怖い思いをしました',\n","'3歳頃、教祖の性暴力から泣いて逃げたら、両親に両手足を抑えられ、すでに老人であったら教祖から身体を弄ばれました',\n","'ドライブに連れて行かれてはキスされ、ﾌｪﾗもしてと言われました',\n","'父の唾液がまだ性器の周りに残っているような気さえするときもあります',\n","'父が裸を見てくる事を17才頃、母と祖母に相談したら驚かれました',\n","'その後に、向けたちんちんにお湯を掛けろ、大きくしろと言われ、',\n","'無理やりキスをされたり、性器を舐められたりされました',\n","'スカートの脚をじろじろ見てくる、両親のセックスの場面を見せようとする、風呂から上がるタイミングで脱衣所の洗濯機置き場に度々いる',\n","'私は中学1年生まで実父に性的虐待をうけていました',\n","'その後、育児に全く無関心だった父のもとで育てられました',\n","'この病は育児への理想と現実のギャップから母親がノイローゼになってしまったり、育児による辛さや不安をまわりに吐き出せないままストレスを抱えてしまうことからなると考えられています',\n","'その家は父子家庭であるそうです',\n","'弟夫婦は共働きで忙しく、平日はほどんど母親が面倒を見ています',\n","'父は仕事柄週のうち半分は仕事で、その間家には知らない男の人が入れ替わり立ち代わり、時には泊まって行くこともありました',\n","'そんな父は現在単身赴任中で県外にいるため、相談することが難しいです',\n","'でも、うちのしつけが他の家庭からしたら、少しおかしいと知ったのは親から離れてからです',\n","'戸籍すら作られていないこどもが現実にいます',\n","'親戚は、子供の顔が見たいと、直ぐに飛んで来る様に、週1日程度で、来てくれますが、母親は忙しい!遠い!となかなか来ません',\n","'車で2分の距離も滅多に帰らなくなりました',\n","'児相の介入が必要としない場合でも助けを求めている人がいることを知ってほしいです',\n","'放任主義で育てられた人が、親になって、その子供が',\n","'毎日、一人っ子のその家の子の相手として夜はその子が寝付くまで世話をしていました',\n","'(おもに父)母は転職癖がある父のかわりにフルで外で働いていたため私の養育はおもに父と祖母がすることになりました',\n","'わたしが母を怒らせると、｢産まなきゃよかった｣｢死ねばいいのに｣と言われました',\n","'暴力やネグレクトはなかったものの、支配的な接し方をされたり、理不尽に当たられたり、無視されたり、学業でしか評価してもらえなかったりといったような子供時代でした',\n","'私の家は母子家庭です',\n","'母を亡くし父子二人だけの生活',\n","'弟は小学生のころにアスペルガー症候群と発覚し、他人とは違う行動をするため、学校から電話がかかってき、それを聞いた母は毎日怒鳴っています',\n","'子供の帰る場所は家しかない…なのにその家が暴力に溢れていたら、私達子供はどうなってしまうのでしょうか',\n","'ですよね苦笑',\n","'頑張って!',\n","'でも仕方ないのです',\n","'ラッキー!!',\n","'本当は苦しい',\n","'応援しています!',\n","'私たちは悪くないのに!!',\n","'自分だけ辛いって',\n","'あぁ!そうですよね',\n","'気の毒なことです',\n","'可哀想な人なんだ',\n","'妹のバンド活動に伴い、遠方に住んでいる女の子のメンバーが数人、',\n","'自分の地域は新興住宅地で若い夫婦が多い所為もあって小さい子が多いのですが',\n","'私のところは家の斜め前で5～6人の子供が遊んでます',\n","'受験大丈夫?',\n","'酸素のない狭い水槽いる魚のように焦り、もがき暴れているのです',\n","'生まれてこのかた、拳で人を殴ったこともないのに、何でこんな事に？裁判でしゅくしゅくとやるしかないのですが、ちょっとおかしすぎます',\n","'DV保護法、その支援策は「もっともだ」と思っていましたが、実際自分が当事者になると、自分が全く反論も証明もする機会を与えられず、DV・虐待の加害者に…',\n","'顕在意識と潜在意識の関係に似ている',\n","'「私は皆から嫌われる存在',\n","\n","]\n","def extract_p(text):\n","  res,extra=extract_keywords(text,20)\n","  keywords_c='、'.join(map(str, res))\n","  print(len(res),res)\n","for t in text:\n","  # extract_p(t+'、青あざ')\n","  a=get_key_word(t)\n","  print(a)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['顔' '腫れ上がり' '額' 'ケロイド' '出来']\n","['間' '頭' '窓' 'ぶつけ' '腕' 'つめ' 'たて' '血' 'で']\n","['あざ' 'なっ' '血' '出' '知っ']\n","['顔' '殴ら' '眉毛' 'した' '赤く' 'なり' '唇' '切れ']\n","['叩か' '頬' '腫れ' '鼻血' '垂らし']\n","['手首' '水' 'ぶ' 'でき' '体' '燃える' '熱い']\n","['リスカ' 'し']\n","['痣' '残る' '叩い']\n","['眼球' '出血' '引か' '一週間' '眼帯' 'し' '中学校' '通っ' '期間' 'あり']\n","['四度' '首' '絞め']\n","['噛ん' '跡' 'ミミズバレ' '腫れ' '内出血' 'し' '痣' 'なる']\n","['首に' '切り傷' '入れ']\n","['発端' '母' '第三者' '身体' '許し' 'はじまり' '小学校' '入学' 'し' '夏' '間' '長い' '当たり前' '弄ら'\n"," '性行為' '言葉' '暴言' '夏の終わり' '首' '付け根' '抑え' '息' '出来' 'なり' '死ぬ' 'しれ' '暴力' '受け']\n","['鼻血' '出る' '殴ら' '妹' '鼓膜' '破れ' '母' '骨折' 'し']\n","['いつものように' '急' '殴り' 'き' '馬乗り' 'なり' '両手' '捕まれ' '叩か' '鼻血' '出て' '横' '流れる'\n"," '抵抗' 'し' '口元' '流れ' '母' '手' 'い' 'トレーナー' '拭い' '赤' 'グレー' '色' '運' '悪く' 'とこ'\n"," '服' '汚し' '突か' '汚す' 'キレ' '殴ら']\n","['頭' '割れ' '思う' '痛く' '殺さ' '思い']\n","['腕' '切る' '血' '出る' '髪の毛' '抜く']\n","['急' '父' '顔色' '変わり' '頭' 'オムライス' 'かけ' '熱' 'もっ' 'フライパン' '叩い']\n","['髪' '引っ張っ' '殴っ' 'お腹' '蹴ら']\n","['ポット' '熱湯' '腕' '流さ' '包丁' '頭' '切ら']\n","['ネグレクト' '受けて' '育ち']\n","['子ども' '連れ' '実家' '帰っ' 'パートナー' 'パチンコ' '行っ']\n","['母親' '子供' '祖母' '病気' '留守番' 'さ' 'ご飯' '作ら' '話し']\n","['母親' '生後' '弟' '産む' '私達' '残し' '浮気' '相手' '行っ']\n","['女の子' '産ん' '服' '物' '与え']\n","['私服' '無い']\n","['暴力' 'ネグレクト' 'なかっ' '支配' '接し' 'さ' '理不尽' '当たら' '無視' '学業' '評価' 'し' '子供' '時代']\n","['ネグレクト' 'カビの生えた' '生乾き' '下着' '服' '着' '風呂' '週' 'ゴミ屋敷' 'コンビニ弁当' '食べ' '精神的'\n"," '身体的' 'ボロボロ']\n","['父' '居' '夕飯' '兄' '呼ば' '二人' '食べる']\n","['食事' 'くれ']\n","['竹刀' '叩か' 'ネグレクト' 'あり']\n","['食事' '父' '作る' '人']\n","['お金' '私達' '渡す' 'パチンコ' '三昧']\n","['関心' '無視' 'する' '産ん']\n","['食事' 'まとも' '食べ' '痩せ' '近所' '家' '子供' '上の' '太っ' '下' '子' '叫び声' 'する' '暴力' 'さ'\n"," '虐待' '言っ' 'そうだ' 'クラスメイト' '教え']\n","['無視' '怒鳴る' '接し' 'なり']\n","['起こし' 'けいれん' '放置' 'し' '一人' '生き' '行ける' 'なる' '教育']\n","['環境' '両親' '不安定' '育児放棄' '受け' '育っ']\n","['食事' '別' '母' '兄弟' '話しかけ' '無視' 'さ' '辛く' '家' '居場所' 'なく' '小学生' '自殺' '企て'\n"," 'あり']\n","['我が家' '貧乏' 'なかっ' '学校' '使う' '裁縫' '道具' '買っ']\n","['小学' '高学年' 'なっ' '父親' '風呂' '入っ' '覗き' 'き' '性器' '見せる' 'いう' 'なり']\n","['パパ' '陰部' '出し' '娘' '押しつけ' '確か']\n","['父' '姉' '性的' 'いたずら' 'し']\n","['裸' 'さ' '胸' '鷲掴み']\n","['小学生' '布団' '潜っ' 'むね' '触ら' '下着' '下半身' 'し']\n","['父親' '小さい' 'セクハラ' 'さ']\n","['風呂' '縛ら' 'レイプ' 'さ']\n","['義父' 'ゆり' 'おっぱい' 'でかい' '言っ' '無視' 'し' '触っ']\n","['祖母' '私の部屋' 'ノック' 'なし' '入っ' '薬' '塗る' '服' '脱い' 'いた事' 'あっ' '子' 'おっぱい' '丸出し'\n"," 'い' '父' '祖父' 'いる' 'リビング' '言い']\n","['娘' '性的' '目' '向け' '残念']\n","['兄' '性器' 'あて' '気持ち' 'わるく' 'たまり']\n","['裸' 'なっ' '父親' '抱きつい' '勃起' 'し' '押し付け' 'トイレ' '入る' 'ドア' '叩き' '使わ' '思春期'\n"," 'すごく' '辛く' '怖い' '思い']\n","['教祖' '性暴力' '泣い' '逃げ' '両親' '手足' '抑え' '老人' '身体' '弄ば']\n","['ドライブ' '連れ' 'キス' 'さ' 'ﾌｪﾗ' 'し' '言わ']\n","['父' '唾液' '性器' '周り' '残っ' 'する' 'あり']\n","['父' '裸' '見' '母' '祖母' '相談' 'し' '驚か']\n","['向け' 'ちんちん' 'お湯' '掛けろ' '大きく' 'しろ' '言わ']\n","['キス' 'さ' '性器' '舐め']\n","['スカート' '脚' '見' '両親' 'セックス' '場面' '見せよ' 'する' '風呂' '上がる' 'タイミング' '脱衣' '洗濯機'\n"," '置き場' 'いる']\n","['実父' '性的虐待' 'うけ']\n","['育児' '関心' '父' '育て']\n","['病' '育児' '理想' '現実' 'ギャップ' '母親' 'ノイローゼ' 'なっ' '辛さ' '不安' 'まわり' '吐き出せ' 'ストレス'\n"," '抱え' 'なる' '考え']\n","['家' '父子家庭']\n","['弟' '夫婦' '共働き' '忙しく' '母親' '面倒' '見']\n","['父' '仕事' '週' '知ら' '男' '人' '入れ替わり' '立ち代わり' '泊まっ' 'あり']\n","['父' '単身赴任' '県外' 'いる' '相談' 'する' '難しい']\n","['うち' 'し' 'つけ' '他' '家庭' 'おかしい' '知っ' '親' '離れ']\n","['戸籍' '作ら' 'こども' '現実' 'い']\n","['親戚' '子供' '顔' '見' '飛ん' '週' '来' '母親' '忙しい' '遠い']\n","['車' '距離' '滅多' '帰ら' 'なり']\n","['児相' '介入' '必要' 'しない' '助け' '求め' '人' 'いる' '知っ']\n","['放任主義' '育て' '人' '親' 'なっ' '子供']\n","['一人っ子' '家の子' '相手' '子' '寝付く' '世話' 'し']\n","['おもに' '父' '母' '転職' 'ある' 'かわり' 'フル' '外' '働い' '養育' '祖母' 'する' 'なり']\n","['母' '怒ら' '産ま' 'よかっ' '死ねばいいのに' '言わ']\n","['暴力' 'ネグレクト' 'なかっ' '支配' '接し' 'さ' '理不尽' '当たら' '無視' '学業' '評価' 'し' '子供' '時代']\n","['家' '母子家庭']\n","['母' '亡くし' '父子' '生活']\n","['弟' '小学生' 'アスペルガー症候群' '発覚' 'し' '他人' '違う' '行動' 'する' '学校' '電話' 'かかっ' '聞い'\n"," '母' '怒鳴っ']\n","['子供' '帰る場所' '家' 'ない' '暴力' '溢れ' '私達' 'なっ']\n","['苦笑']\n","['頑張っ']\n","['仕方']\n","['ラッキー!!']\n","['苦しい']\n","['応援' 'し']\n","['私たち' '悪く']\n","['自分' '辛いっ']\n","['あぁ!']\n","['気の毒']\n","['可哀想' '人']\n","['妹' 'バンド活動' '伴い' '遠方' '住ん' '女の子' 'メンバー' '数人']\n","['自分' '地域' '新興住宅地' '若い' '夫婦' '多い' '所為' 'あっ' 'さい子']\n","['家' '斜め' '子供' '遊ん']\n","['受験' '大丈夫']\n","['酸素' 'ない' '狭い' '水槽' '魚の' '焦り' 'もがき' '暴れ']\n","['生まれ' '拳' '人' '殴っ' 'ない' '裁判' 'ゅくしゅくとやるしかないのですが' 'おかし']\n","['保護' '支援策' '思っ' '自分' '当事者' 'なる' '全く' '反論' '証明' 'する' '機会' '与え' '虐待' '加害者']\n","['顕在' '意識' '潜在意識' '関係' '似']\n","['嫌わ' '存在']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"O6UX9FIxVykT","colab_type":"code","colab":{}},"source":["np.array(extra)[:,0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z4HwPKDAGx70","colab_type":"text"},"source":["# センテンス分割\n","```\n","https://qiita.com/wwwcojp/items/3535985007aa4269009c\n","Pythonで日本語の文区切りにも使えるライブラリとしては、GiNZAがあります。\n","GiNZAを用いた文区切りは以下のように行えます。\n","GiNZAを用いた場合の長所としては、きちんと係り受け解析等をしているので文の途中で改行していたり、句点を省いていたりといった場合でも精度高く文の区切りを検出できる点が挙げられます。\n","重量級ですが、GiNZAの他の機能も併せて利用するならよい選択肢だと思います。\n","sentence-splitter\n","```"]},{"cell_type":"code","metadata":{"id":"MgxRbZozGxSl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":413},"outputId":"9df6cf60-7a3e-4971-8939-83fd3678139f"},"source":["import spacy\n","nlp = spacy.load('ja_ginza')\n","text='私は「あなたの思いに答えられない。他を当たってほしい。」と言われました！呆然として\\nその場にたたずむしかありませんでしたそれでも私は信じたい！'\n","text='顔が腫れ上がり、額にケロイドが出来たのに…'\n","doc = nlp(text)\n","for sent in doc.sents:\n","  print(sent)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-120fb9273943>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ja_ginza'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'私は「あなたの思いに答えられない。他を当たってほしい。」と言われました！呆然として\\nその場にたたずむしかありませんでしたそれでも私は信じたい！'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'顔が腫れ上がり、額にケロイドが出来たのに…'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'ja_ginza'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."]}]},{"cell_type":"markdown","metadata":{"id":"XM-WpDPhgHFn","colab_type":"text"},"source":["# nagisa test\n","https://qiita.com/taishi-i/items/5b9275a606b392f7f58e"]},{"cell_type":"code","metadata":{"id":"a87ZR3l_gJJH","colab_type":"code","colab":{}},"source":["!pip install nagisa"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gdbXej3AgXEK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"cee20b9b-672c-4536-cd70-091dea7c674c"},"source":["import nagisa\n","text  = 'Pythonで簡単に使えるツールです'\n","text='顔が腫れ上がり、額にケロイドが出来たのに…'\n","words = nagisa.tagging(text)\n","\n","print(words)\n","#=> Python/名詞 で/助詞 簡単/形状詞 に/助動詞 使える/動詞 ツール/名詞 です/助動詞"],"execution_count":null,"outputs":[{"output_type":"stream","text":["顔/名詞 が/助詞 腫れ上がり/動詞 、/補助記号 額/名詞 に/助詞 ケロイド/名詞 が/助詞 出来/動詞 た/助動詞 の/助詞 に/助詞 .../補助記号\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ty3lq0GjkJws","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"94acacc0-8d6a-423d-d403-4985189a3322"},"source":["import re\n","word = \"腫完全一致上\"\n","# re_hiragana = re.compile(r'^[あ-ん]+$')\n","re_kanji = re.compile(r'^[\\u4E00-\\u9FD0]+$')\n","status_hira = re_kanji.match(word) #fullmatch:完全一致\n","print(status_hira)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<_sre.SRE_Match object; span=(0, 6), match='腫完全一致上'>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YW8QuEoYTtpd","colab_type":"code","colab":{}},"source":["from gensim import corpora, models, similarities\n","from gensim.models import hdpmodel, ldamodel\n","from gensim import models\n","documents = [\"Human machine interface for lab abc computer applications\",\n","              \"A survey of user opinion of computer system response time\",\n","              \"The EPS user interface management system\",\n","              \"System and human system engineering testing of EPS\",\n","              \"Relation of user perceived response time to error measurement\",\n","              \"The generation of random binary unordered trees\",\n","              \"The intersection graph of paths in trees\",\n","              \"Graph minors IV Widths of trees and well quasi ordering\",\n","              \"Graph minors A survey\"]\n","\n","# remove common words and tokenize\n","stoplist = set('for a of the and to in'.split())\n","texts = [[Word for Word in document.lower().split() if Word not in stoplist]\n","         for document in documents]\n","\n","# remove words that appear only once\n","all_tokens = sum(texts, [])\n","tokens_once = set(Word for Word in set(all_tokens) if all_tokens.count(Word) == 1)\n","texts = [[Word for Word in text if Word not in tokens_once]\n","         for text in texts]\n","dictionary = corpora.Dictionary(texts)\n","corpus = [dictionary.doc2bow(text) for text in texts]\n","tfidf = models.TfidfModel(corpus)\n","corpus_tfidf = tfidf[corpus]\n","# I can print out the topics for LSA\n","lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)\n","corpus_lsi = lsi[corpus]\n","\n","for l,t in zip(corpus_lsi,corpus):\n","  print (l,\"#\",t)\n","print\n","for top in lsi.print_topics(2):\n","  print (top)\n","\n","# I can print out the documents and which is the most probable topics for each doc.\n","lda = ldamodel.LdaModel(corpus, dictionary, 50)\n","corpus_lda = lda[corpus]\n","\n","for l,t in izip(corpus_lda,corpus):\n","  print (l,\"#\",t)\n","print\n","\n","# But I am unable to print out the topics, how should i do it?\n","for top in lda.print_topics(10):\n","  print (top)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vgYCeVW3mrCi","colab_type":"text"},"source":["#tf-idf\n","```\n"," https://tex2e.github.io/blog/python/tf-idf\n","```\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"U_keXK2O4ZlU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"ad01c050-ec82-4fd0-e5dc-87bab3cedd64"},"source":["from gensim.models import word2vec\n","import subprocess\n","import MeCab as mc\n","def get_words(text):\n","    cmd='echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"'\n","    path = (subprocess.Popen(cmd, stdout=subprocess.PIPE,\n","                              shell=True).communicate()[0]).decode('utf-8')\n","    mt=mc.Tagger(\"-d {0}\".format(path))\n","    mt.parse('')\n","\n","    parsed = mt.parseToNode(text)\n","    components = []\n","\n","    while parsed:\n","        components.append(parsed.surface)\n","        parsed = parsed.next\n","    components = [i for i in components if i !='']\n","    return components\n","pathout='/content/mount/My Drive/00_work/guidance/w2vmodel/ja-gensim.50d.data.model'\n","model_w2v = word2vec.Word2Vec.load(pathout) # path of w2v model\n","text = \"天気がいいから散歩しましょう\"\n","word_list = get_words(text)\n","wv=model_w2v[word_list[0]]\n","print(word_list)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['天気', 'が', 'いい', 'から', '散歩', 'し', 'ましょ', 'う']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gaSWIPASw96Z","colab_type":"text"},"source":["## tf idf source"]},{"cell_type":"code","metadata":{"id":"22jkh7rtmuWu","colab_type":"code","colab":{}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# 正規表現で調べる方法：データをクレンジング\n","def check_by_regex(s):\n","  ret= bool(re.search(r\"[a-zA-Z]\", s)) or \\\n","           bool(re.search(r\"[0-9]\", s))\n","  return not ret\n","# Pearson Correlation　最大値を採用　●\n","def pearson_coef_similarity(x,y):\n","  x_=x-np.mean(x)\n","  y_=y-np.mean(y)\n","  return np.dot(x_,y_)/(np.linalg.norm(x_)*np.linalg.norm(y_))\n","#Euclidean Distance 最小値を採用　● 1-x\n","def euclidean_distance_similarity(x,y):\n","  return np.sqrt(np.sum(np.square(x-y)))\n","#Cosine Similarity　●\n","def cosine_similarity(xs,ys):\n","  return np.dot(xs,ys)/(np.linalg.norm(xs)*np.linalg.norm(ys))\n","# Jaccard Similarity ×\n","# stopwords_list=['〇〇','あぁ!','い','いう','いか','いる','いろいろ','き','く','くる','さい','ささい','し','した','しない','す','する','で','でき','できる','できれ','とこ','とっ','ない','ないか','なかっ','なく','なさ','なし','なっ','なら','なり','なる','ひとり','ふたつ','ぶ','み','もし','もっ','一','二','三','四','五','六','七','八','九','十','百','千','0','1','2','3','4','5','6','7','8','9','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\n","stopwords_df=pd.read_csv('/content/mount/My Drive/00_work/guidance/stopwords.csv',sep=',',header=None)\n","stopwords_list=stopwords_df[0].values.tolist()\n","def clean_words(wd):\n","\tres=wd\n","\tif res in stopwords_list:\n","\t  res=''\n","\treturn res\n","\n","filterList=[\n","['動詞','自立'],\n","['名詞','サ変接続'],\n","['名詞','ナイ形容詞語幹'],\n","['名詞','一般'],\n","['名詞','固有名詞'],\n","['名詞','形容動詞語幹'],\n","['形容詞','自立']\n","]\n","\n","def mt_min_max_normal(mt):\n","  mt=np.array(mt,dtype=np.float32)\n","  mt_max=np.max(mt)\n","  mt_min=np.min(mt)\n","  mt_res=(mt-mt_min)/(mt_max-mt_min)\n","  return list(mt_res)\n","def get_key_word(remark):\n","  lines=m.parse(remark).split('\\n')\n","  remarks_key=[]\n","  for line in lines:\n","    items = re.split('[\\t]',line)\n","    if len(items) > 1:\n","      subItems= items[1].split(',')\n","      for f in filterList:\n","        if subItems[0]==f[0] and subItems[1]==f[1] and check_by_regex(items[0]):\n","            remarks_key.append(items[0])\n","  res=[]\n","  # delete duplicate data\n","  [res.append(x) for x in remarks_key if x not in res]\n","  # print(len(res),remark)\n","  return np.array(res)\n","#mecab を使って、形態素解析し、単語へ\n","def sentence_spt_short(sent):\n","\tsps=['・','　','。','、']\n","\tresult = re.sub(r'[・　。、]+', ',', sent)\n","\tresult = result.replace(\",,\",\",\")\n","\tres=result.split(',')\n","\treturn res\n","\n","def tfidf_vec(dv_guidan,dv_guidan_doc_id):\n","  vectorizer=TfidfVectorizer(min_df=1,max_df=50)\n","  X=vectorizer.fit_transform(dv_guidan)\n","  # print('feature_names:',vectorizer.get_feature_names())\n","  words=vectorizer.get_feature_names()\n","  guidances_tfidf=[]\n","  df_guidances=pd.DataFrame()\n","  # print('tfidf_vec:',len(X.toarray()),len(dv_guidan))\n","  for doc_id, vec in zip(dv_guidan_doc_id, X.toarray()):\n","      # print('doc_id:', doc_id)\n","      words_tfidf=[]\n","      for w_id, tfidf in sorted(enumerate(vec), key=lambda x: x[1], reverse=True):\n","          lemma_c = words[w_id]\n","          if tfidf > 0:\n","            lemma=clean_words(lemma_c)\n","            if lemma in model_w2v and len(lemma)>0:\n","              wv=model_w2v[lemma]\n","              words_tfidf.append([lemma, tfidf,wv])\n","      words_tfidf_array=np.array(words_tfidf)\n","      if words_tfidf_array.shape[0] > 0:\n","        tfidf_nor=mt_min_max_normal(words_tfidf_array[:,1])\n","        #tfidf_nor=words_tfidf_array[:,1]\n","        df=pd.DataFrame()\n","        df['g_vector']=words_tfidf_array[:,2]\n","        df['g_word']=words_tfidf_array[:,0]\n","        df['g_tfidf']=tfidf_nor\n","        df['g_id']=doc_id\n","        df_guidances=pd.concat([df_guidances,df],axis=0)\n","  return df_guidances\n","def docs_tfidf(guidan_docs,remark_docs):\n","  dv_guidan=[]\n","  dv_guidan_doc_id=[]\n","  dv_remark=[]\n","  lenGuid=len(guidan_docs)\n","  for doc_id,doc in zip(range(lenGuid),guidan_docs):\n","    #  res_subs=sentence_spt_short(doc)\n","    #  for res_sub in res_subs:\n","      word_list = get_key_word(doc)# get_words(res_sub) #\n","      if len(word_list) > 0:\n","        doc_word=' '.join(map(str, word_list))\n","        # print(doc_word)\n","        dv_guidan.append(doc_word)\n","        dv_guidan_doc_id.append(doc_id)\n","  for doc_id ,doc in zip(range(len(remark_docs)),remark_docs):\n","    word_list = get_key_word(doc)# get_words(res_sub) #\n","    for word_c  in word_list:\n","      word=clean_words(word_c)\n","      if word in model_w2v and len(word)>0:\n","        wv=model_w2v[word]\n","        dv_remark.append([wv,word,doc_id])\n","\n","  df_guidances=tfidf_vec(dv_guidan,dv_guidan_doc_id)\n","  dv_guidances=df_guidances.values.tolist()\n","  # df_remarks=pd.DataFrame(dv_remark,columns=['r_vector','r_word','r_id'])\n","  # print('dv_guidan_doc_id:',len(dv_guidan_doc_id))\n","  return   dv_guidances,dv_remark\n","\n","def load_remark_guidance_dt(col,pathout,keyCnt=10):\n","  json_remark = json.load(open(pathout+\"sentence.json\", \"r\"))\n","  json_title = json.load(open(pathout+\"word_\"+col+\".json\", \"r\"))\n","\n","  remark_docs=[]\n","  guidan_docs=[]\n","  cnt_r=len(json_remark)\n","  for i,remark in zip(range(cnt_r),json_remark):\n","    # use_vec,w2v_vec=create_single_use_w2v_vec(remark['sentence'],i,0)\n","    # use_vecs.append(use_vec)\n","    # w2v_vecs.extend(w2v_vec)\n","    remark_docs.append(remark['sentence'])\n","  cnt_t=len(json_title)\n","  for i, title in  zip(range(cnt_t),json_title):\n","    guidance=title['word']+\"。\"+title[col]\n","    guidan_docs.append(guidance)\n","    # use_vec,w2v_vec=create_single_use_w2v_vec(guidance,i,1)\n","    # use_vecs.append(use_vec)\n","    # w2v_vecs.extend(w2v_vec)\n","  # use_vec_cat,w2v_vec_cat=vec_nor_concat(w2v_vecs,use_vecs,keyCnt)\n","  return remark_docs,guidan_docs,json_remark,json_title\n","\n","def similarity_cal(col_in,path,outputdir,keycnt=10):\n","  remark_docs,guidan_docs,json_remark,json_title=load_remark_guidance_dt(col_in,path)\n","  df_guidances,df_remarks=docs_tfidf(guidan_docs,remark_docs)\n","  sim_out=[]\n","  cnt_r=len(df_remarks)\n","  # print('tfidf_vec2:',len(df_guidances),len(json_title))\n","  # return\n","  for i,r_rc in zip(range(cnt_r),df_remarks):\n","    # df_remarks   columns=['r_vector','r_word','r_id']\n","    # df_guidances columns=['g_vector',\t'g_word',\t'g_tfidf'\t,'g_id']\n","    print(i,cnt_r,col_in)\n","    # if i >2:\n","    #   break\n","    for g_rc in df_guidances:\n","      # g_rc=df_guidances.iloc[j,:]\n","      s_c=np.round(cosine_similarity(r_rc[0],g_rc[0])  ,3)\n","      s_p=np.round(pearson_coef_similarity(r_rc[0],g_rc[0]),3)\n","      s_c_t=np.round(s_c*g_rc[2],3)\n","      s_p_t=np.round(s_p*g_rc[2],3)\n","      \n","      # 'r_id','r_word','g_id',\t'g_word',\t'g_tfidf','sim_pearson','sim_cosine'\n","      sim_out.append([r_rc[2],r_rc[1],g_rc[3],g_rc[1],g_rc[2],s_p,s_c,s_p_t,s_c_t])\n","      # sim_out.append([r_rc[0],g_rc[0],r_rc[2],r_rc[1],g_rc[3],g_rc[1],g_rc[2]])\n","      #'r_v','g_v',\n","  df_sim_out=pd.DataFrame(sim_out,columns=['r_id','r_word','g_id',\t'g_word',\t'g_tfidf','s_pear','s_cos','s_pear_t','s_cos_t'])\n","\n","  def get_similarity(df,sim_key,col,index):\n","    #df: ['r_id','r_word','g_id',\t'g_word',\t'g_tfidf','s_pear','s_cos','s_pear_t','s_cos_t'])\n","    \n","    df = df.sort_values(['r_id','g_id',sim_key], ascending=False)\n","    dt_rgid_list=[]\n","    # gdf=df.groupby(['r_id','r_word'])\n","    gdf=df.groupby(['r_id','g_id'])\n","    for ig in gdf:\n","      sim_words='(|'\n","      dt_rec=ig[1].values.tolist()[0]\n","      tfidf_mean=[]\n","      sim_mean=[]\n","      ig1_s = ig[1].sort_values([sim_key], ascending=False)\n","      cntig=len(ig1_s.values.tolist())\n","      cntig_m=min(cntig,3)\n","      re_list=[]\n","      gu_list=[]\n","      j=0\n","      for i in range(cntig):\n","        rec=ig1_s.values.tolist()[i]\n","        if rec[1] not in re_list and  rec[3] not in gu_list:\n","          sim_words=sim_words+str(rec[1])+'-'+str(rec[3])+':'+str(np.round(rec[index],2))+'|'\n","          tfidf_mean.append(rec[4])\n","          sim_mean.append(rec[index])\n","          re_list.append(rec[1])\n","          gu_list.append(rec[3])\n","          j=j+1\n","          if j>3:\n","            break\n","      \n","      \n","      sim_words=sim_words+')'\n","      tfidf_mean=np.round(np.mean(tfidf_mean),3)\n","      sim_mean=np.round(np.mean(sim_mean),3)\n","      #'r_id','g_id',sim_words,tfidf_mean,sim_mean\n","      dt_rgid_list.append([dt_rec[0],dt_rec[2],sim_words,tfidf_mean,sim_mean])\n","\n","    # df_rg_id=pd.DataFrame(dt_rgid_list,columns=['r_id','r_word','g_id',\t'g_word',\t'g_tfidf',sim_key])\n","    df_rg_id=pd.DataFrame(dt_rgid_list,columns=['r_id','g_id','gr_word','g_tfidf',sim_key])\n","\n","    gdf=df_rg_id.groupby(['r_id'])\n","    dt_rgid3_list=[]\n","    for ig in gdf:\n","      #3位までデータを取得\n","      ig1_s = ig[1].sort_values([sim_key], ascending=False)\n","      dt_rec=ig1_s.values.tolist()[0:3]\n","      dt_rgid3_list.extend(dt_rec)\n","    sim_out_list=[]\n","    #json_remark,json_title\n","    for dt_rgid3 in dt_rgid3_list:\n","      #dt_rgid3 [0'r_id',1'g_id',2'gr_word',3'g_tfidf',4 sim_key])\n","      remark_id=dt_rgid3[0]\n","      guidan_id=dt_rgid3[1]\n","      sim_out_list.append([\n","                         json_remark[remark_id]['sentence'],\n","                         json_title[guidan_id]['word'],\n","                         json_title[guidan_id][col],\n","                         dt_rgid3[2],\n","                         dt_rgid3[3],\n","                         dt_rgid3[4]\n","      ])\n","\n","    df_out=pd.DataFrame(sim_out_list,columns=['sentence','word',col,'sim_words','tfidf-mean',sim_key])\n","    # print(df_out)\n","    return df_out\n","\n","  # df_sim_out=pd.DataFrame(sim_out,columns=['r_id','r_word','g_id',\t'g_word',\t'g_tfidf','s_pear','s_cos','s_pear_t','s_cos_t'])\n","  \n","  # df_sim_out.to_csv(outputdir+columns+\"_similarity.csv\",index=None)\n","  sim_key='s_cos_t'\n","  df_out=get_similarity(df_sim_out,sim_key,columns,8)\n","  df_out.to_csv(outputdir+col_in+\"_similarity_\"+sim_key+\".csv\",index=None)\n","\n","  sim_key='s_cos'\n","  df_out=get_similarity(df_sim_out,sim_key,columns,6)\n","  df_out.to_csv(outputdir+columns+\"_similarity_\"+sim_key+\".csv\",index=None)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j4KPoueB8TAM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"24eeb2a8-3e68-4e38-f7fb-16a4d4b9f6fb"},"source":["# stopwords_list=['〇〇','あぁ!','い','いう','いか','いる','いろいろ','き','く','くる','さい','ささい','し','した','しない','す','する','で','でき','できる','できれ','とこ','とっ','ない','ないか','なかっ','なく','なさ','なし','なっ','なら','なり','なる','ひとり','ふたつ','ぶ','み','もし','もっ','一','二','三','四','五','六','七','八','九','十','百','千','0','1','2','3','4','5','6','7','8','9','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\n","stopwords_df=pd.read_csv('/content/mount/My Drive/00_work/guidance/stopwords.csv',sep=',',header=None)\n","stopwords_list=stopwords_df[0].values.tolist()\n","print(stopwords_list)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['0', '1', '2', '23歳', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'new', 'o', 'or', 'p', 'pdf', 'pptx', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '〇〇', 'あぁ!', 'あげ', 'あげる', 'あそこ', 'あそび', 'あたり', 'あちら', 'あっ', 'あっち', 'あて', 'あと', 'あな', 'あなた', 'あり', 'ある', 'あるとき', 'あれ', 'い', 'いう', 'いか', 'いくつ', 'いくつか', 'いつ', 'いつものように', 'いない', 'いま', 'いや', 'いる', 'いろいろ', 'うち', 'うまく', 'おおまか', 'おか', 'おかしい', 'おまえ', 'おもに', 'および', 'おれ', 'かかっ', 'かかる', 'かかわり', 'かく', 'かけ', 'かたち', 'かやの', 'から', 'がい', 'がら', 'き', 'きた', 'く', 'くせ', 'くる', 'ここ', 'こころ', 'こちら', 'こっち', 'こと', 'ことば', 'これ', 'これら', 'ごっちゃ', 'ごと', 'ごろ', 'さ', 'さい', 'ささい', 'さまざま', 'さらい', 'さん', 'し', 'しかた', 'した', 'しない', 'しゃべり', 'しよう', 'す', 'すか', 'すね', 'すべて', 'する', 'ずつ', 'ぜんぶ', 'そう', 'そうだ', 'そこ', 'そちら', 'そっち', 'そで', 'そのため', 'その後', 'それ', 'それぞれ', 'それなり', 'それや', 'たくさん', 'たち', 'たび', 'ため', 'だめ', 'ちゃ', 'ちゃん', 'てん', 'で', 'でき', 'できる', 'できれ', 'とおり', 'とき', 'とこ', 'ところ', 'とっ', 'どこ', 'どこか', 'どちら', 'どっか', 'どっち', 'どれ', 'ない', 'ないか', 'なか', 'なかっ', 'なかば', 'なく', 'なくなる', 'なさ', 'なし', 'なっ', 'など', 'なに', 'なら', 'なり', 'なる', 'なん', 'はかる', 'はじめ', 'はず', 'はるか', 'ひと', 'ひとつ', 'ひとり', 'ふく', 'ふたつ', 'ぶ', 'ぶり', 'へん', 'べつ', 'ぺん', 'ほう', 'ほか', 'まさ', 'まし', 'まとも', 'まま', 'み', 'みたい', 'みつ', 'みなさん', 'みんな', 'もし', 'もっ', 'もと', 'もの', 'もん', 'やつ', 'やる', 'よい', 'よう', 'よかっ', 'よそ', 'わけ', 'わたし', 'カ所', 'カ月', 'ハイ', 'ルール', 'レ', 'ヵ所', 'ヵ月', 'ヶ所', 'ヶ月', '一', '一つ', '一人', '一人っ子', '一人ひとり', '一定', '一定期間', '一時的', '一本', '一条', '一次', '一種', '一緒', '一緒に', '一般', '一般的', '一般財源', '一週間', '一過性', '一部分', '七', '万', '三', '三昧', '三歳', '三番', '上', '上手', '上記', '下', '下記', '与え', '与える', '世代', '世界', '世話', '両手', '中', '中学1年生', '中学3年', '中学校', '主', '九', '事', '二', '二人', '二次', '互い', '五', '人', '人々', '人たち', '人として', '人前', '人口', '人間', '今', '今回', '他', '他人', '以上', '以下', '以前', '以後', '以降', '会', '伸', '似', '体', '何', '何人', '作', '作ら', '作る', '作業', '使い', '使う', '使っ', '使わ', '使用', '例', '係', '保', '俺', '個', '健', '側', '億', '元', '兆', '先', '入っ', '入る', '入れ', '入れる', '全く', '全部', '八', '六', '具体', '具体化', '内', '円', '冠', '冬', '出', '出さ', '出し', '出て', '出る', '分', '刑', '列', '別', '刺', '前', '前回', '力', '動', '化', '匹', '区', '十', '十分', '千', '午後10時', '半ば', '口', '台', '右', '各', '同じ', '名', '名前', '向こう', '否', '哀', '品', '員', '喜', '器', '四', '回', '国', '土', '地', '場', '場合', '境', '士', '夏', '外', '多い', '多かっ', '多く', '大きい', '大きく', '大丈夫', '大人', '大半', '大変', '大多数', '女', '奴', '婦', '子', '字', '室', '家', '居', '屋', '左', '市', '席', '帰っ', '帰ら', '平成', '平成18年', '平成24年', '年', '年代', '年月日', '年生', '年間', '年齢', '幾つ', '店', '府', '度', '式', '形', '彼', '彼女', '後', '怒', '思い', '思いやり', '思う', '思っ', '性', '情', '感', '感じ', '感じる', '我々', '所', '手', '手段', '扱い', '拳', '放っ', '数', '文', '新た', '方', '方法', '日', '春', '昭和', '是非', '時', '時代', '時点', '時間', '普及', '書', '月', '有', '有し', '有する', '有り', '有る', '期待', '期間', '木', '未満', '本人', '本当', '材料', '村', '束', '条件', '条例', '来', '枚', '校', '楽', '様', '様々', '次', '歳', '歴', '段', '毎', '毎日', '気', '水', '求め', '求める', '法', '火', '点', '玉', '生き', '生じ', '生じる', '用', '男', '町', '界', '略', '登', '百', '的', '目', '県', '着', '確か', '私', '私たち', '私の部屋', '秋', '秒', '第', '等', '箇所', '箇月', '簿', '系', '紀', '結局', '線', '考え', '者', '自体', '自分', '行', '行い', '行う', '行か', '行く', '行け', '行ける', '行っ', '見', '観', '話', '誌', '誰', '課', '論', '輪', '近く', '通', '連', '週', '道', '達', '違い', '部', '都', '金', '間', '関係', '際', '集', '面', '頃', '類', '首', '高', '３つ', 'ＡＬＳ', 'ＤＶ', 'ＭＪＣＡ', 'ＮＰＯ法人', 'ＱＯＬ', 'ＳＦ']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Cit-mrK0xXfA","colab_type":"text"},"source":["## tf idf excute"]},{"cell_type":"code","metadata":{"id":"V9STAgHKsctV","colab_type":"code","colab":{}},"source":["import re\n","import warnings\n","warnings.simplefilter('ignore')\n","keycnt=10\n","outputdir='/content/mount/My\\ Drive/00_work/guidance/output_tfidf_normal727_2/'\n","%mkdir {outputdir}\n","outputdir='/content/mount/My Drive/00_work/guidance/output_tfidf_normal727_2/'\n","\n","# 01_title\n","columns='title'\n","path='/content/mount/My Drive/00_work/guidance/20200701/01_title/'\n","similarity_cal(columns,path,outputdir)\n","\n","#02_confirm\n","columns='confirmation'\n","path='/content/mount/My Drive/00_work/guidance/20200701/02_confirm/'\n","similarity_cal(columns,path,outputdir)\n","\n","#03_detail\n","columns='detail'\n","path='/content/mount/My Drive/00_work/guidance/20200701/03_detail/'\n","similarity_cal(columns,path,outputdir)\n","\n","#04_word\n","columns='synonyms'\n","path='/content/mount/My Drive/00_work/guidance/20200701/04_word/'\n","similarity_cal(columns,path,outputdir)\n","\n","\n","\n","#05_all\n","columns='all'\n","path='/content/mount/My Drive/00_work/guidance/20200701/05_all/'\n","similarity_cal(columns,path,outputdir)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z7FnASEeELbO","colab_type":"text"},"source":["#BM25"]},{"cell_type":"code","metadata":{"id":"qisapCbkENxf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"c8e73679-8a50-40dd-c30d-24b0f618b4b1"},"source":["# https://radimrehurek.com/gensim/summarization/bm25.html\n","from gensim.summarization.bm25 import get_bm25_weights\n","from gensim.summarization import bm25\n","import numpy as np\n","corpus = [\n","    [\"black\", \"cat\", \"white\", \"cat\"],\n","    [\"cat\", \"outer\", \"space\"],\n","    [\"wag\", \"dog\"]\n","]\n","corpus=[\n","        ['顔','腫れ','がり'],\n","['額','ケロイド','出来','たの','に'],\n","['重度','肢体','不自由者','又は','重度','知的','障害'],\n","['若しく','精神障害','行動','上著しい','困難'],\n","['若しく','精神障害','行動','上著しい','困難'],\n","['若しく','精神障害','行動','上著しい','困難'],\n","['若しく','精神障害','行動','上著しい','困難'],\n","['若しく','精神障害','行動','上著しい','困難'],\n","['有する者','あって','常に','介護','必要','人']\n","]\n","# http://lixinzhang.github.io/implementation-of-okapi-bm25-on-python.html\n","result = get_bm25_weights(corpus, n_jobs=-1)\n","print(np.array(result).shape)\n","print(result)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(9, 9)\n","[[6.391854356303302, 0, 0, 0, 0, 0, 0, 0, 0], [0, 8.758688095263764, 0, 0, 0, 0, 0, 0, 0], [0, 0, 11.866107055906536, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1.7010743260162065, 1.7010743260162065, 1.7010743260162065, 1.7010743260162065, 1.7010743260162065, 0], [0, 0, 0, 1.7010743260162065, 1.7010743260162065, 1.7010743260162065, 1.7010743260162065, 1.7010743260162065, 0], [0, 0, 0, 1.7010743260162065, 1.7010743260162065, 1.7010743260162065, 1.7010743260162065, 1.7010743260162065, 0], [0, 0, 0, 1.7010743260162065, 1.7010743260162065, 1.7010743260162065, 1.7010743260162065, 1.7010743260162065, 0], [0, 0, 0, 1.7010743260162065, 1.7010743260162065, 1.7010743260162065, 1.7010743260162065, 1.7010743260162065, 0], [0, 0, 0, 0, 0, 0, 0, 0, 9.652215550143493]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NNqbOn8RiIw_","colab_type":"text"},"source":["## bm25 gens\n"]},{"cell_type":"code","metadata":{"id":"247HBk7AiMxz","colab_type":"code","colab":{}},"source":["# https://programtalk.com/vs2/python/10822/gensim/gensim/summarization/bm25.py/\n","#!/usr/bin/env python\n","# -*- coding: utf-8 -*-\n","#\n","# Licensed under the GNU LGPL v2.1 - http://www.gnu.org/licenses/lgpl.html\n"," \n","import math\n","from six import iteritems\n","from six.moves import xrange\n"," \n"," \n","# BM25 parameters.\n","PARAM_K1 = 1.5\n","PARAM_B = 0.75\n","EPSILON = 0.25\n"," \n"," \n","class BM25(object):\n"," \n","    def __init__(self, corpus):\n","        self.corpus_size = len(corpus)\n","        self.avgdl = sum(map(lambda x: float(len(x)), corpus)) / self.corpus_size\n","        self.corpus = corpus\n","        self.f = []\n","        self.df = {}\n","        self.idf = {}\n","        self.initialize()\n"," \n","    def initialize(self):\n","        for document in self.corpus:\n","            frequencies = {}\n","            for word in document:\n","                if word not in frequencies:\n","                    frequencies[word] = 0\n","                frequencies[word] += 1\n","            self.f.append(frequencies)\n"," \n","            for word, freq in iteritems(frequencies):\n","                if word not in self.df:\n","                    self.df[word] = 0\n","                self.df[word] += 1\n"," \n","        for word, freq in iteritems(self.df):\n","            self.idf[word] = math.log(self.corpus_size-freq+0.5) - math.log(freq+0.5)\n"," \n","    def get_score(self, document, index, average_idf):\n","        score = 0\n","        for word in document:\n","            if word not in self.f[index]:\n","                continue\n","            idf = self.idf[word] if self.idf[word] >= 0 else EPSILON * average_idf\n","            score += (idf*self.f[index][word]*(PARAM_K1+1)\n","                      / (self.f[index][word] + PARAM_K1*(1 - PARAM_B+PARAM_B*self.corpus_size / self.avgdl)))\n","        return score\n"," \n","    def get_scores(self, document, average_idf):\n","        scores = []\n","        for index in xrange(self.corpus_size):\n","            score = self.get_score(document, index, average_idf)\n","            scores.append(score)\n","        return scores\n"," \n"," \n","def get_bm25_weights(corpus):\n","    bm25 = BM25(corpus)\n","    average_idf = sum(map(lambda k: float(bm25.idf[k]), bm25.idf.keys())) / len(bm25.idf.keys())\n"," \n","    weights = []\n","    for doc in corpus:\n","        scores = bm25.get_scores(doc, average_idf)\n","        weights.append(scores)\n"," \n","    return weights"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IMgH7JBVopXj","colab_type":"text"},"source":["## sample"]},{"cell_type":"code","metadata":{"id":"1kzO25wUPPVR","colab_type":"code","colab":{}},"source":["from gensim.summarization.bm25 import get_bm25_weights\n","import numpy as np\n","#顔 腫れ がり、額 ケロイド 出来 たの に \n","corpus = [\n","    [\"black\", \"cat\", \"white\", \"cat\"],\n","    [\"cat\", \"outer\", \"space\"],\n","    [\"wag\", \"dog\"]\n","]\n","corpus=[\n","        ['顔','腫れ','がり'],\n","['額','ケロイド','出来','たの','に'],\n","['重度','肢体','不自由者','又は','重度','知的','障害'],\n","['若しく','精神障害','行動','上著しい','困難'],\n","['有する者','あって','常に','介護','必要','人']\n","]\n","result = get_bm25_weights(corpus, n_jobs=-1)\n","print(np.array(result).shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pU5g07sMO_NM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"cbd9f592-b6de-44c4-ad4e-cbaefee6d8d9"},"source":["from gensim.summarization import bm25\n","import pandas as pd\n","\n","corpus=[['顔','腫れ上がり','額','ケロイド','出来'],\n","['間','頭','窓','ぶつけ','腕','つめ','たて','血','で'],\n","['あざ','なっ','血','出','知っ'],\n","['顔','殴ら','眉毛','した','赤く','なり','唇','切れ'],\n","['叩か','頬','腫れ','鼻血','垂らし'],\n","['手首','水','ぶ','でき','体','燃える','熱い'],\n","['リスカ','し'],\n","['痣','残る','叩い'],\n","['眼球','出血','引か','一週間','眼帯','し','中学校','通っ','期間','あり'],\n","['四度','首','絞め'],\n","['噛ん','跡','ミミズバレ','腫れ','内出血','し','痣','なる'],\n","['首に','切り傷','入れ'],\n","['発端','母','第三者','身体','許し','はじまり','小学校','入学','し','夏','間','長い','当たり前','第三者','身体','弄ら','性行為','し','言葉','暴言','夏の終わり','首','付け根','抑え','息','出来','なり','死ぬ','しれ','暴力','受け'],\n","['鼻血','出る','殴ら','妹','殴ら','鼓膜','破れ','母','骨折','し'],\n","['いつものように','急','殴り','き','馬乗り','なり','両手','捕まれ','叩か','鼻血','出て','横','流れる','抵抗','し','口元','流れ','母','手','い','鼻血','トレーナー','拭い','赤','グレー','色','トレーナー','拭い','運','悪く','グレー','とこ','服','汚し','突か','グレー','汚す','キレ','殴ら'],\n","['頭','割れ','思う','痛く','殺さ','思い'],\n","['腕','切る','血','出る','髪の毛','抜く'],\n","['急','父','顔色','変わり','頭','オムライス','かけ','熱','もっ','フライパン','叩い'],\n","['髪','引っ張っ','殴っ','お腹','蹴ら'],\n","['ポット','熱湯','腕','流さ','包丁','頭','切ら'],\n","['ネグレクト','受けて','育ち'],\n","['子ども','連れ','実家','帰っ','パートナー','パチンコ','行っ'],\n","['母親','子供','祖母','病気','留守番','さ','ご飯','作ら','話し'],\n","['母親','生後','弟','産む','私達','残し','浮気','相手','行っ'],\n","['女の子','産ん','女の子','服','物','与え'],\n","['私服','無い'],\n","['暴力','ネグレクト','なかっ','支配','接し','さ','理不尽','当たら','無視','さ','学業','評価','し','子供','時代'],\n","['ネグレクト','カビの生えた','生乾き','下着','服','着','風呂','週','ゴミ屋敷','コンビニ弁当','食べ','精神的','身体的','ボロボロ']]\n","document1=['父','居','夕飯','兄','呼ば','二人','食べる']\n","\n","document=['精神障害','又は','ケロイド','出来','たの','に']\n","def get_scores_doc(bm25,keywords_list):\n","  average_idf = sum(map(lambda k: float(bms.idf[k]), bms.idf.keys())) / len(bms.idf.keys())\n","  sc=bm25.get_scores(keywords_list,average_idf)\n","  return sc\n","def get_nmax(dt_list,n):\n","  col='doc'\n","  df=pd.DataFrame(dt_list,columns=[col])\n","  dfs=df.sort_values([col], ascending=False)\n","  dt_list=dfs.values.tolist()\n","  res=[]\n","  doc_ids=list(dfs.index)\n","  for doc_id , i in zip(doc_ids[0:n],range(n)):\n","    # print(doc_id,dfs.iloc[i,:])\n","    res.append([doc_id,dt_list[i][0]])\n","  return res\n","bms=bm25.BM25(corpus)\n","sc=get_scores_doc(bms,document1)\n","dt_nmax=get_nmax(sc,5)\n","print(dt_nmax)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[17, 2.623137390356224], [0, 0.0], [1, 0.0], [26, 0.0], [25, 0.0]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7vY8G2DGfhHW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"e41a4c9d-9358-4b0a-e24f-1250961b4760"},"source":["from gensim.summarization import bm25\n","import pandas as pd\n","\n","corpus=[['顔','腫れ上がり','額','ケロイド','出来'],\n","['間','頭','窓','ぶつけ','腕','つめ','たて','血','で'],\n","['あざ','なっ','血','出','知っ'],\n","['顔','殴ら','眉毛','した','赤く','なり','唇','切れ'],\n","['叩か','頬','腫れ','鼻血','垂らし'],\n","['手首','水','ぶ','でき','体','燃える','熱い'],\n","['リスカ','し'],\n","['痣','残る','叩い'],\n","['眼球','出血','引か','一週間','眼帯','し','中学校','通っ','期間','あり'],\n","['四度','首','絞め'],\n","['噛ん','跡','ミミズバレ','腫れ','内出血','し','痣','なる'],\n","['首に','切り傷','入れ'],\n","['発端','母','第三者','身体','許し','はじまり','小学校','入学','し','夏','間','長い','当たり前','第三者','身体','弄ら','性行為','し','言葉','暴言','夏の終わり','首','付け根','抑え','息','出来','なり','死ぬ','しれ','暴力','受け'],\n","['鼻血','出る','殴ら','妹','殴ら','鼓膜','破れ','母','骨折','し'],\n","['いつものように','急','殴り','き','馬乗り','なり','両手','捕まれ','叩か','鼻血','出て','横','流れる','抵抗','し','口元','流れ','母','手','い','鼻血','トレーナー','拭い','赤','グレー','色','トレーナー','拭い','運','悪く','グレー','とこ','服','汚し','突か','グレー','汚す','キレ','殴ら'],\n","['頭','割れ','思う','痛く','殺さ','思い'],\n","['腕','切る','血','出る','髪の毛','抜く'],\n","['急','父','顔色','変わり','頭','オムライス','かけ','熱','もっ','フライパン','叩い'],\n","['髪','引っ張っ','殴っ','お腹','蹴ら'],\n","['ポット','熱湯','腕','流さ','包丁','頭','切ら'],\n","['ネグレクト','受けて','育ち'],\n","['子ども','連れ','実家','帰っ','パートナー','パチンコ','行っ'],\n","['母親','子供','祖母','病気','留守番','さ','ご飯','作ら','話し'],\n","['母親','生後','弟','産む','私達','残し','浮気','相手','行っ'],\n","['女の子','産ん','女の子','服','物','与え'],\n","['私服','無い'],\n","['暴力','ネグレクト','なかっ','支配','接し','さ','理不尽','当たら','無視','さ','学業','評価','し','子供','時代'],\n","['ネグレクト','カビの生えた','生乾き','下着','服','着','風呂','週','ゴミ屋敷','コンビニ弁当','食べ','精神的','身体的','ボロボロ']]\n","doc1=['父','居','夕飯','兄','呼ば','二人','食べる']\n","\n","doc2=['精神障害','又は','ケロイド','出来','たの','に']\n","\n","average_idf = sum(map(lambda k: float(bms.idf[k]), bms.idf.keys())) / len(bms.idf.keys())\n","bms=bm25.BM25(corpus)\n","ref_doc1=bms.get_scores(doc1,average_idf)\n","\n","ref_doc2=bms.get_scores(doc2,average_idf)\n","print(ref_doc1)\n","print(ref_doc2)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2.623137390356224, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[6.553934677157629, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.1109901181731865, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6xtLTqaqovQL","colab_type":"text"},"source":["## source"]},{"cell_type":"code","metadata":{"id":"IN7inie2ouih","colab_type":"code","colab":{}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from gensim.summarization import bm25\n","def get_sim_words(str,r=0.8):\n","  wds=[]\n","  rvs=[]\n","  if str in model_w2v:\n","    for sims in model_w2v.most_similar(str):\n","      if sims[1]>r:\n","        wds.append( sims[0])\n","        rvs.append( sims[1])\n","  return wds,rvs\n","import pandas as pd\n","def get_stopwords():\n","  stopwords_df=pd.read_csv('/content/mount/My Drive/00_work/guidance/stopwords.csv',sep=',',header=None)\n","  stopwords_list=stopwords_df[0].values.tolist()\n","  return stopwords_list\n","def clean_words(wd,stopwords_list):\n","\tres=wd\n","\tif res in stopwords_list:\n","\t  res=''\n","\treturn res\n","def get_scores_doc(bms,keywords_list):\n","  average_idf = sum(map(lambda k: float(bms.idf[k]), bms.idf.keys())) / len(bms.idf.keys())\n","  sc=bms.get_scores(keywords_list,average_idf)\n","  return sc\n","def get_nmax(dt_list,n):\n","  col='doc'\n","  df=pd.DataFrame(dt_list,columns=[col])\n","  dfs=df.sort_values([col], ascending=False)\n","  dt_list=dfs.values.tolist()\n","  res=[]\n","  doc_ids=list(dfs.index)\n","  for doc_id , i in zip(doc_ids[0:n],range(n)):\n","    # print(doc_id,dfs.iloc[i,:])\n","    res.append([doc_id,dt_list[i][0]])\n","  return res\n","\n","# 正規表現で調べる方法：データをクレンジング\n","def check_by_regex(s):\n","  ret= bool(re.search(r\"[a-zA-Z]\", s)) or \\\n","           bool(re.search(r\"[0-9]\", s))\n","  return not ret\n","# Pearson Correlation　最大値を採用　●\n","def pearson_coef_similarity(x,y):\n","  x_=x-np.mean(x)\n","  y_=y-np.mean(y)\n","  return np.dot(x_,y_)/(np.linalg.norm(x_)*np.linalg.norm(y_))\n","#Euclidean Distance 最小値を採用　● 1-x\n","def euclidean_distance_similarity(x,y):\n","  return np.sqrt(np.sum(np.square(x-y)))\n","#Cosine Similarity　●\n","def cosine_similarity(xs,ys):\n","  return np.dot(xs,ys)/(np.linalg.norm(xs)*np.linalg.norm(ys))\n","# Jaccard Similarity ×\n","# stopwords_list=['〇〇','あぁ!','い','いう','いか','いる','いろいろ','き','く','くる','さい','ささい','し','した','しない','す','する','で','でき','できる','できれ','とこ','とっ','ない','ないか','なかっ','なく','なさ','なし','なっ','なら','なり','なる','ひとり','ふたつ','ぶ','み','もし','もっ','一','二','三','四','五','六','七','八','九','十','百','千','0','1','2','3','4','5','6','7','8','9','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\n","\n","\n","filterList_nv=[\n","['動詞','自立'],\n","['名詞','サ変接続'],\n","['名詞','ナイ形容詞語幹'],\n","['名詞','一般'],\n","['名詞','固有名詞'],\n","['名詞','形容動詞語幹'],\n","# ['形容詞','自立'],\n","]\n","\n","filterList_nva=[\n","['動詞','自立'],\n","['名詞','サ変接続'],\n","['名詞','ナイ形容詞語幹'],\n","['名詞','一般'],\n","['名詞','固有名詞'],\n","['名詞','形容動詞語幹'],\n","['形容詞','自立'],\n","]\n","def mt_min_max_normal(mt):\n","  mt=np.array(mt,dtype=np.float32)\n","  mt_max=np.max(mt)\n","  mt_min=np.min(mt)\n","  mt_res=(mt-mt_min)/(mt_max-mt_min)\n","  return list(mt_res)\n","def get_key_word(remark,nva_flg,cls_flg=False):\n","  lines=m.parse(remark).split('\\n')\n","  keywords=[]\n","  stopwords_list=get_stopwords()\n","  filterList=None\n","  if nva_flg:\n","    filterList=filterList_nva\n","  else:\n","    filterList=filterList_nv\n","  for line in lines:\n","    items = re.split('[\\t]',line)\n","    if len(items) > 1:\n","      subItems= items[1].split(',')\n","      for f in filterList:\n","        if subItems[0]==f[0] and subItems[1]==f[1]:\n","          item_ap=''\n","          if cls_flg and check_by_regex(items[0]):\n","            item_ap=clean_words(items[0],stopwords_list)\n","          else:\n","            item_ap=items[0]\n","          if len(item_ap)>0:\n","            keywords.append(items[0])\n","  res=[]\n","  # delete duplicate data\n","  [res.append(x) for x in keywords if x not in res]\n","  return np.array(res)\n","#mecab を使って、形態素解析し、単語へ\n","def sentence_spt_short(sent):\n","\tsps=['・','　','。','、']\n","\tresult = re.sub(r'[・　。、]+', ',', sent)\n","\tresult = result.replace(\",,\",\",\")\n","\tres=result.split(',')\n","\treturn res\n","\n","def create_docs(guidan_docs,remark_docs,nva_flg,sim_flg=False,rt=0.8):\n","  dv_guidan=[]\n","  dv_remark=[]\n","  lenGuid=len(guidan_docs)\n","  def ext_sims(word_list,rt):\n","    res=[]\n","    for str in word_list:\n","        if len(str)>0:\n","          res.append(str)\n","          wds,rts=get_sim_words(str,rt)\n","          if len(wds)>0:\n","            res.extend(wds)\n","    return res\n","  for doc_id,doc in zip(range(lenGuid),guidan_docs):\n","    word_list = get_key_word(doc,nva_flg,cls_flg=True)# get_words(res_sub) #\n","    if sim_flg:\n","      word_list=ext_sims(word_list,rt)\n","    dv_guidan.append(word_list)\n","\n","  for doc_id ,doc in zip(range(len(remark_docs)),remark_docs):\n","    word_list = get_key_word(doc,nva_flg,cls_flg=True)# get_words(res_sub) #\n","    if sim_flg:\n","      word_list=ext_sims(word_list,rt)\n","    dv_remark.append(word_list)\n","  \n","  \n","  return   dv_guidan,dv_remark\n","\n","def load_remark_guidance_dt(col,pathout):\n","  json_remark = json.load(open(pathout+\"sentence.json\", \"r\"))\n","  json_title = json.load(open(pathout+\"word_\"+col+\".json\", \"r\"))\n","\n","  remark_docs=[]\n","  guidan_docs=[]\n","  cnt_r=len(json_remark)\n","  for i,remark in zip(range(cnt_r),json_remark):\n","    remark_docs.append(remark['sentence'])\n","  cnt_t=len(json_title)\n","  for i, title in  zip(range(cnt_t),json_title):\n","    guidance=title['word']+\"。\"+title[col]\n","    guidan_docs.append(guidance)\n","  return remark_docs,guidan_docs,json_remark,json_title\n","\n","def similarity_cal(col_in,path,outputdir,nva_flg=False,keycnt=3,sim_flg=False,rt=0.8):\n","  remark_docs,guidan_docs,json_remark,json_title=load_remark_guidance_dt(col_in,path)\n","  dt_guidances,dt_remarks=create_docs(guidan_docs,remark_docs,nva_flg,sim_flg=sim_flg,rt=rt)\n","  sim_out_list=[]\n","  bms=bm25.BM25(dt_guidances)\n","  print('guidan_docs:',len(dt_guidances),len(guidan_docs))\n","  for remark_id , dt in zip(range(len(dt_remarks)),dt_remarks):\n","    sc=get_scores_doc(bms,dt)\n","    dt_nmax=get_nmax(sc,keycnt)\n","    for i in range(keycnt):\n","      guidan_id=dt_nmax[i][0]\n","      docs_ref=dt_nmax[i][1]\n","\n","      sim_out_list.append([\n","                            json_remark[remark_id]['sentence'],\n","                            json_title[guidan_id]['word'],\n","                            json_title[guidan_id][col_in],\n","                            docs_ref])\n","  \n","  nva='nv_'\n","  if nva_flg:\n","    nva='nva_'\n","  fn=nva+col_in+\"_similarity.csv\"\n","  df_out=pd.DataFrame(sim_out_list,columns=['sentence','word',col_in,'docs_sim'])\n","  df_out.to_csv(outputdir+fn,index=None)\n","  \n","\n","  def out_keywords(docs,keywords):\n","    out_dt=[]\n","    for doc, key in zip(docs,keywords):\n","      out_dt.append([doc,key])\n","    return out_dt\n","  df_re=out_keywords(remark_docs,dt_remarks)\n","  df_out=pd.DataFrame(df_re,columns=['doc','keywords'])\n","  df_out.to_csv(outputdir+\"remarks_keywords.csv\",index=None)\n","  \n","  df_re=out_keywords(guidan_docs,dt_guidances)\n","  df_out=pd.DataFrame(df_re,columns=['doc','keywords'])\n","  nva='nv_'\n","  if nva_flg:\n","    nva='nva_'\n","  fn=nva+col_in+\"_guidance_keywords.csv\"\n","  df_out.to_csv(outputdir+fn,index=None)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oKdbN0VH72Ob","colab_type":"text"},"source":["##excu"]},{"cell_type":"code","metadata":{"id":"mKYNdmKV75U_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"7ef284ff-9bec-4ad0-f08d-13a1df5e10ff"},"source":["import re\n","import warnings\n","warnings.simplefilter('ignore')\n","keycnt=3\n","sim_flg=True\n","rt=0.9\n","outputdir='/content/mount/My\\ Drive/00_work/guidance/output/bm25_VN_0730/'\n","%mkdir {outputdir}\n","outputdir='/content/mount/My Drive/00_work/guidance/output/bm25_VN_0730/'\n","\n","# 07_tcd\n","col='tcd'\n","path='/content/mount/My Drive/00_work/guidance/20200701/07_tcd/'\n","similarity_cal(col,path,outputdir,nva_flg=True,keycnt=keycnt,sim_flg=sim_flg,rt=rt)\n","# 07_tcd\n","col='tcd'\n","path='/content/mount/My Drive/00_work/guidance/20200701/07_tcd/'\n","similarity_cal(col,path,outputdir,nva_flg=False,keycnt=keycnt,sim_flg=sim_flg,rt=rt)\n","\n","# # 01_title\n","# columns='title'\n","# path='/content/mount/My Drive/00_work/guidance/20200701/01_title/'\n","# similarity_cal(columns,path,outputdir,keycnt=keycnt,sim_flg=sim_flg,rt=rt)\n","\n","# #02_confirm\n","# columns='confirmation'\n","# path='/content/mount/My Drive/00_work/guidance/20200701/02_confirm/'\n","# similarity_cal(columns,path,outputdir,keycnt=keycnt,sim_flg=sim_flg,rt=rt)\n","\n","# #03_detail\n","# columns='detail'\n","# path='/content/mount/My Drive/00_work/guidance/20200701/03_detail/'\n","# similarity_cal(columns,path,outputdir,keycnt=keycnt,sim_flg=sim_flg,rt=rt)\n","\n","# #04_word\n","# columns='synonyms'\n","# path='/content/mount/My Drive/00_work/guidance/20200701/04_word/'\n","# similarity_cal(columns,path,outputdir,keycnt=keycnt,sim_flg=sim_flg,rt=rt)\n","\n","# #05_all\n","# columns='all'\n","# path='/content/mount/My Drive/00_work/guidance/20200701/05_all/'\n","# similarity_cal(columns,path,outputdir,keycnt=keycnt,sim_flg=sim_flg,rt=rt)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory ‘/content/mount/My Drive/00_work/guidance/output_bm25_VN_0730/’: File exists\n","guidan_docs: 274 274\n","guidan_docs: 274 274\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cov86UM_JLqo","colab_type":"code","colab":{}},"source":["def get_sim_words(str,r=0.8):\n","  wds=[]\n","  rvs=[]\n","  if str in model_w2v:\n","    for sims in model_w2v.most_similar(str):\n","      if sims[1]>r:\n","        wds.append( sims[0])\n","        rvs.append( sims[1])\n","  return wds,rvs\n","get_sim_words('家庭',0.7)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TMyII8wqP28I","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1R0Tavm2P3SX","colab_type":"text"},"source":["#sentence split"]},{"cell_type":"markdown","metadata":{"id":"jMicBHc1P7X9","colab_type":"text"},"source":["##sample"]},{"cell_type":"code","metadata":{"id":"EPBUWj3YQXrn","colab_type":"code","colab":{}},"source":["text=['顔が腫れ上がり、額にケロイドが出来たのに…',\n","'この間なんか頭を窓にぶつけられ、腕につめをたてられて、血がでました',\n","'それが青あざになって血が出てるの知ってる?',\n","'昨日も顔を殴られて眉毛のしたが赤くなり唇が切れました',\n","'叩かれすぎて頬も腫れ、鼻血も垂らしました',\n","'手首に水ぶくれができて体中が燃えるように熱いんです!!',\n","'リスカもいっぱいしてます',\n","'最近は痣が一ヶ月以上残るほど叩いてきます・・・',\n","'眼球の出血が引かず、一週間眼帯をして中学校へ通った期間があります',\n","'四度ほど、首を絞められました',\n","'噛んだ跡がミミズバレのように腫れて、その後内出血をしたような痣になるんです',\n","'私が首に切り傷を入れた時',\n","'発端は母が第三者に身体を許してたことからはじまり小学校を入学して直ぐから去年の夏頃から二カ月の間まで長い間当たり前の様に第三者から身体を弄られ性行為をしないと言葉による暴言や去年の夏の終わりから首の付け根を抑えこまれ息が出来なくなり死ぬかもしれない暴力を受けました',\n","'鼻血が出るまで殴られ続け妹は殴られて鼓膜が破れ、母は骨折しました',\n","'またいつものように急に殴りにきて、私に馬乗りになり、両手を捕まれ叩かれてたときに、やっぱり鼻血が出て、いつもは横に流れるんですけど、そのとき抵抗をしてたので口元に流れてき、母の手を一瞬ほどいて鼻血をトレーナーで拭いてしまったら、そのとき赤とグレーの色のトレーナーで、拭いたところがなぜか運悪くグレーのとこで、まぁ服を汚したらど突かれるんですけど、なぜグレーに汚す!とよけいキレられ、殴られ続けてぐったり',\n","'頭も、割れたかと思うくらい痛くて、殺されると思いました',\n","'それは腕を切るや血が出ることではなく、髪の毛を抜くことでした',\n","'そしたら急に父の顔色が変わり、私の頭にオムライスをかけ、まだ熱をもっているフライパンで私を叩いたのです',\n","'髪を引っ張っては殴ったりお腹を思いっきり蹴られます',\n","'ポットの熱湯を腕に流されたり、包丁で頭を切られたり･･･',\n","'ネグレクトを受けて育ちました',\n","'子どもを連れて実家に帰っても、パートナーとパチンコへ行ってしまったり',\n","'母親が子供の頃、私の祖母が病気のため、長時間留守番をさせ、ご飯を作らなかったなどと話しています',\n","'そんな母親は、生後数ヶ月の弟を産むと、私達を残しあっさり浮気相手の所に行ってしまいました',\n","'女の子に産んでおきながら、女の子らしい服や物を与えない',\n","'もちろん私服など無い',\n","'暴力やネグレクトはなかったものの、支配的な接し方をされたり、理不尽に当たられたり、無視されたり、学業でしか評価してもらえなかったりといったような子供時代でした',\n","'また、ネグレクトで常にカビの生えた生乾きの下着や服を着ており、お風呂は週一、ゴミ屋敷の中でコンビニ弁当を食べたりと、精神的、身体的に毎日ボロボロでした',\n","'父が居ない夕飯は、兄だけ呼ばれ二人で食べる',\n","'もちろん食事なんてくれません',\n","'竹刀で叩かれたり、ネグレクトもありました',\n","'食事も父のだけを一生懸命作る人でした',\n","'お金を私達に渡すとパチンコ三昧の日々',\n","'無関心無視するなら産んでほしくなかった',\n","'食事はまともに食べさせてもらえず、痩せすぎて、近所ではあの家の子供は上の子の方は太ってるのに下の子は痩せすぎてるし、しょっちゅう叫び声がする(暴力されるから)虐待だと言っているそうだとクラスメイトに教えてもらった',\n","'そしたらもう今現在、無視or怒鳴るのどちらかでしか私には接してくれなくなりました',\n","'毎晩起こしたけいれんを放置したのは、一人で生きて行けるようになる教育だったそうです',\n","'そんな環境か、両親は不安定でわたしは育児放棄も受けて育ったようでした',\n","'食事も別、母や兄弟に話しかけても無視されあまりにも辛く家に居場所がなく小学生で自殺を企てた事もあります',\n","'我が家は貧乏ではなかったのに学校で使う裁縫道具(2000円ぐらい)を買ってもらえませんでした',\n","'小学高学年になってから、父親は、私がお風呂に入っているのを覗きにきて、性器を見せるようにいうようになりました',\n","'パパがを陰部を出し娘に押しつけているのか・・・・陰部を出しているのは確かです',\n","'また、父は姉に性的いたずらをしていました',\n","'裸にさせられて、胸を鷲掴みされました',\n","'小学生のとき夜いきなり布団に潜ってきてむねを触られたり下着の上から下半身を触られたりしました',\n","'父親には小さい頃、セクハラされてた',\n","'よく風呂場で縛られてレイプされてた',\n","'そしたら義父が…ゆりちゃんのおっぱい結構でかいねって言ってきて…無視してたら触ってきました',\n","'祖母はよく私の部屋にノックなしで入ってきて、私が薬を塗るため服を脱いでいた事があったのですが、「あの子おっぱい丸出しでいたよ」と父と祖父もいるリビングでわざわざ言いました',\n","'娘に対して性的な目を向けているのが残念でなりません',\n","'中学3年からは兄が性器をあててきて、気持ちわるくてたまりませんでした',\n","'裸になった父親が抱きついてきて勃起しているものを私に押し付けてきたり、トイレに入るとどんどんドアを叩き、トイレを使わせないなど、思春期の私はすごく辛く怖い思いをしました',\n","'3歳頃、教祖の性暴力から泣いて逃げたら、両親に両手足を抑えられ、すでに老人であったら教祖から身体を弄ばれました',\n","'ドライブに連れて行かれてはキスされ、ﾌｪﾗもしてと言われました',\n","'父の唾液がまだ性器の周りに残っているような気さえするときもあります',\n","'父が裸を見てくる事を17才頃、母と祖母に相談したら驚かれました',\n","'その後に、向けたちんちんにお湯を掛けろ、大きくしろと言われ、',\n","'無理やりキスをされたり、性器を舐められたりされました',\n","'スカートの脚をじろじろ見てくる、両親のセックスの場面を見せようとする、風呂から上がるタイミングで脱衣所の洗濯機置き場に度々いる',\n","'私は中学1年生まで実父に性的虐待をうけていました',\n","'その後、育児に全く無関心だった父のもとで育てられました',\n","'この病は育児への理想と現実のギャップから母親がノイローゼになってしまったり、育児による辛さや不安をまわりに吐き出せないままストレスを抱えてしまうことからなると考えられています',\n","'その家は父子家庭であるそうです',\n","'弟夫婦は共働きで忙しく、平日はほどんど母親が面倒を見ています',\n","'父は仕事柄週のうち半分は仕事で、その間家には知らない男の人が入れ替わり立ち代わり、時には泊まって行くこともありました',\n","'そんな父は現在単身赴任中で県外にいるため、相談することが難しいです',\n","'でも、うちのしつけが他の家庭からしたら、少しおかしいと知ったのは親から離れてからです',\n","'戸籍すら作られていないこどもが現実にいます',\n","'親戚は、子供の顔が見たいと、直ぐに飛んで来る様に、週1日程度で、来てくれますが、母親は忙しい!遠い!となかなか来ません',\n","'車で2分の距離も滅多に帰らなくなりました',\n","'児相の介入が必要としない場合でも助けを求めている人がいることを知ってほしいです',\n","'放任主義で育てられた人が、親になって、その子供が',\n","'毎日、一人っ子のその家の子の相手として夜はその子が寝付くまで世話をしていました',\n","'(おもに父)母は転職癖がある父のかわりにフルで外で働いていたため私の養育はおもに父と祖母がすることになりました',\n","'わたしが母を怒らせると、｢産まなきゃよかった｣｢死ねばいいのに｣と言われました',\n","'暴力やネグレクトはなかったものの、支配的な接し方をされたり、理不尽に当たられたり、無視されたり、学業でしか評価してもらえなかったりといったような子供時代でした',\n","'私の家は母子家庭です',\n","'母を亡くし父子二人だけの生活',\n","'弟は小学生のころにアスペルガー症候群と発覚し、他人とは違う行動をするため、学校から電話がかかってき、それを聞いた母は毎日怒鳴っています',\n","'子供の帰る場所は家しかない…なのにその家が暴力に溢れていたら、私達子供はどうなってしまうのでしょうか',\n","'ですよね苦笑',\n","'頑張って!',\n","'でも仕方ないのです',\n","'ラッキー!!',\n","'本当は苦しい',\n","'応援しています!',\n","'私たちは悪くないのに!!',\n","'自分だけ辛いって',\n","'あぁ!そうですよね',\n","'気の毒なことです',\n","'可哀想な人なんだ',\n","'妹のバンド活動に伴い、遠方に住んでいる女の子のメンバーが数人、',\n","'自分の地域は新興住宅地で若い夫婦が多い所為もあって小さい子が多いのですが',\n","'私のところは家の斜め前で5～6人の子供が遊んでます',\n","'受験大丈夫?',\n","'酸素のない狭い水槽いる魚のように焦り、もがき暴れているのです',\n","'生まれてこのかた、拳で人を殴ったこともないのに、何でこんな事に？裁判でしゅくしゅくとやるしかないのですが、ちょっとおかしすぎます',\n","'DV保護法、その支援策は「もっともだ」と思っていましたが、実際自分が当事者になると、自分が全く反論も証明もする機会を与えられず、DV・虐待の加害者に…',\n","'顕在意識と潜在意識の関係に似ている',\n","'「私は皆から嫌われる存在',\n","\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S6fMmCzPgMIw","colab_type":"text"},"source":["## common"]},{"cell_type":"code","metadata":{"id":"YpIvboEhfm2c","colab_type":"code","colab":{}},"source":["def pre_process_sen(sent):\n","  result=sent\n","  # result=half2all(result)\n","  \n","  splst=[\n","        ['　',''], # 重要\n","        # [' ',''], # 重要\n","         ]\n","  for sp in splst:\n","    result=result.replace(sp[0],sp[1])\n","  return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZAIm14p1jAsH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1598945107063,"user_tz":-540,"elapsed":993,"user":{"displayName":"于勝龍","photoUrl":"","userId":"09684187088444140086"}},"outputId":"04820962-0b55-4165-8385-9f03f9a59e01"},"source":["sent='顔が腫れ上がり、  額に　ケロイドが出来たのに…。'\n","pre_process_sen(sent)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'顔が腫れ上がり、額にケロイドが出来たのに。'"]},"metadata":{"tags":[]},"execution_count":153}]},{"cell_type":"markdown","metadata":{"id":"_p5pbjrd7ant","colab_type":"text"},"source":["## 00_src"]},{"cell_type":"code","metadata":{"id":"alaAz4c-4Rd7","colab_type":"code","colab":{}},"source":["import re\n","import pandas as pd\n","import json\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","def short_concat(res,cnt=5):\n","  rc=[]\n","  sec=''\n","  lcnt=len(res)\n","  for s in res:\n","    sec=sec+s\n","    if len(sec)>=cnt:\n","      rc.append(sec)\n","      sec=''\n","  #last \n","  if len(sec)<cnt and len(sec)>0:\n","     rc.append(sec)\n","  return rc\n","def sentence_spt_short(sent):\n","  sps=['…','･･･','※']\n","  for s in sps:\n","    sent=sent.replace(s,',')\n","  # clean\n","  sps=['-']\n","  for s in sps:\n","    sent=sent.replace(s,'')\n","  result = re.sub(r'[・　。、･]+', ',', sent)\n","  result = result.replace(\",,\",\",\")\n","  resplit=result.split(',')\n","  res=short_concat(resplit)\n","  return res\n","def load_remark_guidance_dt(col,pathout):\n","  json_remark = json.load(open(pathout+\"sentence.json\", \"r\"))\n","  json_title = json.load(open(pathout+\"word_\"+col+\".json\", \"r\"))\n","\n","  remark_docs=[]\n","  guidan_docs=[]\n","  json_remark_n=[]\n","  cnt_r=len(json_remark)\n","  for i,remark in zip(range(cnt_r),json_remark):\n","    remark_docs.append(remark['sentence'])\n","    json_remark_n.append(remark)\n","    if i ==79:\n","      break\n","  cnt_t=len(json_title)\n","  for i, title in  zip(range(cnt_t),json_title):\n","    if title['word'] not in title[col]:\n","      guidance=title['word']+\"。\"+title[col]\n","    else:\n","      guidance=title[col]\n","    guidan_docs.append(guidance)\n","  return remark_docs,guidan_docs,json_remark_n,json_title\n","def get_sim_s3g(sen_s,sen_g,cnt=3):\n","  vec_s=embed_ml3(sen_s)\n","  vec_g=embed_ml3(sen_g)\n","  # vec_s=model.encode(sen_s)\n","  # vec_g=model.encode(sen_g)\n","  sim=cosine_similarity( vec_s, vec_g)\n","  # sim= np.inner( vec_s, vec_g)\n","  sim=np.round(sim,2)\n","  sim_s=sorted(enumerate(sim.reshape(-1)),key=lambda x:x[1], reverse=True)\n","  sim_s=np.round(sim_s,3)\n","  sim3g=np.array(sim_s[0:cnt])\n","  sim_mean=np.round(np.mean(sim3g[:,1]),2)\n","  sim_str=''\n","  ls=len(sen_s)\n","  lg=len(sen_g)\n","  for s in sim_s[0:cnt]:\n","    n=s[0]\n","    i_s=int(n/lg)\n","    i_g=int(n%lg)\n","    # print(i_s,i_g)\n","    str_s=sen_s[i_s]\n","    str_g=sen_g[i_g]\n","    str_t=str_s+'*'+str_g+':'+str(s[1])\n","    sim_str=sim_str+'|'+str_t\n","  return sim_str,sim_mean\n","def similarity_cal(col,path,outputdir,keycnt=3):\n","  split_out=[]\n","  remark_docs,guidan_docs,json_remark,json_title=load_remark_guidance_dt(col,path)\n","  remark_sims_out=[]\n","  cnt_remark=len(remark_docs)\n","  i=0\n","  for r in remark_docs:\n","    i=i+1\n","    # rs= sentence_spt_short(r)\n","    rs=[r]\n","    print(i,cnt_remark,rs)\n","\n","    sim_rg_list=[]\n","    glen=len(guidan_docs)\n","    for doc_id, g in zip(range(glen),guidan_docs):\n","      gs=sentence_spt_short(g)\n","      sim_str,sim_mean=get_sim_s3g(rs,gs,cnt=keycnt)\n","      sim_rg_list.append([doc_id,sim_str,sim_mean])\n","    sim_rg_list_sort=sorted(sim_rg_list,key=lambda x:x[2], reverse=True)\n","    for sim_rg in sim_rg_list_sort[0:3]:\n","      doc_id=sim_rg[0]\n","      sim_str=sim_rg[1]\n","      sim_mean=sim_rg[2]\n","      gs_w=json_title[doc_id]['word']\n","      gs_s=json_title[doc_id][col]\n","      # gs_stcd=json_title[doc_id]['tcd']\n","      remark_sims_out.append([r,gs_w,gs_s,sim_str,sim_mean])\n","  df_out=pd.DataFrame(remark_sims_out,columns=['sentence','word',col,'short_sens','similarity'])\n","  fn_out=str(keycnt)+\"w_bert\"+col+\"_short_sentense_similarity.csv\"\n","  df_out.to_csv(outputdir+fn_out,index=None)\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"whE5Jkn_ZgrY","colab_type":"text"},"source":["## 00_src2 use ml3"]},{"cell_type":"code","metadata":{"id":"MJPO0wN1ZjO9","colab_type":"code","colab":{}},"source":["import re\n","import pandas as pd\n","import json\n","from sklearn.metrics.pairwise import cosine_similarity\n","from datetime import datetime as dt\n","def short_concat(res,cnt=15):\n","  rc=[]\n","  sec=''\n","  lcnt=len(res)\n","  for s in res:\n","    sec=sec+s\n","    if len(sec)>=cnt:\n","      rc.append(sec)\n","      sec=''\n","  #last \n","  if len(sec)<cnt and len(sec)>0:\n","     rc.append(sec)\n","  return rc\n","def sentence_spt_short(sent):\n","  sps=['…','･･･','※']\n","  for s in sps:\n","    sent=sent.replace(s,',')\n","  # clean\n","  sps=['-']\n","  for s in sps:\n","    sent=sent.replace(s,'')\n","  result = re.sub(r'[・　。･]+', ',', sent)\n","  result = result.replace(\",,\",\",\")\n","  result = result.replace(\",,\",\",\")\n","  result = result.replace(\",,\",\",\")\n","  resplit=result.split(',')\n","  res=short_concat(resplit)\n","  return res\n","def load_remark_guidance_dt(col,pathout):\n","  json_remark = json.load(open(pathout+\"sentence.json\", \"r\"))\n","  json_title = json.load(open(pathout+\"word_\"+col+\".json\", \"r\"))\n","\n","  remark_docs=[]\n","  guidan_docs=[]\n","  json_remark_n=[]\n","  cnt_r=len(json_remark)\n","  for i,remark in zip(range(cnt_r),json_remark):\n","    remark_docs.append(remark['sentence'])\n","    json_remark_n.append(remark)\n","    if i ==79:\n","      break\n","  cnt_t=len(json_title)\n","  for i, title in  zip(range(cnt_t),json_title):\n","    if title['word'] not in title[col]:\n","      guidance=title['word']+\"。\"+title[col]\n","    else:\n","      guidance=title[col]\n","    guidan_docs.append(guidance)\n","  return remark_docs,guidan_docs,json_remark_n,json_title\n","def similarity_cal(col,path,outputdir,keycnt=3):\n","  remark_sims_out=[]\n","  remark_docs,guidan_docs,json_remark,json_title=load_remark_guidance_dt(col,path)\n","  grlst=[]\n","  remark_veclst=[]\n","  guidan_veclst=[]\n","  guidan_lst=[]\n","  guidan_col_lst=[]\n","  guidan_docs_lst=[]\n","  guidan_context_list=[]\n","  remark_veclst=embed_ml3(remark_docs)\n","  # for i,r in enumerate(remark_docs):\n","  #   remark_veclst.append(embed_ml3(r)[0])\n","  \n","  for i,g in enumerate(guidan_docs):\n","    gslst=sentence_spt_short(g)\n","    w=json_title[i]['word']\n","    for gs in gslst:\n","      guidan_lst.append([gs,i,g,w])\n","      guidan_docs_lst.append(gs)\n","      guidan_context_list.append(''.join(map(str,list(reversed(gs)))))\n","  print('vec create',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","\n","  # for i,v in enumerate(guidan_lst):\n","  #   guidan_veclst.append(v[0])\n","  \n","  guidan_veclst=embed_ml3(guidan_docs_lst)\n","  sim_rg_lst=cosine_similarity(remark_veclst,guidan_veclst)\n","  # sim_rg_lst=embed_qa_sim(remark_docs,guidan_docs_lst,guidan_context_list)\n","  print('cosine_similarity',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","\n","  for i,r in enumerate(remark_docs):\n","    sim_rg_matrix_lst=[]\n","    for j, g in enumerate(guidan_lst):\n","      _,g_id,gui,w=g\n","      sim=sim_rg_lst[i,j]\n","      sim_rg_matrix_lst.append([g_id,r,w,gui,sim])\n","    print(i,r)\n","    sim_rg_matrix_lst_sort=sorted(sim_rg_matrix_lst,key=lambda x:x[4], reverse=True)\n","    df=pd.DataFrame(sim_rg_matrix_lst_sort,columns=['docid','sent','word','guidance','sim'])\n","    dg=df.groupby(['docid'])\n","    sim_rg_lst_single_sent=[]\n","    for dgi in dg:\n","      docid=dgi[0]\n","      mean_n=dgi[1][0:keycnt].agg({'sim':'mean'})\n","      dti=dgi[1].values.tolist()[0]\n","      sim_rg_lst_single_sent.append([dti[1],dti[2],dti[3],np.float(mean_n)])\n","    sim_rg_lst_single_sent_sort=sorted(sim_rg_lst_single_sent,key=lambda x:x[3], reverse=True)\n","    remark_sims_out.extend(sim_rg_lst_single_sent_sort[0:3])\n","  df_out=pd.DataFrame(remark_sims_out,columns=['sentence','word',col,'similarity'])\n","  fn_out=str(keycnt)+\"w_bert\"+col+\"_short_sentense_similarity.csv\"\n","  df_out.to_csv(outputdir+fn_out,index=None)\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FVbg7VwdXZS2","colab_type":"text"},"source":["##00_src use qa"]},{"cell_type":"code","metadata":{"id":"kMisk7jlXcCd","colab_type":"code","colab":{}},"source":["import re\n","import pandas as pd\n","import json\n","from sklearn.metrics.pairwise import cosine_similarity\n","from datetime import datetime as dt\n","def short_concat(res,cnt=10):\n","  rc=[]\n","  sec=''\n","  lcnt=len(res)\n","  for s in res:\n","    sec=sec+s\n","    if len(sec)>=cnt:\n","      rc.append(sec)\n","      sec=''\n","  #last \n","  if len(sec)<cnt and len(sec)>0:\n","     rc.append(sec)\n","  return rc\n","def sentence_spt_short(sent):\n","  sps=['…','･･･','※']\n","  for s in sps:\n","    sent=sent.replace(s,',')\n","  # clean\n","  sps=['-']\n","  for s in sps:\n","    sent=sent.replace(s,'')\n","  result = re.sub(r'[・　。･]+', ',', sent)\n","  result = result.replace(\",,\",\",\")\n","  result = result.replace(\",,\",\",\")\n","  result = result.replace(\",,\",\",\")\n","  resplit=result.split(',')\n","  res=short_concat(resplit)\n","  return res\n","def load_remark_guidance_dt(col,pathout):\n","  json_remark = json.load(open(pathout+\"sentence.json\", \"r\"))\n","  json_title = json.load(open(pathout+\"word_\"+col+\".json\", \"r\"))\n","\n","  remark_docs=[]\n","  guidan_docs=[]\n","  json_remark_n=[]\n","  cnt_r=len(json_remark)\n","  for i,remark in zip(range(cnt_r),json_remark):\n","    clean_sent=pre_process_sen(remark['sentence'])\n","    remark_docs.append(clean_sent)\n","    json_remark_n.append(remark)\n","    if i ==79:\n","      break\n","  cnt_t=len(json_title)\n","  for i, title in  zip(range(cnt_t),json_title):    \n","    clean_sent=pre_process_sen(title[col])\n","    # guidance=title[col]\n","    guidan_docs.append(clean_sent)\n","  return remark_docs,guidan_docs,json_remark_n,json_title\n","def similarity_cal(col,path,outputdir,keycnt=3,threhold=0.1):\n","  \n","  remark_sims_out=[]\n","  remark_docs,guidan_docs,json_remark,json_title=load_remark_guidance_dt(col,path)\n","  grlst=[]\n","  remark_veclst=[]\n","  guidan_veclst=[]\n","  guidan_lst=[]\n","  guidan_docs_lst=[]\n","  guidan_context_list=[]\n","  guidan_word_lst=[]\n","  guidan_context_word_list=[]\n","  for i,g in enumerate(json_title):\n","    guidan_word_lst.append(g['word'])\n","    guidan_context_word_list.append('')\n","  for i,g in enumerate(guidan_docs):\n","    gslst=sentence_spt_short(g)\n","    w=json_title[i]['word']\n","    for gs in gslst:\n","      guidan_lst.append([gs,i,g,w])\n","      guidan_docs_lst.append(gs)\n","      # guidan_context_list.append(''.join(map(str,list(reversed(gs)))))\n","      guidan_context_list.append('')\n","  print('vec create',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","\n","  # sim_rg_lst=cosine_similarity(remark_veclst,guidan_veclst)\n","  sim_rg_lst=embed_qa_sim(remark_docs,guidan_docs_lst,guidan_context_list)\n","  sim_rg_word_lst=embed_qa_sim(remark_docs,guidan_word_lst,guidan_context_word_list)\n","  print('embed_qa_sim',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","\n","  for i,r in enumerate(remark_docs):\n","    sim_rg_matrix_lst=[]\n","    for j, g in enumerate(guidan_lst):\n","      _,g_id,gui,w=g\n","      sim=sim_rg_lst[i,j]\n","      sim_rg_matrix_lst.append([g_id,r,w,gui,sim])\n","    print(i,r)\n","    sim_rg_matrix_lst_sort=sorted(sim_rg_matrix_lst,key=lambda x:x[4], reverse=True)\n","    df=pd.DataFrame(sim_rg_matrix_lst_sort,columns=['docid','sent','word','guidance','sim'])\n","    dg=df.groupby(['docid'])\n","    sim_rg_lst_single_sent=[]\n","    for dgi in dg:\n","      docid=dgi[0]\n","      mean_n=dgi[1][0:keycnt].agg({'sim':'mean'})\n","      dti=dgi[1].values.tolist()[0]\n","      sim_rg_lst_single_sent.append([dti[1],dti[2],dti[3],np.float(mean_n)])\n","    sim_rg_lst_single_sent_all=sorted(sim_rg_lst_single_sent,key=lambda x:x[0], reverse=False)\n","    sim_rg_lst_single_sent_wrd=sim_rg_word_lst[i]\n","    sim_rg_lst_single_sent_wrd_exp=np.expand_dims(sim_rg_lst_single_sent_wrd,axis=1)\n","    sim_rg_lst_single_sent_aw=np.concatenate([sim_rg_lst_single_sent_all,sim_rg_lst_single_sent_wrd_exp],axis=1)\n","    df2=pd.DataFrame(sim_rg_lst_single_sent_aw,columns=['sentence','word',col,'similarity','simword'])\n","    df3=df2[df2.simword.apply(pd.to_numeric)>threhold]\n","    sim_rg_lst_single_sent_sort=sorted(df3.values.tolist(),key=lambda x:x[3], reverse=True)\n","    remark_sims_out.extend(sim_rg_lst_single_sent_sort[0:3])\n","  df_out=pd.DataFrame(remark_sims_out,columns=['sentence','word',col,'similarity','sim_w'])\n","  fn_out=str(keycnt)+\"w_bert\"+col+\"_short_sentense_similarity.csv\"\n","  df_out.to_csv(outputdir+fn_out,index=None)\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QOYbcf2HSu4r","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598940861768,"user_tz":-540,"elapsed":1144,"user":{"displayName":"于勝龍","photoUrl":"","userId":"09684187088444140086"}},"outputId":"06bd071d-d2ec-447c-bac0-bc09433384d9"},"source":["print('vec create',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["vec create 20/09/01 06:14:20\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3CoUTMr4q03T","colab_type":"text"},"source":["##01_src sub sentence vector plus"]},{"cell_type":"code","metadata":{"id":"OHl9RhSOqtLD","colab_type":"code","colab":{}},"source":["import re\n","import pandas as pd\n","import json\n","from datetime import datetime as dt\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","def short_concat(res,cnt=5):\n","  rc=[]\n","  sec=''\n","  lcnt=len(res)\n","  for s in res:\n","    sec=sec+s\n","    if len(sec)>=cnt:\n","      rc.append(sec)\n","      sec=''\n","  #last \n","  if len(sec)<cnt and len(sec)>0:\n","     rc.append(sec)\n","  return rc\n","def sentence_spt_short(sent):\n","  sps=['…','･･･','※']\n","  for s in sps:\n","    sent=sent.replace(s,',')\n","  # clean\n","  sps=['-']\n","  for s in sps:\n","    sent=sent.replace(s,'')\n","  result = re.sub(r'[・　。、･]+', ',', sent)\n","  result = result.replace(\",,\",\",\")\n","  resplit=result.split(',')\n","  res=short_concat(resplit)\n","  return res\n","def load_remark_guidance_dt(col,pathout):\n","  json_remark = json.load(open(pathout+\"sentence.json\", \"r\"))\n","  json_title = json.load(open(pathout+\"word_\"+col+\".json\", \"r\"))\n","\n","  remark_docs=[]\n","  guidan_docs=[]\n","  json_remark_n=[]\n","  cnt_r=len(json_remark)\n","  for i,remark in zip(range(cnt_r),json_remark):\n","    remark_docs.append(remark['sentence'])\n","    json_remark_n.append(remark)\n","    if i ==79:\n","      break\n","  cnt_t=len(json_title)\n","  for i, title in  zip(range(cnt_t),json_title):\n","    if title['word'] not in title[col]:\n","      guidance=title['word']+\"。\"+title[col]\n","    else:\n","      guidance=title[col]\n","    guidan_docs.append(guidance)\n","  return remark_docs,guidan_docs,json_remark_n,json_title\n","# def get_sim_plus(sen_s,sen_g):\n","#   # vec_s_a=embed(sen_s)\n","#   # vec_g_a=embed(sen_g)\n","#   vec_s_a=model.encode(sen_s)\n","#   vec_g_a=model.encode(sen_g)\n","#   vec_s=np.sum(vec_s_a,axis=0)\n","#   vec_g=np.sum(vec_g_a,axis=0)\n","#   # sim=cosine_similarity( vec_s, vec_g)\n","#   sim=cosine_similarity([vec_s], [vec_g])[0][0]\n","#   sim=np.round(sim,3)\n","  \n","#   return sim\n","def similarity_cal(col,path,outputdir,filterList,keycnt=3,sim_flg=True,rt=1):\n","  split_out=[]\n","  remark_docs,guidan_docs,json_remark,json_title=load_remark_guidance_dt(col,path)\n","  cnt_remark=len(remark_docs)\n","  def get_short_sum_vec(sen):\n","    rs= sentence_spt_short(sen)\n","    vec_s_a=embed(rs)\n","    vec_s=np.sum(vec_s_a,axis=0)\n","    return vec_s\n","  re_vecs_list=[]\n","  gu_vecs_list=[]\n","  for r in remark_docs:\n","    r_vec=get_short_sum_vec(r)\n","    re_vecs_list.append(r_vec)\n","  for r in guidan_docs:\n","    r_vec=get_short_sum_vec(r)\n","    gu_vecs_list.append(r_vec)\n","\n","  print('guidance vec end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","  sim_rg_matrix_org=np.inner(re_vecs_list, gu_vecs_list)\n","  # sim_rg_matrix_org=cosine_similarity( re_vecs_list, gu_vecs_list)\n","  print('sim caculate end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","  sim_rg_matrix=np.squeeze(sim_rg_matrix_org)\n","  \n","  remark_sims_out=[]\n","  for i, r_doc in enumerate(remark_docs):\n","    sim_rg_list=[]\n","    for j, g_doc in enumerate(guidan_docs):\n","      sim=sim_rg_matrix[i,j]\n","      sim_rg_list.append([j,sim])\n","    sim_rg_list_sort=sorted(sim_rg_list,key=lambda x:x[1], reverse=True)\n","    for sim_rg in sim_rg_list_sort[0:3]:\n","      doc_id=sim_rg[0]\n","      sim_mean=sim_rg[1]\n","      gs_w=json_title[doc_id]['word']\n","      gs_s=json_title[doc_id][col]\n","      gs_stcd=json_title[doc_id]['tcd']\n","      remark_sims_out.append([r_doc,gs_w,gs_stcd,gs_s,sim_mean])\n","  df_out=pd.DataFrame(remark_sims_out,columns=['sentence','word','tcd',col,'similarity'])\n","  fn_out=col+\"_inner_use_vec_plus_short_sentense_similarity.csv\"\n","  df_out.to_csv(outputdir+fn_out,index=None)\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FwH_d0mgqy6Z","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"es4fVtrWJ49x","colab_type":"text"},"source":["## 標題、確認事項、詳細結合"]},{"cell_type":"code","metadata":{"id":"VH-xZf95KAUV","colab_type":"code","colab":{}},"source":["import json\n","import pandas as pd\n","\n","filterList_nva=[\n","['動詞','自立'],\n","['名詞','サ変接続'],\n","['名詞','ナイ形容詞語幹'],\n","['名詞','一般'],\n","['名詞','固有名詞'],\n","['名詞','形容動詞語幹'],\n","['形容詞','自立'],\n","]\n","\n","filterList_nv=[\n","['動詞','自立'],\n","['名詞','サ変接続'],\n","['名詞','ナイ形容詞語幹'],\n","['名詞','一般'],\n","['名詞','固有名詞'],\n","['名詞','形容動詞語幹']\n","]\n","def get_key_word_con(remark,filterList,cls_flg=False):\n","  lines=m.parse(remark).split('\\n')\n","  keywords=[]\n","  stopwords_list=get_stopwords()\n","  for line in lines:\n","    items = re.split('[\\t]',line)\n","    if len(items) > 1:\n","      subItems= items[1].split(',')\n","      for f in filterList:\n","        if subItems[0]==f[0] and subItems[1]==f[1]:\n","          item_ap=''\n","          if cls_flg and check_by_regex(items[0]):\n","            item_ap=clean_words(items[0],stopwords_list)\n","          else:\n","            item_ap=items[0]\n","          if len(item_ap)>0:\n","            keywords.append(items[0])\n","  res=[]\n","  # delete duplicate data\n","  [res.append(x) for x in keywords if x not in res]\n","  keywords_c=''.join(map(str, res))\n","  return keywords_c\n","def load_data_short_sen(col,path):\n","  json_d = json.load(open(path+\"word_\"+col+\".json\", \"r\"))\n","  dt_list=[]\n","  for dt in json_d:\n","    word=dt['word']\n","    detail=dt[col]\n","    res=sentence_spt_short(detail)\n","    detail_nva=''\n","    detail_nv=''\n","    for sen in res:\n","      str_nva=get_key_word_con(sen,filterList_nva,cls_flg=True)\n","      str_nv=get_key_word_con(sen,filterList_nv,cls_flg=True)\n","      detail_nva=detail_nva+str_nva+','\n","      detail_nv=detail_nv+str_nv+','\n","    dt_list.append([word,detail,detail_nva,detail_nv])\n","  df=pd.DataFrame(dt_list,columns=['word',col,col+'_nva',col+'_nv'])\n","  return df\n","\n","def load_data(col,path):\n","  json_d = json.load(open(path+\"word_\"+col+\".json\", \"r\"))\n","  dt_list=[]\n","  for dt in json_d:\n","    word=dt['word']\n","    detail=dt[col]\n","    dt_list.append([word,detail])\n","  df=pd.DataFrame(dt_list,columns=['word',col])\n","  return df\n","# def concat_tcd():\n","col_t='title'\n","path='/content/mount/My Drive/00_work/guidance/20200701/01_title/'\n","df_t = load_data_short_sen(col_t,path)\n","#02_confirm\n","col_c='confirmation'\n","path='/content/mount/My Drive/00_work/guidance/20200701/02_confirm/'\n","df_c = load_data_short_sen(col_c,path)\n","#03_detail\n","col_d='detail'\n","path='/content/mount/My Drive/00_work/guidance/20200701/03_detail/'\n","df_d = load_data_short_sen(col_d,path)\n","\n","#04_word\n","col_s='synonyms'\n","path='/content/mount/My Drive/00_work/guidance/20200701/04_word/'\n","df_s = load_data(col_s,path)\n","\n","df_tc=pd.merge(df_t,df_c,how='outer',on='word')\n","df_tcd_n=pd.merge(df_tc,df_d,how='outer',on='word')\n","df_tcd=pd.merge(df_tcd_n,df_s,how='outer',on='word')\n","df_tcd=df_tcd.fillna('')\n","\n","\n","nva='_nva'\n","df_tcd['tcd'+nva]=df_tcd[col_t+nva].str.cat(df_tcd[col_c+nva], sep=',').\\\n","        str.cat(df_tcd[col_d+nva], sep=',').str.cat(df_tcd[col_s], sep=',')\n","df_tcd['tcd'+nva]=df_tcd['tcd'+nva].str.strip(',')\n","\n","\n","nva='_nv'\n","df_tcd['tcd'+nva]=df_tcd[col_t+nva].str.cat(df_tcd[col_c+nva], sep=',').\\\n","        str.cat(df_tcd[col_d+nva], sep=',').str.cat(df_tcd[col_s], sep=',')\n","df_tcd['tcd'+nva]=df_tcd['tcd'+nva].str.strip(',')\n","\n","\n","# original column\n","nva=''\n","df_tcd['tcd'+nva]=df_tcd[col_t+nva].str.cat(df_tcd[col_c+nva], sep='。').\\\n","        str.cat(df_tcd[col_d+nva], sep='。').str.cat(df_tcd[col_s], sep='。')\n","df_tcd['tcd'+nva]=df_tcd['tcd'+nva].str.strip('。')\n","\n","\n","df_tcd.to_json('/content/mount/My Drive/00_work/guidance/20200701/07_tcd/word_tcd_nva.json',orient='records',force_ascii=False)\n","df_tcd.to_json('/content/mount/My Drive/00_work/guidance/20200701/07_tcd/word_tcd_nv.json',orient='records',force_ascii=False)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Bt03YoFS3Sd","colab_type":"text"},"source":["## permutations test"]},{"cell_type":"code","metadata":{"id":"JRxhHdZwkgGP","colab_type":"code","colab":{}},"source":["from itertools import combinations\n","from itertools import permutations\n","g=[1,2,3,4,5]\n","# print(list(combinations(g,3)))\n","cnt=len(g)\n","# for i in range(cnt):\n","#   a=list(permutations(g,i+1))\n","#   print(len(a))\n","# print(list(permutations(g,2)))\n","dt_lst=['abc','123','mmm']\n","# df_tcd.to_json('/content/mount/My Drive/00_work/guidance/20200701/07_tcd/word_tcd_nva.json',orient='records',force_ascii=False)\n","\n","def get_perm_lst(dt_lst):\n","  perm_lst=[]\n","  cnt =len(dt_lst)\n","  for i  in range(cnt):\n","    pe_lst=permutations(dt_lst,i+1)\n","    for dt in pe_lst:\n","      perm=''.join(map(str, dt))\n","      perm_lst.append(perm)\n","  return perm_lst\n","k=get_perm_lst(dt_lst)\n","print(k)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xhvmmwfAbb4Z","colab_type":"code","colab":{}},"source":["import re\n","import random\n","def short_concat(res,cnt=5):\n","  rc=[]\n","  sec=''\n","  lcnt=len(res)\n","  for s in res:\n","    sec=sec+s\n","    if len(sec)>=cnt:\n","      rc.append(sec)\n","      sec=''\n","  #last \n","  if len(sec)<cnt and len(sec)>0:\n","     rc.append(sec)\n","  return rc\n","def sentence_spt_short(sent):\n","  sps=['…','･･･','※']\n","  for s in sps:\n","    sent=sent.replace(s,',')\n","  # clean\n","  sps=['-']\n","  for s in sps:\n","    sent=sent.replace(s,'')\n","  # result = re.sub(r'[・　。、･]+', ',', sent)\n","  result = re.sub(r'[・　･]+', '', sent)\n","  result = re.sub(r'[。、]+', ',', result)\n","  result = result.replace(\",,\",\",\")\n","  resplit=result.split(',')\n","  res=short_concat(resplit)\n","  return res\n","def get_perm_lst(dt_lst):\n","  perm_lst=[]\n","  cnt =len(dt_lst)\n","  pe_lst=permutations(dt_lst,cnt)\n","  for dt in pe_lst:\n","    perm='/'.join(map(str, dt))\n","    perm_lst.append(perm)\n","  return perm_lst\n","\n","def get_perm_lst2(dt_lst):\n","  perm_lst=[]\n","  cnt =len(dt_lst)\n","  if cnt >1:\n","    cnt=cnt*(cnt-1)\n","  # pe_lst=permutations(dt_lst,cnt)\n","  for i in range(cnt):\n","    random.shuffle(dt_lst)\n","    perm='/'.join(map(str, dt_lst))\n","    perm_lst.append(perm)\n","  return perm_lst\n","# k=get_perm_lst(dt_lst)\n","# print(k)\n","import tensorflow_hub as hub\n","import tensorflow_text\n","embed_3 = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WMCQBqd8gEdB","colab_type":"code","colab":{}},"source":["from itertools import permutations\n","from sklearn.metrics.pairwise import cosine_similarity\n","def list_number(cnt,cntMax=4):\n","  cntLst=np.zeros(cntMax)\n","  idt=0\n","  for i in range(cnt):\n","    if i%cntMax==0:\n","       idt=0\n","    cntLst[idt]= cntLst[idt]+1\n","    idt=idt+1\n","  return cntLst\n","def pre_process_per_sen(dtLst,cntMax=4):\n","    cntdt=len(dtLst)\n","    if cntdt<=cntMax:\n","      return dtLst\n","    dtListOut=[]\n","    nLst=list_number(cntdt,cntMax)\n","    s_pos=0\n","    for i in nLst:\n","      e_pos=int(i)+s_pos\n","      sub_dt_list=dtLst[s_pos:e_pos]\n","      sub_dt='、'.join(map(str, sub_dt_list))\n","      dtListOut.append(sub_dt)\n","      s_pos=e_pos\n","    return dtListOut\n","\n","def get_sen_perm_list(str_sen):\n","  dtLst=sentence_spt_short(str_sen)\n","  dtLst_pre=pre_process_per_sen(dtLst,cntMax=4)\n","  perm_list=get_perm_lst(dtLst_pre)\n","  return perm_list\n","\n","str_sen1='眼球の出血が引かず、一週間眼帯をして中学校へ通った期間があります'\n","str_sen2='血出血'\n","def cosine_similarity_perm(str_sen1,str_sen2,n):\n","  sen_lst1=get_sen_perm_list(str_sen1)\n","  sen_lst2=get_sen_perm_list(str_sen2)\n","  v_lst1=embed_3(sen_lst1)\n","  v_lst2=embed_3(sen_lst2)\n","  sim_rg_matrix_org=cosine_similarity( v_lst1, v_lst2)\n","  return np.max(sim_rg_matrix_org)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6HoBW1rvVZL-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"034d51f1-21ee-4984-a85c-2a6e429ad300"},"source":["list_number(10)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([3., 3., 2., 2.])"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"code","metadata":{"id":"DZKknRVbi_Ch","colab_type":"code","colab":{}},"source":["import numpy as np\n","str_sen1='この間なんか頭を窓にぶつけられ、腕につめをたてられて、血がでました'\n","str_sen2='内出血・腹部や太腿内側などの柔らかい組織にある傷は虐待が疑われる。・首に内出血がある場合は首を絞められた可能性を疑う。線上の出血などはその可能性が高い。-'\n","# cosine_similarity_perm(str_sen1,str_sen2,5)\n","sen_lst1=get_sen_perm_list(str_sen1)\n","sen_lst2=get_sen_perm_list(str_sen2)\n","\n","v_lst1=embed(sen_lst1)\n","v_lst2=embed(sen_lst2)\n","sim_rg_matrix_org=cosine_similarity( v_lst1, v_lst2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w2jvSrnPFLXb","colab_type":"code","colab":{}},"source":["sen_lst2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MiwIEbXnhPFF","colab_type":"code","colab":{}},"source":["sen_lst1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-u2_FcTShTGz","colab_type":"code","colab":{}},"source":["sen_lst2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3HY2kHo7hKiv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"aae15648-482f-469a-e5ff-61fabfa445da"},"source":["sen_lst1[3].replace(',','/')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'腕につめをたてられて/血がでました/この間なんか頭を窓にぶつけられ'"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"2USIqX0ogtLn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"074b8193-5e7e-49da-f4fc-e370db0ba7e3"},"source":["vec_s=embed(sen_lst1[1].replace(',',''))\n","vec_g=embed(sen_lst2[3].replace(',',''))\n","  # use_vect=np.squeeze(use_vec)\n","cosine_similarity( vec_s, vec_g)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.4094473]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"Mgf4ostN7fKw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"0d51fefa-8cd5-4156-f87a-dfb550dcc5fc"},"source":["get_sen_perm_list(str_sen1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['眼球の出血が引かず,一週間眼帯をして中学校へ通った期間があります', '一週間眼帯をして中学校へ通った期間があります,眼球の出血が引かず']"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"A8vmbxn843d8","colab_type":"code","colab":{}},"source":["import random\n","dtLst=sentence_spt_short(str_sen1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vmjjsd1WjS15","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"c762a898-18b7-4397-842b-d9050b97a10c"},"source":["from itertools import permutations\n","strList=['私に馬乗りになり','両手を捕まれ叩かれてたときに','やっぱり鼻血が出て','いつもは横に流れるんですけど','そのとき抵抗をしてたので口元に流れてき','母の手を一瞬ほどいて鼻血をトレーナーで拭いてしまったら','そのとき赤とグレーの色のトレーナーで']\n","# cnt=0\n","# for pi in permutations(strList):\n","#   cnt=cnt+1\n","print(len(strList))\n","pre_process_per_sen(strList,cntMax=3)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["7\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['私に馬乗りになり、両手を捕まれ叩かれてたときに、やっぱり鼻血が出て',\n"," 'いつもは横に流れるんですけど、そのとき抵抗をしてたので口元に流れてき',\n"," '母の手を一瞬ほどいて鼻血をトレーナーで拭いてしまったら、そのとき赤とグレーの色のトレーナーで']"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"markdown","metadata":{"id":"lnz7xnn-xQ8G","colab_type":"text"},"source":["## run"]},{"cell_type":"code","metadata":{"id":"mLJXGAnw5fxD","colab_type":"code","colab":{}},"source":["import re\n","import warnings\n","warnings.simplefilter('ignore')\n","keycnt=3\n","sim_flg=True\n","rt=0.80\n","filterList_nv=[\n","['動詞','自立'],\n","['名詞','サ変接続'],\n","['名詞','ナイ形容詞語幹'],\n","['名詞','一般'],\n","['名詞','固有名詞'],\n","['名詞','形容動詞語幹'],\n","# ['形容詞','自立'],\n","]\n","\n","filterList_nva=[\n","['動詞','自立'],\n","['名詞','サ変接続'],\n","['名詞','ナイ形容詞語幹'],\n","['名詞','一般'],\n","['名詞','固有名詞'],\n","['名詞','形容動詞語幹'],\n","['形容詞','自立'],\n","]\n","outputdir='/content/mount/My\\ Drive/00_work/guidance/output/short_use_08/'\n","%mkdir {outputdir}\n","outputdir='/content/mount/My Drive/00_work/guidance/output/short_use_08/'\n","\n","# 07_tcd\n","col='tcd'\n","path='/content/mount/My Drive/00_work/guidance/20200701/07_tcd/'\n","\n","# def similarity_cal(col,path,outputdir,keycnt=3):\n","similarity_cal(col,path,outputdir,keycnt=1)\n","# similarity_cal(col,path,outputdir,filterList_nva,keycnt=2,sim_flg=sim_flg,rt=rt)\n","# similarity_cal(col,path,outputdir,filterList_nva,keycnt=10,sim_flg=sim_flg,rt=rt)\n","# col='tcd_nva'\n","# path='/content/mount/My Drive/00_work/guidance/20200701/07_tcd/'\n","# similarity_cal(col,path,outputdir,filterList_nva,keycnt=keycnt,sim_flg=sim_flg,rt=rt)\n","# col='tcd_nv'\n","# path='/content/mount/My Drive/00_work/guidance/20200701/07_tcd/'\n","# similarity_cal(col,path,outputdir,filterList_nv,keycnt=keycnt,sim_flg=sim_flg,rt=rt)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IE9V4NpXUGFa","colab_type":"text"},"source":["## run2 ml3 qa"]},{"cell_type":"code","metadata":{"id":"hKWeac_fUIdo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1598952610152,"user_tz":-540,"elapsed":59262,"user":{"displayName":"于勝龍","photoUrl":"","userId":"09684187088444140086"}},"outputId":"45ae6bca-dba8-4619-a176-ee19ab23a3be"},"source":["# #05_all\n","col='all'\n","path='/content/mount/My Drive/00_work/guidance/20200701/05_all/'\n","outputdir='/content/mount/My Drive/00_work/guidance/output/00_USE_TEST/'\n","similarity_cal(col,path,outputdir,keycnt=2,threhold=0.1)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["vec create 20/09/01 09:29:11\n","embed_qa_sim 20/09/01 09:29:13\n","0 顔が腫れ上がり、額にケロイドが出来たのに…\n","1 この間なんか頭を窓にぶつけられ、腕につめをたてられて、血がでました\n","2 それが青あざになって血が出てるの知ってる?\n","3 昨日も顔を殴られて眉毛のしたが赤くなり唇が切れました\n","4 叩かれすぎて頬も腫れ、鼻血も垂らしました\n","5 手首に水ぶくれができて体中が燃えるように熱いんです!!\n","6 リスカもいっぱいしてます\n","7 最近は痣が一ヶ月以上残るほど叩いてきます・・・\n","8 眼球の出血が引かず、一週間眼帯をして中学校へ通った期間があります\n","9 四度ほど、首を絞められました\n","10 噛んだ跡がミミズバレのように腫れて、その後内出血をしたような痣になるんです\n","11 私が首に切り傷を入れた時\n","12 発端は母が第三者に身体を許してたことからはじまり小学校を入学して直ぐから去年の夏頃から二カ月の間まで長い間当たり前の様に第三者から身体を弄られ性行為をしないと言葉による暴言や去年の夏の終わりから首の付け根を抑えこまれ息が出来なくなり死ぬかもしれない暴力を受けました\n","13 鼻血が出るまで殴られ続け妹は殴られて鼓膜が破れ、母は骨折しました\n","14 またいつものように急に殴りにきて、私に馬乗りになり、両手を捕まれ叩かれてたときに、やっぱり鼻血が出て、いつもは横に流れるんですけど、そのとき抵抗をしてたので口元に流れてき、母の手を一瞬ほどいて鼻血をトレーナーで拭いてしまったら、そのとき赤とグレーの色のトレーナーで、拭いたところがなぜか運悪くグレーのとこで、まぁ服を汚したらど突かれるんですけど、なぜグレーに汚す!とよけいキレられ、殴られ続けてぐったり\n","15 頭も、割れたかと思うくらい痛くて、殺されると思いました\n","16 それは腕を切るや血が出ることではなく、髪の毛を抜くことでした\n","17 そしたら急に父の顔色が変わり、私の頭にオムライスをかけ、まだ熱をもっているフライパンで私を叩いたのです\n","18 髪を引っ張っては殴ったりお腹を思いっきり蹴られます\n","19 ポットの熱湯を腕に流されたり、包丁で頭を切られたり･･･\n","20 ネグレクトを受けて育ちました\n","21 子どもを連れて実家に帰っても、パートナーとパチンコへ行ってしまったり\n","22 母親が子供の頃、私の祖母が病気のため、長時間留守番をさせ、ご飯を作らなかったなどと話しています\n","23 そんな母親は、生後数ヶ月の弟を産むと、私達を残しあっさり浮気相手の所に行ってしまいました\n","24 女の子に産んでおきながら、女の子らしい服や物を与えない\n","25 もちろん私服など無い\n","26 暴力やネグレクトはなかったものの、支配的な接し方をされたり、理不尽に当たられたり、無視されたり、学業でしか評価してもらえなかったりといったような子供時代でした\n","27 また、ネグレクトで常にカビの生えた生乾きの下着や服を着ており、お風呂は週一、ゴミ屋敷の中でコンビニ弁当を食べたりと、精神的、身体的に毎日ボロボロでした\n","28 父が居ない夕飯は、兄だけ呼ばれ二人で食べる\n","29 もちろん食事なんてくれません\n","30 竹刀で叩かれたり、ネグレクトもありました\n","31 食事も父のだけを一生懸命作る人でした\n","32 お金を私達に渡すとパチンコ三昧の日々\n","33 無関心無視するなら産んでほしくなかった\n","34 食事はまともに食べさせてもらえず、痩せすぎて、近所ではあの家の子供は上の子の方は太ってるのに下の子は痩せすぎてるし、しょっちゅう叫び声がする(暴力されるから)虐待だと言っているそうだとクラスメイトに教えてもらった\n","35 そしたらもう今現在、無視or怒鳴るのどちらかでしか私には接してくれなくなりました\n","36 毎晩起こしたけいれんを放置したのは、一人で生きて行けるようになる教育だったそうです\n","37 そんな環境か、両親は不安定でわたしは育児放棄も受けて育ったようでした\n","38 食事も別、母や兄弟に話しかけても無視されあまりにも辛く家に居場所がなく小学生で自殺を企てた事もあります\n","39 我が家は貧乏ではなかったのに学校で使う裁縫道具(2000円ぐらい)を買ってもらえませんでした\n","40 小学高学年になってから、父親は、私がお風呂に入っているのを覗きにきて、性器を見せるようにいうようになりました\n","41 パパがを陰部を出し娘に押しつけているのか・・・・陰部を出しているのは確かです\n","42 また、父は姉に性的いたずらをしていました\n","43 裸にさせられて、胸を鷲掴みされました\n","44 小学生のとき夜いきなり布団に潜ってきてむねを触られたり下着の上から下半身を触られたりしました\n","45 父親には小さい頃、セクハラされてた\n","46 よく風呂場で縛られてレイプされてた\n","47 そしたら義父が…ゆりちゃんのおっぱい結構でかいねって言ってきて…無視してたら触ってきました\n","48 祖母はよく私の部屋にノックなしで入ってきて、私が薬を塗るため服を脱いでいた事があったのですが、「あの子おっぱい丸出しでいたよ」と父と祖父もいるリビングでわざわざ言いました\n","49 娘に対して性的な目を向けているのが残念でなりません\n","50 中学3年からは兄が性器をあててきて、気持ちわるくてたまりませんでした\n","51 裸になった父親が抱きついてきて勃起しているものを私に押し付けてきたり、トイレに入るとどんどんドアを叩き、トイレを使わせないなど、思春期の私はすごく辛く怖い思いをしました\n","52 3歳頃、教祖の性暴力から泣いて逃げたら、両親に両手足を抑えられ、すでに老人であったら教祖から身体を弄ばれました\n","53 ドライブに連れて行かれてはキスされ、ﾌｪﾗもしてと言われました\n","54 父の唾液がまだ性器の周りに残っているような気さえするときもあります\n","55 父が裸を見てくる事を17才頃、母と祖母に相談したら驚かれました\n","56 その後に、向けたちんちんにお湯を掛けろ、大きくしろと言われ、\n","57 無理やりキスをされたり、性器を舐められたりされました\n","58 スカートの脚をじろじろ見てくる、両親のセックスの場面を見せようとする、風呂から上がるタイミングで脱衣所の洗濯機置き場に度々いる\n","59 私は中学1年生まで実父に性的虐待をうけていました\n","60 その後、育児に全く無関心だった父のもとで育てられました\n","61 この病は育児への理想と現実のギャップから母親がノイローゼになってしまったり、育児による辛さや不安をまわりに吐き出せないままストレスを抱えてしまうことからなると考えられています\n","62 その家は父子家庭であるそうです\n","63 弟夫婦は共働きで忙しく、平日はほどんど母親が面倒を見ています\n","64 父は仕事柄週のうち半分は仕事で、その間家には知らない男の人が入れ替わり立ち代わり、時には泊まって行くこともありました\n","65 そんな父は現在単身赴任中で県外にいるため、相談することが難しいです\n","66 でも、うちのしつけが他の家庭からしたら、少しおかしいと知ったのは親から離れてからです\n","67 戸籍すら作られていないこどもが現実にいます\n","68 親戚は、子供の顔が見たいと、直ぐに飛んで来る様に、週1日程度で、来てくれますが、母親は忙しい!遠い!となかなか来ません\n","69 車で2分の距離も滅多に帰らなくなりました\n","70 児相の介入が必要としない場合でも助けを求めている人がいることを知ってほしいです\n","71 放任主義で育てられた人が、親になって、その子供が\n","72 毎日、一人っ子のその家の子の相手として夜はその子が寝付くまで世話をしていました\n","73 (おもに父)母は転職癖がある父のかわりにフルで外で働いていたため私の養育はおもに父と祖母がすることになりました\n","74 わたしが母を怒らせると、｢産まなきゃよかった｣｢死ねばいいのに｣と言われました\n","75 暴力やネグレクトはなかったものの、支配的な接し方をされたり、理不尽に当たられたり、無視されたり、学業でしか評価してもらえなかったりといったような子供時代でした\n","76 私の家は母子家庭です\n","77 母を亡くし父子二人だけの生活\n","78 弟は小学生のころにアスペルガー症候群と発覚し、他人とは違う行動をするため、学校から電話がかかってき、それを聞いた母は毎日怒鳴っています\n","79 子供の帰る場所は家しかない…なのにその家が暴力に溢れていたら、私達子供はどうなってしまうのでしょうか\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-9RRT8u_zBW9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1598952000423,"user_tz":-540,"elapsed":60453,"user":{"displayName":"于勝龍","photoUrl":"","userId":"09684187088444140086"}},"outputId":"f96790af-8af5-43a9-b80d-07b3f04826fb"},"source":["# f similarity_cal(col,path,outputdir,keycnt=3,threhold=0.1):\n","threhold=0.1\n","remark_sims_out=[]\n","remark_docs,guidan_docs,json_remark,json_title=load_remark_guidance_dt(col,path)\n","grlst=[]\n","remark_veclst=[]\n","guidan_veclst=[]\n","guidan_lst=[]\n","guidan_docs_lst=[]\n","guidan_context_list=[]\n","guidan_word_lst=[]\n","guidan_context_word_list=[]\n","for i,g in enumerate(json_title):\n","  guidan_word_lst.append(g['word'])\n","  guidan_context_word_list.append('')\n","for i,g in enumerate(guidan_docs):\n","  gslst=sentence_spt_short(g)\n","  w=json_title[i]['word']\n","  for gs in gslst:\n","    guidan_lst.append([gs,i,g,w])\n","    guidan_docs_lst.append(gs)\n","    # guidan_context_list.append(''.join(map(str,list(reversed(gs)))))\n","    guidan_context_list.append('')\n","print('vec create',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","\n","# sim_rg_lst=cosine_similarity(remark_veclst,guidan_veclst)\n","sim_rg_lst=embed_qa_sim(remark_docs,guidan_docs_lst,guidan_context_list)\n","sim_rg_word_lst=embed_qa_sim(remark_docs,guidan_word_lst,guidan_context_word_list)\n","print('embed_qa_sim',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","\n","for i,r in enumerate(remark_docs):\n","  sim_rg_matrix_lst=[]\n","  for j, g in enumerate(guidan_lst):\n","    _,g_id,gui,w=g\n","    sim=sim_rg_lst[i,j]\n","    sim_rg_matrix_lst.append([g_id,r,w,gui,sim])\n","  print(i,r)\n","  sim_rg_matrix_lst_sort=sorted(sim_rg_matrix_lst,key=lambda x:x[4], reverse=True)\n","  df=pd.DataFrame(sim_rg_matrix_lst_sort,columns=['docid','sent','word','guidance','sim'])\n","  dg=df.groupby(['docid'])\n","  sim_rg_lst_single_sent=[]\n","  for dgi in dg:\n","    docid=dgi[0]\n","    mean_n=dgi[1][0:keycnt].agg({'sim':'mean'})\n","    dti=dgi[1].values.tolist()[0]\n","    sim_rg_lst_single_sent.append([dti[1],dti[2],dti[3],np.float(mean_n)])\n","  sim_rg_lst_single_sent_all=sorted(sim_rg_lst_single_sent,key=lambda x:x[0], reverse=False)\n","  sim_rg_lst_single_sent_wrd=sim_rg_word_lst[i]\n","  sim_rg_lst_single_sent_wrd_exp=np.expand_dims(sim_rg_lst_single_sent_wrd,axis=1)\n","  sim_rg_lst_single_sent_aw=np.concatenate([sim_rg_lst_single_sent_all,sim_rg_lst_single_sent_wrd_exp],axis=1)\n","  df2=pd.DataFrame(sim_rg_lst_single_sent_aw,columns=['sentence','word',col,'similarity','simword'])\n","  df3=df2[df2.simword.apply(pd.to_numeric)>threhold]\n","  sim_rg_lst_single_sent_sort=sorted(df3.values.tolist(),key=lambda x:x[3], reverse=True)\n","  remark_sims_out.extend(sim_rg_lst_single_sent_sort[0:3])\n","df_out=pd.DataFrame(remark_sims_out,columns=['sentence','word',col,'similarity','sim_w'])\n","fn_out=str(keycnt)+\"w_bert\"+col+\"_short_sentense_similarity.csv\"\n","df_out.to_csv(outputdir+fn_out,index=None)\n","  "],"execution_count":null,"outputs":[{"output_type":"stream","text":["vec create 20/09/01 09:19:00\n","embed_qa_sim 20/09/01 09:19:03\n","0 顔が腫れ上がり、額にケロイドが出来たのに…\n","1 この間なんか頭を窓にぶつけられ、腕につめをたてられて、血がでました\n","2 それが青あざになって血が出てるの知ってる?\n","3 昨日も顔を殴られて眉毛のしたが赤くなり唇が切れました\n","4 叩かれすぎて頬も腫れ、鼻血も垂らしました\n","5 手首に水ぶくれができて体中が燃えるように熱いんです!!\n","6 リスカもいっぱいしてます\n","7 最近は痣が一ヶ月以上残るほど叩いてきます・・・\n","8 眼球の出血が引かず、一週間眼帯をして中学校へ通った期間があります\n","9 四度ほど、首を絞められました\n","10 噛んだ跡がミミズバレのように腫れて、その後内出血をしたような痣になるんです\n","11 私が首に切り傷を入れた時\n","12 発端は母が第三者に身体を許してたことからはじまり小学校を入学して直ぐから去年の夏頃から二カ月の間まで長い間当たり前の様に第三者から身体を弄られ性行為をしないと言葉による暴言や去年の夏の終わりから首の付け根を抑えこまれ息が出来なくなり死ぬかもしれない暴力を受けました\n","13 鼻血が出るまで殴られ続け妹は殴られて鼓膜が破れ、母は骨折しました\n","14 またいつものように急に殴りにきて、私に馬乗りになり、両手を捕まれ叩かれてたときに、やっぱり鼻血が出て、いつもは横に流れるんですけど、そのとき抵抗をしてたので口元に流れてき、母の手を一瞬ほどいて鼻血をトレーナーで拭いてしまったら、そのとき赤とグレーの色のトレーナーで、拭いたところがなぜか運悪くグレーのとこで、まぁ服を汚したらど突かれるんですけど、なぜグレーに汚す!とよけいキレられ、殴られ続けてぐったり\n","15 頭も、割れたかと思うくらい痛くて、殺されると思いました\n","16 それは腕を切るや血が出ることではなく、髪の毛を抜くことでした\n","17 そしたら急に父の顔色が変わり、私の頭にオムライスをかけ、まだ熱をもっているフライパンで私を叩いたのです\n","18 髪を引っ張っては殴ったりお腹を思いっきり蹴られます\n","19 ポットの熱湯を腕に流されたり、包丁で頭を切られたり･･･\n","20 ネグレクトを受けて育ちました\n","21 子どもを連れて実家に帰っても、パートナーとパチンコへ行ってしまったり\n","22 母親が子供の頃、私の祖母が病気のため、長時間留守番をさせ、ご飯を作らなかったなどと話しています\n","23 そんな母親は、生後数ヶ月の弟を産むと、私達を残しあっさり浮気相手の所に行ってしまいました\n","24 女の子に産んでおきながら、女の子らしい服や物を与えない\n","25 もちろん私服など無い\n","26 暴力やネグレクトはなかったものの、支配的な接し方をされたり、理不尽に当たられたり、無視されたり、学業でしか評価してもらえなかったりといったような子供時代でした\n","27 また、ネグレクトで常にカビの生えた生乾きの下着や服を着ており、お風呂は週一、ゴミ屋敷の中でコンビニ弁当を食べたりと、精神的、身体的に毎日ボロボロでした\n","28 父が居ない夕飯は、兄だけ呼ばれ二人で食べる\n","29 もちろん食事なんてくれません\n","30 竹刀で叩かれたり、ネグレクトもありました\n","31 食事も父のだけを一生懸命作る人でした\n","32 お金を私達に渡すとパチンコ三昧の日々\n","33 無関心無視するなら産んでほしくなかった\n","34 食事はまともに食べさせてもらえず、痩せすぎて、近所ではあの家の子供は上の子の方は太ってるのに下の子は痩せすぎてるし、しょっちゅう叫び声がする(暴力されるから)虐待だと言っているそうだとクラスメイトに教えてもらった\n","35 そしたらもう今現在、無視or怒鳴るのどちらかでしか私には接してくれなくなりました\n","36 毎晩起こしたけいれんを放置したのは、一人で生きて行けるようになる教育だったそうです\n","37 そんな環境か、両親は不安定でわたしは育児放棄も受けて育ったようでした\n","38 食事も別、母や兄弟に話しかけても無視されあまりにも辛く家に居場所がなく小学生で自殺を企てた事もあります\n","39 我が家は貧乏ではなかったのに学校で使う裁縫道具(2000円ぐらい)を買ってもらえませんでした\n","40 小学高学年になってから、父親は、私がお風呂に入っているのを覗きにきて、性器を見せるようにいうようになりました\n","41 パパがを陰部を出し娘に押しつけているのか・・・・陰部を出しているのは確かです\n","42 また、父は姉に性的いたずらをしていました\n","43 裸にさせられて、胸を鷲掴みされました\n","44 小学生のとき夜いきなり布団に潜ってきてむねを触られたり下着の上から下半身を触られたりしました\n","45 父親には小さい頃、セクハラされてた\n","46 よく風呂場で縛られてレイプされてた\n","47 そしたら義父が…ゆりちゃんのおっぱい結構でかいねって言ってきて…無視してたら触ってきました\n","48 祖母はよく私の部屋にノックなしで入ってきて、私が薬を塗るため服を脱いでいた事があったのですが、「あの子おっぱい丸出しでいたよ」と父と祖父もいるリビングでわざわざ言いました\n","49 娘に対して性的な目を向けているのが残念でなりません\n","50 中学3年からは兄が性器をあててきて、気持ちわるくてたまりませんでした\n","51 裸になった父親が抱きついてきて勃起しているものを私に押し付けてきたり、トイレに入るとどんどんドアを叩き、トイレを使わせないなど、思春期の私はすごく辛く怖い思いをしました\n","52 3歳頃、教祖の性暴力から泣いて逃げたら、両親に両手足を抑えられ、すでに老人であったら教祖から身体を弄ばれました\n","53 ドライブに連れて行かれてはキスされ、ﾌｪﾗもしてと言われました\n","54 父の唾液がまだ性器の周りに残っているような気さえするときもあります\n","55 父が裸を見てくる事を17才頃、母と祖母に相談したら驚かれました\n","56 その後に、向けたちんちんにお湯を掛けろ、大きくしろと言われ、\n","57 無理やりキスをされたり、性器を舐められたりされました\n","58 スカートの脚をじろじろ見てくる、両親のセックスの場面を見せようとする、風呂から上がるタイミングで脱衣所の洗濯機置き場に度々いる\n","59 私は中学1年生まで実父に性的虐待をうけていました\n","60 その後、育児に全く無関心だった父のもとで育てられました\n","61 この病は育児への理想と現実のギャップから母親がノイローゼになってしまったり、育児による辛さや不安をまわりに吐き出せないままストレスを抱えてしまうことからなると考えられています\n","62 その家は父子家庭であるそうです\n","63 弟夫婦は共働きで忙しく、平日はほどんど母親が面倒を見ています\n","64 父は仕事柄週のうち半分は仕事で、その間家には知らない男の人が入れ替わり立ち代わり、時には泊まって行くこともありました\n","65 そんな父は現在単身赴任中で県外にいるため、相談することが難しいです\n","66 でも、うちのしつけが他の家庭からしたら、少しおかしいと知ったのは親から離れてからです\n","67 戸籍すら作られていないこどもが現実にいます\n","68 親戚は、子供の顔が見たいと、直ぐに飛んで来る様に、週1日程度で、来てくれますが、母親は忙しい!遠い!となかなか来ません\n","69 車で2分の距離も滅多に帰らなくなりました\n","70 児相の介入が必要としない場合でも助けを求めている人がいることを知ってほしいです\n","71 放任主義で育てられた人が、親になって、その子供が\n","72 毎日、一人っ子のその家の子の相手として夜はその子が寝付くまで世話をしていました\n","73 (おもに父)母は転職癖がある父のかわりにフルで外で働いていたため私の養育はおもに父と祖母がすることになりました\n","74 わたしが母を怒らせると、｢産まなきゃよかった｣｢死ねばいいのに｣と言われました\n","75 暴力やネグレクトはなかったものの、支配的な接し方をされたり、理不尽に当たられたり、無視されたり、学業でしか評価してもらえなかったりといったような子供時代でした\n","76 私の家は母子家庭です\n","77 母を亡くし父子二人だけの生活\n","78 弟は小学生のころにアスペルガー症候群と発覚し、他人とは違う行動をするため、学校から電話がかかってき、それを聞いた母は毎日怒鳴っています\n","79 子供の帰る場所は家しかない…なのにその家が暴力に溢れていたら、私達子供はどうなってしまうのでしょうか\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hHBB6KFW9V5o","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1598951912245,"user_tz":-540,"elapsed":848,"user":{"displayName":"于勝龍","photoUrl":"","userId":"09684187088444140086"}},"outputId":"80663f7e-d2c1-422c-a421-ff9a0a6dfcce"},"source":["sim_rg_lst_single_sent_sort[0:3]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['子供の帰る場所は家しかない…なのにその家が暴力に溢れていたら、私達子供はどうなってしまうのでしょうか',\n"," '面前DV',\n"," '面前DV子どもの精神状態父母どちらかが子どもの前で、配偶者に暴力をふるったり、暴言を吐いたりする行為。',\n"," '0.3813726603984833',\n"," '0.22087255']"]},"metadata":{"tags":[]},"execution_count":252}]},{"cell_type":"code","metadata":{"id":"0m1ru81jzSZP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1598951735765,"user_tz":-540,"elapsed":1139,"user":{"displayName":"于勝龍","photoUrl":"","userId":"09684187088444140086"}},"outputId":"2e2a3fae-1ca2-4ed9-bd65-cec068335ac2"},"source":["sim_rg_lst_single_sent_wrd_exp=np.expand_dims(sim_rg_lst_single_sent_wrd,axis=1)\n","threhold=0.1\n","sim_rg_lst_single_sent_aw=np.concatenate([sim_rg_lst_single_sent_all,sim_rg_lst_single_sent_wrd_exp],axis=1)\n","df2=pd.DataFrame(sim_rg_lst_single_sent_aw,columns=['sentence','word',col,'similarity','simword'])\n","df2=df2[df2.simword.apply(pd.to_numeric)>threhold]\n","sim_rg_lst_single_sent_sort=sorted(df2.values.tolist(),key=lambda x:x[3], reverse=True)\n","np.array(sim_rg_lst_single_sent_sort[0:3])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([['顔が腫れ上がり、額にケロイドが出来たのに…', '湿疹', '湿疹ネグレクトの可能性-被れ汗疹',\n","        '0.3180438280105591', '0.32830137'],\n","       ['顔が腫れ上がり、額にケロイドが出来たのに…', '水ぶくれ',\n","        '水ぶくれ受傷部位、受傷頻度※受傷理由入院加療が必要な外傷の有無-水疱', '0.22795969247817993',\n","        '0.3141306'],\n","       ['顔が腫れ上がり、額にケロイドが出来たのに…', 'リケッチア感染症',\n","        'リケッチア感染症ネズミから移る感染症。以下の病状が見られる。・発熱・頭痛・発疹・関節痛主にネズミ、ノミが媒介する発疹熱リケッチアによっておこる感染症で、発熱、頭痛、発疹、関節痛などの症状を認める。リケッチア',\n","        '0.2169712372124195', '0.30510673']], dtype='<U103')"]},"metadata":{"tags":[]},"execution_count":248}]},{"cell_type":"code","metadata":{"id":"8i4z7DVq6Dtp","colab_type":"code","colab":{}},"source":["sim_rg_lst_single_sent_wrd_exp"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sb29brM3dUlX","colab_type":"text"},"source":["\n","\n","## cosine_similarity test"]},{"cell_type":"code","metadata":{"id":"WwglcARmc7OC","colab_type":"code","colab":{}},"source":["from sklearn.metrics.pairwise import cosine_similarity\n","sen_g=['子ども時代に受けた虐待が精神的後遺症 （トラウマ）となって残り', '青年期 成人期になってからいろいろな問題を引き起こすことは少なくない', 'これまでの報告からは', 'うつ症状や自殺企図', 'アルコール', '薬物依存を有する男女では', '一般の人よりも虐待された経験を持つことが多いということが分かっている']\n","sen_s=['またいつものように急に殴りにきて', '私に馬乗りになり', '両手を捕まれ叩かれてたときに', 'やっぱり鼻血が出て', 'いつもは横に流れるんですけど', 'そのとき抵抗をしてたので口元に流れてき', '母の手を一瞬ほどいて鼻血をトレーナーで拭いてしまったら', 'そのとき赤とグレーの色のトレーナーで', '拭いたところがなぜか運悪くグレーのとこで', 'まぁ服を汚したらど突かれるんですけど', 'なぜグレーに汚す!とよけいキレられ', '殴られ続けてぐったり']\n","def get_sim_s2g(sen_s,sen_g,cnt=3):\n","  vec_s=embed(sen_s)\n","  vec_g=embed(sen_g)\n","  # use_vect=np.squeeze(use_vec)\n","  sim=cosine_similarity( vec_s, vec_g)\n","  sim=np.round(sim,2)\n","  sim_s=sorted(enumerate(sim.reshape(-1)),key=lambda x:x[1], reverse=True)\n","  sim_s=np.round(sim_s,3)\n","  sim3g=np.array(sim_s[0:3])\n","  sim_mean=np.round(np.mean(sim3g[:,1]),2)\n","  # print(sim,np.array(sim).shape)\n","  # print(np.array(sim)[0,1])\n","  # print(sim_mean)\n","  # print(sim3g)\n","  sim_str=''\n","  ls=len(sen_s)\n","  lg=len(sen_g)\n","  for s in sim_s[0:cnt]:\n","    n=s[0]\n","    i_s=int(n/lg)\n","    i_g=int(n%lg)\n","    # print(i_s,i_g)\n","    str_s=sen_s[i_s]\n","    str_g=sen_g[i_g]\n","    str_t=str_s+'*'+str_g+':'+str(s[1])\n","    sim_str=sim_str+'|'+str_t\n","  return sim_str,sim_mean\n","  \n","sim_str,sim_mean=get_sim_s2g(sen_s,sen_g,cnt=3)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Jtd8O9uDYMe","colab_type":"code","colab":{}},"source":["import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","sen_s=['またいつものように急に殴りにきて', '私に馬乗りになり', '両手を捕まれ叩かれてたときに', 'やっぱり鼻血が出て']\n","sen_g=['子ども時代に受けた虐待が精神的後遺症 （トラウマ）となって残り', '私に馬乗りになり']\n","\n","vec_s=embed(sen_s)\n","vec_g=embed(sen_g)\n","sim1=cosine_similarity( vec_s, vec_g)\n","sim2=np.inner(vec_s, vec_g)\n","# sim2 = 1 - np.arccos(cosine_similarity(vec_s,vec_g))/np.pi\n","print(sim1)\n","print(sim2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jNlkWaoTJ5Fa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"34628dd6-0146-42e3-dbfa-2986edf77a73"},"source":["sim1.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4, 2)"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"id":"JRgQ0XJ3Dolv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"142092bc-70ff-49bb-df0a-6275168a8ae6"},"source":["k=np.sum(vec_g,axis=0)\n","m=k.reshape(1,-1)\n","np.squeeze(m).shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(512,)"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"fyYBvUUSJD3K","colab_type":"code","colab":{}},"source":["embed('dd')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Y07TT5DJgkw","colab_type":"code","colab":{}},"source":["np.array(k)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C7QN5BlmEy8D","colab_type":"code","colab":{}},"source":["sim2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4mwJqIJf0TxV","colab_type":"text"},"source":["## test"]},{"cell_type":"code","metadata":{"id":"aEyl79le0TTh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"762fecf6-c643-4c15-d3ab-137885c6185b"},"source":["get_words('児童指導員子ども指導員')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['児童指導員', '子ども', '指導員']"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"sa4Qol6su_y5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"45177547-c129-4b17-bc52-81211674a3cf"},"source":["vec_s=[[1,1,2,2],[1,1,3,3]]\n","vec=np.sum(vec_s,axis=0)\n","print(vec)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[2 2 5 5]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"87cRzPqEyL1D","colab_type":"code","colab":{}},"source":["sim=cosine_similarity( vec_s, vec_g)\n","sim=np.round(sim,3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ToGlHK4mYTK","colab_type":"code","colab":{}},"source":["from sklearn.metrics.pairwise import cosine_similarity\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JELnZDdFxPMX","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"2YdgpJWvppBa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":90},"outputId":"cca805dd-d09b-4c87-8102-22ee38d04d44"},"source":["sps=['・','　','。','、']\n","for sp in sps:\n","  print(sp) \n","# print(\"aa dd,ff\".split([' ',',']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["・\n","　\n","。\n","、\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Li7hdP9jBP2Z","colab_type":"text"},"source":["# pytermextract"]},{"cell_type":"code","metadata":{"id":"MlQk5E7HBPHx","colab_type":"code","colab":{}},"source":["#https://qiita.com/EastResident/items/0cdc7c5ac1f0a6b3cf1d\n","#http://gensen.dl.itc.u-tokyo.ac.jp/soft/pytermextract-0_01.zip\n","!wget \"http://gensen.dl.itc.u-tokyo.ac.jp/soft/pytermextract-0_01.zip\"\n","!unzip pytermextract-0_01.zip  > /dev/null\n","%cd pytermextract-0_01/\n","!python setup.py install > /dev/null\n","%cd ..\n","!pip install janome > /dev/null\n","# JUMAN"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"izfIpnwZ4ddu","colab_type":"text"},"source":["##和文ストップワード方式のキーワード抽出"]},{"cell_type":"markdown","metadata":{"id":"Y1qV2RRU6KuO","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"R-DZs5BKBdDP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"236de9ef-420b-42b3-f29b-50444c6f782e"},"source":["import collections\n","import termextract.japanese_plaintext\n","import termextract.core\n","\n","#　抽出されたキーワードにて、一つセンテンスにて、3位までの平均値を取得して、\n","# ファイルを読み込む\n","tagged_text = '昨日も顔を殴られて眉毛のしたが赤くなり唇が切れました'\n","# text='食事も別、母や兄弟に話しかけても無視されあまりにも辛く家に居場所がなく小学生で自殺を企てた事もあります'\n","# 複合語を抽出し、重要度を算出\n","frequency = termextract.japanese_plaintext.cmp_noun_dict(tagged_text)\n","LR = termextract.core.score_lr(frequency,\n","         ignore_words=termextract.japanese_plaintext.IGNORE_WORDS,\n","         lr_mode=1, average_rate=1\n","     )\n","term_imp = termextract.core.term_importance(frequency, LR)\n","\n","# 重要度が高い順に並べ替えて出力\n","data_collection = collections.Counter(term_imp)\n","for cmp_noun, value in data_collection.most_common():\n","    print(termextract.core.modify_agglutinative_lang(cmp_noun), value, sep=\"\\t\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["昨日\t1.4142135623730951\n","眉毛\t1.4142135623730951\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HpsWPmdg4KcU","colab_type":"text"},"source":["## Mecabでの形態素解析結果"]},{"cell_type":"code","metadata":{"id":"VSpOqQqn4Oc3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"5910bf59-e6ed-44a6-ae94-8e309651656e"},"source":["import termextract.mecab\n","import termextract.core\n","import collections\n","# JUMAN \n","# ファイルを読み込む\n","# tagged_text = open(\"sample.txt\", \"r\", encoding=\"utf-8\").read()\n","lines=m.parse(tagged_text)#.split('\\n')\n","# 複合語を抽出し、重要度を算出\n","frequency = termextract.mecab.cmp_noun_dict(lines)\n","LR = termextract.core.score_lr(frequency,\n","         ignore_words=termextract.mecab.IGNORE_WORDS,\n","         lr_mode=1, average_rate=1\n","     )\n","term_imp = termextract.core.term_importance(frequency, LR)\n","\n","# 重要度が高い順に並べ替えて出力\n","data_collection = collections.Counter(term_imp)\n","for cmp_noun, value in data_collection.most_common():\n","    print(termextract.core.modify_agglutinative_lang(cmp_noun), value, sep=\"\\t\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["顔\t1.0\n","眉毛\t1.0\n","した\t1.0\n","唇\t1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rndQZZN79r_y","colab_type":"code","colab":{}},"source":["m.parse(tagged_text).split('\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KLtAvJ2e5Ez7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"f56f4a80-2e3e-4934-afc8-a51a3545b1be"},"source":["tagged_text"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'食事はまともに食べさせてもらえず、痩せすぎて、近所ではあの家の子供は上の子の方は太ってるのに下の子は痩せすぎてるし、しょっちゅう叫び声がする(暴力されるから)虐待だと言っているそうだとクラスメイトに教えてもらった'"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"uxAlY99w0VZv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"5299f54a-8ae5-4aa9-ad79-ad6ce584c3be"},"source":["import re\n","\n","delwords_list=['〇〇','あぁ!','い','いう','いか','いる','いろいろ','き','く']\n","sent='〇〇ｋｋｋあぁ!'\n","for x in delwords_list:\n","  sent=sent.replace(x,'')\n","print(sent)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["ｋｋｋ\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3zXeXV9CGzwO","colab_type":"text"},"source":["##yake"]},{"cell_type":"code","metadata":{"id":"2uWD3__0G4Ia","colab_type":"code","colab":{}},"source":["!pip install git+https://github.com/LIAAD/yake  > /dev/null"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kv5OUYheG_Bs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170},"outputId":"ff08c3e2-45d3-432d-a999-f4d5fd71b447"},"source":["#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\n","import MeCab\n","import yake\n","\n","# read data\n","# with open('test.txt', encoding='utf-8') as f:\n","#     read_data = f.read()\n","read_data='食事はまともに食べさせてもらえず、痩せすぎて、近所ではあの家の子供は上の子の方は太ってるのに下の子は痩せすぎてるし、しょっちゅう叫び声がする(暴力されるから)虐待だと言っているそうだとクラスメイトに教えてもらった'\n","# tokenize Japanese text\n","# mecab = MeCab.Tagger(\"-Owakati\")\n","tokenized_text = m.parse(read_data)\n","\n","# extract keywords\n","kw_extractor = yake.KeywordExtractor(lan=\"ja\", n=1)\n","keywords = kw_extractor.extract_keywords(tokenized_text)\n","\n","for kw in keywords:\n","    print(kw)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["('まとも', 0.08644417426530192)\n","('もらえ', 0.08644417426530192)\n","('しょっちゅう', 0.08644417426530192)\n","('そうだ', 0.08644417426530192)\n","('クラスメイト', 0.08644417426530192)\n","('もらっ', 0.08644417426530192)\n","('叫び声', 0.11991845995215332)\n","('eos', 0.14075745983667484)\n","('リョク', 0.15644756057763543)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xILGJA27HYPb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"faf0950f-216f-49d5-c335-cc4a896a6c91"},"source":["keywords\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"markdown","metadata":{"id":"7SyJWgy0vaSU","colab_type":"text"},"source":["## TfIdfWord2VecVectorizer"]},{"cell_type":"code","metadata":{"id":"9REeMQOfpeA_","colab_type":"code","colab":{}},"source":["import tensorflow.compat.v1 as tf\n","class  TfIdfWord2VecVectorizer:\n","    def __init__(self):\n","        pass\n","\n","    def fit(self, X, y):\n","        self.tfidf_model = TfidfVectorizer(min_df=1, max_df=100).fit(X)\n","        return self\n","\n","    def transform(self, X):\n","        return self.tfidf_and_w2v(X, self.tfidf_model)\n","\n","    def texts_encoder(self, texts):\n","        with tf.Graph().as_default():\n","            embed = hub.Module(\"https://tfhub.dev/google/nnlm-ja-dim128/1\")\n","            embeddings = embed(texts)\n","            with tf.Session() as sess:\n","                sess.run(tf.global_variables_initializer())\n","                sess.run(tf.tables_initializer())\n","                result = sess.run(embeddings)\n","        return result\n","\n","    def tfidf_and_w2v(self, X, tfidf_model):\n","        tmp = tfidf_model.transform(X)\n","        a = tmp.toarray()\n","        if isinstance(X, pd.core.series.Series):\n","            X = X.tolist()\n","        b = self.texts_encoder(X)\n","        return np.hstack((a, b))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mps5F4CNpf8H","colab_type":"code","colab":{}},"source":["aa=TfIdfWord2VecVectorizer()\n","docs_org = [\n","    '引き続きよろしくお願いいたします。',\n","    '食事はまともに食べさせてもらえず',\n","    '痩せすぎて',\n","    '近所ではあの家の子供は上の子の方は太ってるのに下の子は痩せすぎてるし',\n","    'しょっちゅう叫び声がする(暴力されるから)虐待だと言っているそうだとクラスメイトに教えてもらった',\n","]\n","docs=[]\n","for doc in docs_org:\n","   word_list =get_key_word(doc)# get_words(doc)\n","   doc_word=' '.join(map(str, word_list))\n","   docs.append(doc_word)\n","\n","#   tf-idfの計算文書全体の90%以上で出現する単語は無視する\n","# vectorizer = TfidfVectorizer(max_df=0.9)\n","# X = vectorizer.fit_transform(docs)\n","aa.fit(docs,\"tt\")\n","dd=aa.tfidf_and_w2v(docs,aa.tfidf_model)\n","# print(aa(\"重要度が高い順に並べ替えて出力\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_CabvLI4r6BA","colab_type":"code","colab":{}},"source":["!rm *.hdf5"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kewLed2iun83","colab_type":"text"},"source":["# sentence USE + tfidf+ w2v"]},{"cell_type":"code","metadata":{"id":"r1nwdNGvCO-c","colab_type":"code","colab":{}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","docs_org=['引き続きよろしくお願いいたします。','食事はまともに食べさせてもらえず',]\n","#mecab を使って、形態素解析し、単語へ\n","docs=[]\n","for doc in docs_org:\n","  word_list=get_words(doc)\n","  doc_word=' '.join(map(str,word_list))\n","  docs.append(doc_word)\n","vectorizer=TfidfVectorizer(max_df=0.9)\n","X=vectorizer.fit_transform(docs)\n","print('feature_names:',vectorizer.get_feature_names())\n","words=vectorizer.get_feature_names()\n","for doc_id, vec in zip(range(len(docs)), X.toarray()):\n","  print('doc_id:',doc_id)\n","  for w_id, tfidf in sorted(enumerate(vec),key=lambda x:x[1], reverse=True):\n","    lemma=words[w_id]\n","    if tfidf > 0:\n","      print('\\t{0:s}: {1:f}'.format(lemma, tfidf))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nYzzdrEiUKvx","colab_type":"code","colab":{}},"source":["import numpy as np\n","a=[1,2,3]\n","b=[2,3,5]\n","a=np.expand_dims(a,axis=1)\n","b=np.expand_dims(b,axis=1)\n","print(np.concatenate([a,b],axis=1).shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G40BbkZ8GS1Z","colab_type":"text"},"source":["## curl -Lo data/wiki.en.vec https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec"]},{"cell_type":"markdown","metadata":{"id":"-fvTrjxLjErf","colab_type":"text"},"source":["# bert sim\n","https://www.ogis-ri.co.jp/otc/hiroba/technical/similar-document-search/part9.html"]},{"cell_type":"markdown","metadata":{"id":"Uu_nj2msd_qU","colab_type":"text"},"source":["# mode training USE\n","https://gist.github.com/candlewill/552fa102352ccce42fd829ae26277d24"]},{"cell_type":"markdown","metadata":{"id":"O_buMDVfTKrt","colab_type":"text"},"source":["## create permuation data"]},{"cell_type":"code","metadata":{"id":"p5f90K1STKKQ","colab_type":"code","colab":{}},"source":["# df_tcd.to_json('/content/mount/My Drive/00_work/guidance/20200701/07_tcd/word_tcd.json',orient='records',force_ascii=False)\n","from itertools import permutations\n","import json\n","\n","def get_perm_lst(dt_lst):\n","  perm_lst=[]\n","  cnt =len(dt_lst)\n","  for i  in range(cnt):\n","    pe_lst=permutations(dt_lst,i+1)\n","    for dt in pe_lst:\n","      perm='、'.join(map(str, dt))\n","      perm_lst.append(perm)\n","  return perm_lst\n","fn_tcd='/content/mount/My Drive/00_work/guidance/20200701/07_tcd/word_tcd.json'\n","json_tcd = json.load(open(fn_tcd, \"r\"))\n","cols=[ 'title','confirmation','detail','synonyms']\n","dt_train_list=[]\n","wd='word'\n","for tcd in json_tcd:\n","  dt_col_lst=[]\n","  for col in cols:\n","    dt=tcd[col]\n","    if len(dt)>1:\n","      # print(len(dt),col,dt)\n","      dt_col_lst.append(dt)\n","  if len(dt_col_lst)>0:\n","    perm_list=get_perm_lst(dt_col_lst)\n","    for perm in perm_list:\n","      dt_train_list.append([tcd[wd],perm])\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","path='/content/mount/My Drive/00_work/guidance/train_dt.csv'\n","df=pd.read_csv(path,sep=',',header=0)\n","df.shape\n","df_t=pd.DataFrame(dt_train_list,columns=['word','guid_t'])\n","df_tc=pd.merge(df,df_t,how='left',left_on='title',right_on='word')\n","df_n=df_tc.dropna(how='any').loc[:,['remark','word','guid_t','label']]\n","dt_x=df_n.loc[:,['remark','guid_t']].values.tolist()\n","dt_y=df_n.loc[:,['label']].values.tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OxVZLd1_6VNW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"5226d3bb-036a-410e-e5ca-a7834c07dd1d"},"source":["len(dt_x)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["25681"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"eThJCeh5TGYz","colab_type":"text"},"source":["## create data"]},{"cell_type":"code","metadata":{"id":"vkuxYjXthZ8R","colab_type":"code","colab":{}},"source":["\n","dt_xv=[]\n","cnt=len(dt_x)\n","for i, x in enumerate(dt_x):\n","  vec=embed(x)\n","  if i%1000 ==0:\n","    print(i,cnt)\n","  dt=np.array(vec)\n","  dt_xv.append(dt.reshape(1,-1)[0])\n","  # dt_xv.append(np.squeeze(dt.reshape(1,1024)))\n","  # dt_xv.append(dt)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bmNnQHDyyafT","colab_type":"text"},"source":["## makedata old"]},{"cell_type":"code","metadata":{"id":"vcD1dFHyBOwt","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CfSljK-xLKRT","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"adl1MMHnNcVE","colab_type":"code","colab":{}},"source":["np.array(dt_y).shape\n","dt_out=np.concatenate([dt_xv,dt_y],axis=1)\n","pd.DataFrame(dt_out).to_csv('/content/mount/My Drive/00_work/model/train.csv',index=None,header=None)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R0DgXnW-LLoQ","colab_type":"text"},"source":["## training"]},{"cell_type":"code","metadata":{"id":"cMKxIGGRuSx6","colab_type":"code","colab":{}},"source":["from keras.models import Sequential\n","from keras.layers import Dense, Dropout\n","from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten\n","from sklearn.model_selection import train_test_split\n","import keras\n","\n","from keras.layers import Embedding\n","from keras.optimizers import Adam\n","from keras.layers import LSTM\n","from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n","X_train, X_test, y_train, y_test = train_test_split(\n","       dt_xv, dt_y, test_size=0.55, random_state=42)\n","def createmode():\n","  model = Sequential()\n","\n","  data_dim=512\n","  batch_size=128\n","  num_hidden_units=256\n","  len_sequence=2\n","  learning_rate = 0.001 \n","  # model.add(LSTM(\n","  #     num_hidden_units,\n","  #     input_shape=(len_sequence, data_dim),\n","  #     return_sequences=False))\n","  # model.add(Dropout(0.5))\n","  # model.add(Flatten())\n","  # model.add(Dense(1024))\n","  model.add(Dense(1024, input_dim=1024, activation='relu'))\n","  model.add(Dropout(0.5))\n","  model.add(Dense(256, activation='sigmoid'))\n","  model.add(Dropout(0.5))\n","  model.add(Dense(64, activation='relu'))\n","  model.add(Dropout(0.5))\n","  model.add(Dense(1, activation='sigmoid'))\n","  model.compile(loss='binary_crossentropy',\n","                optimizer='rmsprop',\n","                metrics=['accuracy'])\n","  return model\n","X_train=np.array(X_train)\n","y_train=np.array(y_train)\n","X_test=np.array(X_test)\n","y_test=np.array(y_test)\n","model=createmode()\n","# model = keras.models.load_model('/content/mount/My Drive/00_work/model/weights.0.995398-.0.998731-137-0.00-0.03.hdf5')\n","log_filepath='/content/'\n","es_cb = keras.callbacks.EarlyStopping(monitor='accuracy', patience=2, verbose=1, mode='auto')\n","tb_cb = keras.callbacks.TensorBoard(log_dir=log_filepath, histogram_freq=1)\n","fpath ='/content/mount/My Drive/00_work/model/'+\\\n"," 'weights.{val_accuracy:.6f}-.{accuracy:.6f}-{epoch:02d}-{loss:.2f}-{val_loss:.2f}.hdf5'\n","cp_cb = keras.callbacks.ModelCheckpoint(filepath=fpath, \n","                                        monitor='val_accuracy',\n","                                        # monitor='accuracy',\n","                                         mode='max',\n","                                        verbose=1,\n","                                        save_best_only=True)\n","model.fit(X_train, y_train, batch_size=128, \n","          epochs=30000, \n","          verbose=0, \n","          validation_data=(X_test, y_test), \n","          callbacks=[cp_cb,  tb_cb])\n","# model.save_weights('param.hdf5')\n","# score = model.evaluate(X_test, y_test, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MIkLLzbKCcI-","colab_type":"text"},"source":["## traing 2"]},{"cell_type":"code","metadata":{"id":"ux9VDdnpCfUU","colab_type":"code","colab":{}},"source":["from keras.models import Sequential\n","from keras.layers import Dense, Dropout\n","from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten\n","from sklearn.model_selection import train_test_split\n","import keras\n","\n","from keras.layers import Embedding\n","from keras.optimizers import Adam\n","from keras.layers import LSTM\n","from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n","X_train, X_test, y_train, y_test = train_test_split(\n","       dt_xv, dt_y, test_size=0.11, random_state=42)\n","\n","batch_size=128\n","def createmode():\n","  model = Sequential()\n","\n","  len_sequence=2\n","  learning_rate = 0.001 \n","  model.add(Dense(1024, input_dim=1024, activation='relu'))\n","  model.add(Dropout(0.5))\n","  # model.add(Dense(256, activation='sigmoid'))\n","  # model.add(Dropout(0.5))\n","  # model.add(Dense(64, activation='relu'))\n","  # model.add(Dropout(0.5))\n","  model.add(Dense(1, activation='sigmoid'))\n","  model.compile(loss='binary_crossentropy',\n","                optimizer='rmsprop',\n","                metrics=['accuracy'])\n","  return model\n","X_train=np.array(X_train)\n","y_train=np.array(y_train)\n","X_test=np.array(X_test)\n","y_test=np.array(y_test)\n","model=createmode()\n","# model = keras.models.load_model('/content/mount/My Drive/00_work/model/weights.0.995398-.0.998731-137-0.00-0.03.hdf5')\n","log_filepath='/content/'\n","es_cb = keras.callbacks.EarlyStopping(monitor='accuracy', patience=2, verbose=1, mode='auto')\n","tb_cb = keras.callbacks.TensorBoard(log_dir=log_filepath, histogram_freq=1)\n","fpath ='/content/mount/My Drive/00_work/model/'+\\\n"," 'weights.{val_accuracy:.6f}-.{accuracy:.6f}-{epoch:02d}-{loss:.2f}-{val_loss:.2f}.hdf5'\n","cp_cb = keras.callbacks.ModelCheckpoint(filepath=fpath, \n","                                        monitor='val_accuracy',\n","                                        # monitor='accuracy',\n","                                         mode='max',\n","                                        verbose=1,\n","                                        save_best_only=True)\n","model.fit(X_train, y_train, batch_size=batch_size, \n","          epochs=30000, \n","          verbose=0, \n","          validation_data=(X_test, y_test), \n","          callbacks=[cp_cb,  tb_cb])\n","# model.save_weights('param.hdf5')\n","# score = model.evaluate(X_test, y_test, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zn8iBaaGhk0s","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gfnc15SRHmKZ","colab_type":"code","colab":{}},"source":["from keras.models import Sequential\n","from keras.layers.convolutional import Conv1D\n","from keras.layers.pooling import GlobalMaxPooling1D\n","\n","channel_size = 4\n","kernel_size = 5\n","\n","model = Sequential()\n","model.add( Conv1D(filters=channel_size, kernel_size=kernel_size,\n","                  strides=1, padding=\"same\", activation=\"relu\",\n","                  input_shape=(2, 512) ) )\n","model.add( Conv1D(filters=1, kernel_size=8, padding='same', activation='tanh' ) )\n","model.add( GlobalMaxPooling1D() )\n","\n","model.compile(loss='mse', optimizer='adam',metrics=['accuracy'])\n","# model.summary()\n","X_train=np.array(X_train)\n","y_train=np.array(y_train)\n","X_test=np.array(X_test)\n","y_test=np.array(y_test)\n","model.fit(X_train, y_train, batch_size=batch_size, epochs=200)\n","score = model.evaluate(X_test, y_test, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HP9JRK2ndPfO","colab_type":"code","colab":{}},"source":["model = keras.models.load_model('/content/weights.0.925532-355-0.00-1.54.hdf5')\n","# score = model.evaluate(X_train, y_train, batch_size=batch_size)\n","model.predict(X_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PCcG0-voLSZc","colab_type":"text"},"source":["## predition"]},{"cell_type":"code","metadata":{"id":"oNPKaNl_uDEO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"352a8a73-225a-4973-de4b-24795adcf00b"},"source":["import tensorflow_hub as hub\n","import tensorflow_text\n","import numpy as np\n","import pandas as pd\n","import json\n","from  datetime import datetime as dt\n","# model = keras.models.load_model('weights.1.000000-83-0.00-0.00.hdf5')\n","\n","fn='weights.0.993628-.0.998702-80-0.01-0.05.hdf5'\n","fn='weights.0.994690-.0.998206-43-0.01-0.03.hdf5'\n","fn='weights.0.993274-.0.999244-81-0.00-0.05.hdf5'\n","model = keras.models.load_model('/content/mount/My Drive/00_work/model/'+fn)\n","def get_remark_dt():\n","  col='all'\n","  basedir='/content/mount/My Drive/00_work/guidance/20200701/05_all/'\n","\n","  # col='synonyms'\n","  # basedir='/content/mount/My Drive/00_work/guidance/20200701/04_word/'\n","\n","  fn=\"/word_\"+col+\".json\"\n","  json_remark = json.load(open(basedir+\"sentence.json\", \"r\"))\n","  json_title = json.load(open(basedir+fn, \"r\"))\n","  return json_remark,json_title\n","def get_remark_dt2(fn_in):\n","  col='all'\n","  basedir='/content/mount/My Drive/00_work/guidance/20200701/05_all/'\n","\n","  # col='synonyms'\n","  # basedir='/content/mount/My Drive/00_work/guidance/20200701/04_word/'\n","\n","  fn_remark='/content/mount/My Drive/00_work/guidance/remak/'+fn_in\n","  json_remark = json.load(open(fn_remark, \"r\"))\n","  fn=\"/word_\"+col+\".json\"\n","  json_title = json.load(open(basedir+fn, \"r\"))\n","  return json_remark,json_title\n","col='all'\n","fn='notification_case_sentence'\n","fn='human_power_extraction_sentence'\n","json_remark,json_title=get_remark_dt2(fn+'.json')\n","# json_remark,json_title=get_remark_dt()\n","\n","results = []\n","print('remark vec start',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","re_gu_list=[]\n","\n","\n","remark_vec_list=[]\n","guidan_vec_list=[]\n","for remark in json_remark:\n","  remark_vec = embed(remark[\"sentence\"])\n","  remark_vec_list.append(remark_vec[0])\n","for title in json_title:\n","\ttitle_vec = embed(title[col])\n","\tguidan_vec_list.append(title_vec[0])\n","print('guidance vec end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","\n","for i,remark in enumerate(json_remark):\n","    rg_list=[]\n","    r_doc=remark['sentence']\n","    for j, g_doc in enumerate(json_title):\n","      re_gu_vec=np.concatenate([remark_vec_list[i],guidan_vec_list[j]],axis=0)\n","      # re_gu_vec=re_gu_vec.reshape(2,512)\n","      rg_list.append(re_gu_vec)\n","    sims=model.predict(np.array(rg_list))\n","    sim_rg_list=[]\n","    for doc_id, sim in enumerate(sims): \n","      sim_rg_list.append([doc_id,sim[0]])\n","    sim_rg_list_sort=sorted(sim_rg_list,key=lambda x:x[1], reverse=True)\n","    for sim_rg in sim_rg_list_sort[0:3]:\n","      doc_id=sim_rg[0]\n","      sim_mean=sim_rg[1]\n","      gs_w=json_title[doc_id]['word']\n","      gs_s=json_title[doc_id][col]\n","      results.append([r_doc,gs_w,gs_s,sim_mean])\n","df_out=pd.DataFrame(results,columns=['sentence','word',col,'similarity'])\n","outdir='/content/mount/My Drive/00_work/guidance/output/00_nozawa/'\n","df_out.to_csv(outdir+col+fn+\"_tcds_deep_modified_sentence_word_synonyms_similarity.csv\")\n","print('finish out',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["remark vec start 20/08/09 02:58:50\n","guidance vec end 20/08/09 02:58:56\n","finish out 20/08/09 02:59:01\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DLFxQh0Uusz_","colab_type":"code","colab":{}},"source":["fn_in='human_power_extraction_sentence.json'\n","fn_remark='/content/mount/My Drive/00_work/guidance/remak/'+fn_in\n","json_remark = json.load(open(fn_remark, \"r\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GvhyiTqK_qL_","colab_type":"code","colab":{}},"source":["json_remark"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kpCsXoQBKTlf","colab_type":"code","colab":{}},"source":["from keras.models import Sequential\n","from keras.layers import Dense, Dropout\n","from keras.layers import Embedding\n","from keras.layers import LSTM\n","from keras.optimizers import Adam\n","from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n","\n","model = Sequential()\n","data_dim=512\n","batch_size=128\n","num_hidden_units=128\n","len_sequence=2\n","learning_rate = 0.001 \n","model = Sequential()\n","model.add(Conv1D(64, 3, activation='relu', input_shape=(len_sequence, data_dim)))\n","model.add(Conv1D(64, 3, activation='relu'))\n","model.add(MaxPooling1D(3))\n","model.add(Conv1D(128, 3, activation='relu'))\n","model.add(Conv1D(128, 3, activation='relu'))\n","model.add(GlobalAveragePooling1D())\n","model.add(Dropout(0.5))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy',\n","              optimizer='rmsprop',\n","              metrics=['accuracy'])\n","X_train=np.array(X_train)\n","y_train=np.array(y_train)\n","X_test=np.array(X_test)\n","y_test=np.array(y_test)\n","model.fit(X_train, y_train, batch_size=batch_size, epochs=100)\n","score = model.evaluate(X_test, y_test, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ABQAMjB4wwv-","colab_type":"code","colab":{}},"source":["model.predict([X_test[0]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iNMNZeDDzyhe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"187605b0-5eb2-4cd8-fb6f-9dc3d740e76a"},"source":["X_test.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(376, 2, 512)"]},"metadata":{"tags":[]},"execution_count":247}]},{"cell_type":"code","metadata":{"id":"Ra4-amjcrkDQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"bae6b695-a852-4d31-abb4-ddd521c782bb"},"source":["X_train.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1142, 1024)"]},"metadata":{"tags":[]},"execution_count":122}]},{"cell_type":"code","metadata":{"id":"ns7rpKoWjfLb","colab_type":"code","colab":{}},"source":["tf.reshape(rec,(-1,1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3O7ljYFPiehw","colab_type":"code","colab":{}},"source":["train_vec"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z_9Uq_kXz6rc","colab_type":"code","colab":{}},"source":["# !pip install -U sentence-transformers > /dev/null\n","# !apt-get install mecab mecab-ipadic-utf8 python-mecab libmecab-dev > /dev/null\n","# !pip install mecab-python3  > /dev/null\n","# !pip install ginza > /dev/null\n","# !pip install unidic-lite > /dev/null"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dW6aB9YelVrJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"27ae5129-9014-435c-d8e5-42b520459ac8"},"source":["\n","# !aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y > /dev/null\n","# !pip install unidic-lite > /dev/null\n","# !pip install transformers > /dev/null\n","# !pip install ginza > /dev/null"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pbzkh2P28VMo","colab_type":"code","colab":{}},"source":["!apt install aptitude swig > /dev/null\n","!pip install transformers > /dev/null\n","!pip install -U sentence-transformers > /dev/null\n","!pip install mecab-python3==0.996.5 > /dev/null"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l7vBH-7tjngw","colab_type":"code","colab":{}},"source":["import transformers\n","transformers.BertTokenizer = transformers.BertJapaneseTokenizer\n","from sentence_transformers import SentenceTransformer\n","from sentence_transformers import models\n","\n","transformer = models.BERT('cl-tohoku/bert-base-japanese-whole-word-masking')\n","pooling = models.Pooling(transformer.get_word_embedding_dimension(), pooling_mode_mean_tokens=True, pooling_mode_cls_token=False, pooling_mode_max_tokens=False)\n","model = SentenceTransformer(modules=[transformer, pooling])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FgA0DfEMkBEp","colab_type":"code","colab":{}},"source":["sentences = ['吾輩は猫である','本日は晴天なり']\n","vec_b = model.encode(sentences)\n","vec_u=embed(sentences)\n","for i, embedding in enumerate(vec_u):\n","  print(\"[%d] : %s\" % (i, embedding.shape, ))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JrydvuCVo1tv","colab_type":"code","colab":{}},"source":["from sklearn.metrics.pairwise import cosine_similarity\n","sim_b=cosine_similarity( vec_b, vec_b)\n","sim_u=cosine_similarity( vec_u, vec_u)\n","print(sim_b)\n","print(sim_u)\n","# sim2 = 1 - np.arccos(cosine_similarity(embeddings,embeddings))/np.pi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7MtgPT8CqX-v","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"2da83171-e3ef-465a-9f07-561cc8f55f1a"},"source":["sim"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1.0000002 , 0.75205743],\n","       [0.75205743, 1.0000001 ]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"YkacnQrrqbk0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"5f2fb8ac-b625-4b73-80d7-775e54fde090"},"source":["sim2"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[      nan, 0.7709384],\n","       [0.7709384,       nan]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"WXMQs6OMnCoc","colab_type":"code","colab":{}},"source":["!git clone https://github.com/STAIR-Lab-CIT/STAIR-captions\n","!tar zxvf STAIR-captions/stair_captions_v1.2.tar.gz\n","!ls -lh *.json"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MF1Qfc1HnGeL","colab_type":"code","colab":{}},"source":["import json\n","with open(\"stair_captions_v1.2_val.json\", \"r\") as f:\n","  json_data_val = json.load(f)\n","with open(\"stair_captions_v1.2_train.json\", \"r\") as f:\n","  json_data_train = json.load(f)\n","dataset = {}\n","ids = []\n","captions = []\n","\n","def build_dataset(dataset, json_data):\n","  num_samples = len(json_data['annotations'])\n","  for i in range(num_samples):\n","    anno = json_data['annotations'][i]\n","    image_id = anno[\"image_id\"]\n","    image_captions = dataset.get(image_id, [])\n","    image_captions.append((anno[\"id\"], anno[\"caption\"]))\n","    ids.append(anno[\"id\"])\n","    captions.append(anno[\"caption\"])\n","    dataset[image_id] = image_captions\n","\n","build_dataset(dataset, json_data_val)\n","build_dataset(dataset, json_data_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ioRBFlSenOJD","colab_type":"code","colab":{}},"source":["id2idx = {id:idx for idx, id in enumerate(ids)}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tOPiXSlVnT9E","colab_type":"code","colab":{}},"source":["import numpy as np\n","import spacy\n","import pkg_resources, imp\n","imp.reload(pkg_resources)\n","\n","nlp = spacy.load(\"ja_ginza\")\n","\n","vectors = []\n","for caption in captions:\n","  doc = nlp(caption, disable=['ner'])\n","  vectors.append(doc.veSentence-BERTctor)\n","\n","del nlp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MaS3zC9lnaIK","colab_type":"code","colab":{}},"source":["from sklearn.metrics.pairwise import cosine_similarity\n","def similarity(id1, id2):\n","  return cosine_similarity([vectors[id2idx[id1]]], [vectors[id2idx[id2]]])[0][0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CyJtNom5nz45","colab_type":"code","colab":{}},"source":["import random\n","\n","def make_triplets(dataset, threshold=0.85, seed=7, max_tries=25):\n","  triplets = []\n","  random.seed(seed)\n","  neg_candidate_indices = list(range(len(ids)))\n","  random.shuffle(neg_candidate_indices)\n","  def log(i, str):\n","    if i % 5000 == 0:\n","      print(str)\n","\n","  for i, image_id in enumerate(list(dataset.keys())): \n","    log(i, \"### %d ###\" % (image_id))\n","\n","    # pickup positive pair.\n","    score = 0.0\n","    tries = 0\n","    while score < threshold : \n","      [(id, caption), (id_pos, caption_pos)] = random.sample(dataset[image_id],2)\n","      score = similarity(id, id_pos)\n","      tries+=1\n","      if tries > max_tries:\n","        break\n","    if score < threshold:\n","      continue\n","\n","    # pickup negative one.\n","    id_neg = id\n","    current_caption_ids = [id_cap[0] for id_cap in dataset[image_id]]\n","    while id_neg in current_caption_ids:\n","      idx_neg = neg_candidate_indices.pop()\n","      id_neg = ids[idx_neg]\n","    caption_neg = captions[id2idx[id_neg]]\n","\n","    log(i, \"  pos:  score: %4.2f [%s]:[%s]\" % (score, caption, caption_pos))\n","    log(i, \"  neg:  score: %4.2f [%s]:[%s]\" % (similarity(id, id_neg), caption, caption_neg))\n","    triplets.append({\n","      \"image_id\": image_id,\n","      \"id\": id,\n","      \"id_pos\": id_pos,\n","      \"id_neg\": id_neg,\n","      \"caption\": caption,\n","      \"caption_pos\": caption_pos,\n","      \"caption_neg\": caption_neg  \n","    })\n","  return triplets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U7PtWiWDn3Bb","colab_type":"code","colab":{}},"source":["triplets = make_triplets(dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CzZ4EaWSn5HC","colab_type":"code","colab":{}},"source":["triplets"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dZHSrYvIBVui","colab_type":"text"},"source":["# random forest"]},{"cell_type":"markdown","metadata":{"id":"j0BLz_2ojOth","colab_type":"text"},"source":["## sample"]},{"cell_type":"code","metadata":{"id":"vQNyLEl2jR8l","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":145},"outputId":"c085a277-0109-4f30-932b-a611dd2129c7"},"source":["#https://www.sejuku.net/blog/64374\n","from sklearn import datasets\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.ensemble import RandomForestRegressor as RFR\n"," \n","from sklearn.model_selection import train_test_split, GridSearchCV\n","boston = datasets.load_boston()\n"," \n","train_data_bs, test_data_bs, train_labels_bs, test_labels_bs \\\n","= train_test_split(boston.data, boston.target, test_size=0.2)\n","rg = RFR(n_jobs=-1, random_state=2525)\n"," \n","rg.fit(train_data_bs,train_labels_bs)\n","\n","# RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n","#            max_features='auto', max_leaf_nodes=None,\n","#            min_impurity_decrease=0.0, min_impurity_split=None,\n","#            min_samples_leaf=1, min_samples_split=2,\n","#            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n","#            oob_score=False, random_state=2525, verbose=0, warm_start=False)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n","                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n","                      max_samples=None, min_impurity_decrease=0.0,\n","                      min_impurity_split=None, min_samples_leaf=1,\n","                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n","                      n_estimators=100, n_jobs=-1, oob_score=False,\n","                      random_state=2525, verbose=0, warm_start=False)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"J2G0vuUbjv0b","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":344},"outputId":"a94465b9-14a9-44b4-b9be-8238d359a0d5"},"source":["predicted_labels_bs = rg.predict(test_data_bs)\n","rg.score(test_data_bs, test_labels_bs)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-8fdc3ae72d07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredicted_labels_bs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_bs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_bs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels_bs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0;31m# Assign chunk of trees to jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]},{"cell_type":"markdown","metadata":{"id":"IqOdETWkjAhu","colab_type":"text"},"source":["##data prepare"]},{"cell_type":"code","metadata":{"id":"Ov_WvNb3jIQX","colab_type":"code","colab":{}},"source":["X_train, X_test, y_train, y_test = train_test_split(\n","       dt_xv, dt_y, test_size=0.22, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3nN1m2W3jC1D","colab_type":"text"},"source":["## src\n","\n"]},{"cell_type":"code","metadata":{"id":"gzgLZKUfkZyk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":145},"outputId":"577254ee-2cec-4a84-d4a4-95fcff4e09b0"},"source":["from sklearn import datasets\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.ensemble import RandomForestRegressor as RFR\n"," \n","from sklearn.model_selection import train_test_split, GridSearchCV\n","# boston = datasets.load_boston()\n"," \n","train_data_bs, test_data_bs, train_labels_bs, test_labels_bs \\\n","= train_test_split(dt_xv, dt_y, test_size=0.2)\n","rg = RFR(n_jobs=-1, random_state=2525)\n","rg.fit(train_data_bs,train_labels_bs)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n","                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n","                      max_samples=None, min_impurity_decrease=0.0,\n","                      min_impurity_split=None, min_samples_leaf=1,\n","                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n","                      n_estimators=100, n_jobs=-1, oob_score=False,\n","                      random_state=2525, verbose=0, warm_start=False)"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"7HhfGs5UkjcK","colab_type":"code","colab":{}},"source":["predicted_labels_bs = rg.predict(test_data_bs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GbnR_MVo3Q9V","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"56dce15a-ac19-404c-d0b8-88da7386959a"},"source":["rg.score(test_data_bs, test_labels_bs)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.867460100713807"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"LOcqV9DW3bsl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"92589ae3-589c-41c5-89ce-f44704c16de2"},"source":["predicted_labels_bs"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.    , 0.    , 0.99  , ..., 0.01  , 0.04  , 0.7975])"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"GeRq5nI4komK","colab_type":"text"},"source":["# svm regression"]},{"cell_type":"markdown","metadata":{"id":"mw3RLMz7ksCt","colab_type":"text"},"source":["## sample"]},{"cell_type":"code","metadata":{"id":"UxqUt6oIkt0g","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":90},"outputId":"415a8e6a-7cf8-4931-9658-2244b4b22143"},"source":["# 1：ライブラリのインポート--------------------------------\n","import numpy as np #numpyという行列などを扱うライブラリを利用\n","import pandas as pd #pandasというデータ分析ライブラリを利用\n","import matplotlib.pyplot as plt #プロット用のライブラリを利用\n","# from sklearn import cross_validation, preprocessing, linear_model, svm #機械学習用のライブラリを利用\n","from sklearn import  linear_model, svm #機械学習用のライブラリを利用\n","\n","# # 2：Housingのデータセットを読み込む--------------------------------\n","# df=pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data', header=None, sep='\\s+')\n","# df.columns=['CRIM','ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n","# X_rm=df[['RM']].values\n","# X=df.iloc[:, 0:13]\n","# #X=df[['AGE']].values\n","# Y=df['MEDV'].values\n","\n","# # 3：データの整形-------------------------------------------------------\n","# sc=preprocessing.StandardScaler()\n","# sc.fit(X)\n","# X=sc.transform(X)\n","# sc.fit(X_rm)\n","# X_rm=sc.transform(X_rm)\n","\n","# 4：学習データとテストデータに分割する-------------------------------\n","# X_rm_train, X_rm_test, Y_train, Y_test = cross_validation.train_test_split(X_rm, Y, test_size=0.5, random_state=0)\n","\n","X_rm_train, X_rm_test, Y_train, Y_test \\\n","= train_test_split(dt_xv[0:1000], dt_y[0:1000], test_size=0.2)\n","\n","# 5：SGD Regressorを適用する-------------------------------------------\n","clf_rm = linear_model.SGDRegressor(max_iter=1000)\n","clf_rm.fit(X_rm_train, Y_train)\n","\n","# 解説6：SVR linear Regressorを適用する-------------------------------------------\n","clf_svr = svm.SVR(kernel='linear', C=1e3, epsilon=2.0)\n","clf_svr.fit(X_rm_train, Y_train)\n","\n","# 7：結果をプロットする------------------------------------------------\n","# %matplotlib inline\n","# line_X=np.arange(-4, 4, 0.1) #3から10まで1刻み\n","# line_Y_sgd=clf_rm.predict(line_X[:, np.newaxis])\n","# line_Y_svr=clf_svr.predict(line_X[:, np.newaxis])\n","# plt.figure(figsize=(10,10))\n","# plt.subplot(2, 1, 1)\n","# plt.scatter(X_rm_train, Y_train, c='b', marker='s')\n","# plt.plot(line_X, line_Y_sgd, c='r')\n","# plt.plot(line_X, line_Y_svr, c='g')\n","# plt.show\n","\n","# 8：誤差-------------------------------------------------\n","Y_pred_sgd=clf_rm.predict(X_rm_test)\n","Y_pred_svr=clf_svr.predict(X_rm_test)\n","print(\"\\n「SGDの平均2乗誤差」と「SVRの平均二乗誤差」\")\n","RMS_sgd=np.mean((Y_pred_sgd - Y_test) ** 2)\n","RMS_svr=np.mean((Y_pred_svr - Y_test) ** 2)\n","print(RMS_sgd)\n","print(RMS_svr)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","「SGDの平均2乗誤差」と「SVRの平均二乗誤差」\n","0.13704733703649827\n","0.25\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Kc0ej9b_uNKL","colab_type":"code","colab":{}},"source":["clf_svr.score(Y_test, Y_pred_svr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GvLoOeCMuegR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"68be1d0b-84ab-4769-fda0-da2aa85e30aa"},"source":["Y_pred_sgd.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(200,)"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"GmDIb-S9qHQC","colab_type":"text"},"source":["#LinearRegression"]},{"cell_type":"code","metadata":{"id":"oi8DF5DZqMGt","colab_type":"code","colab":{}},"source":["#https://www.freecodecamp.org/news/how-to-build-and-train-linear-and-logistic-regression-ml-models-in-python/\n","from sklearn.linear_model import LinearRegression\n","from sklearn import metrics\n","model = LinearRegression()\n","x_train, x_test, y_train, y_test \\\n","= train_test_split(dt_xv, dt_y, test_size=0.2)\n","model.fit(x_train, y_train)\n","print(model.coef_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"copyikJ2q06G","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":283},"outputId":"575ca6f3-ee39-4b7c-97c0-0e5f9345a627"},"source":["import matplotlib.pyplot as plt\n","predictions = model.predict(x_test)\n","metrics.mean_absolute_error(y_test, predictions)\n","metrics.mean_squared_error(y_test, predictions)\n","plt.scatter(y_test, predictions)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.collections.PathCollection at 0x7fa748657518>"]},"metadata":{"tags":[]},"execution_count":15},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"shG4cepnrQFC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"4f90c9b1-ebb7-45f3-f201-3f8cae0ffdb6"},"source":["metrics.mean_absolute_error(y_test, predictions)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.14884536740193693"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"-j1dfsP-RKm6","colab_type":"text"},"source":["# USE max tokens test"]},{"cell_type":"code","metadata":{"id":"w3yY73hRJPVf","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","fn='/content/mount/My Drive/00_work/USE'\n","# model_1=tf.keras.models.load_model(fn)\n","model_1 = tf.saved_model.load(fn)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"07-R0UC6krwZ","colab_type":"code","colab":{}},"source":["embeds = hub.KerasLayer(fn,name='Embeddings/sharded_16:0')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v9apRvDLnudJ","colab_type":"code","colab":{}},"source":["embeds2 = hub.KerasLayer(fn,name='Embeddings/ssssssssssssssharded_16:0')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kIHrGOF2Lq2a","colab_type":"code","colab":{}},"source":["model_1.trainable_variables.trainable_variables"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Lzr22hJREJs","colab_type":"code","colab":{}},"source":["from keras.models import Sequential\n","from keras.layers import Dense, Dropout\n","from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten\n","use_layer = hub.KerasLayer(model_1, trainable=True)\n","\n","model = tf.keras.Sequential()\n","model.add(Dense(1024, input_dim=1024, activation='relu'))\n","model = tf.keras.Sequential([\n","     model,\n","     use_layer\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"117l-B28aa1j","colab_type":"code","colab":{}},"source":["embed(\"aa\", signature=\"default\", as_dict=True)[\"default\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6CGvgplQY8fv","colab_type":"code","colab":{}},"source":["# module_handler = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n","use_layer = hub.KerasLayer(module_handler, trainable=True)\n","model = tf.keras.Sequential([\n","     tf.keras.layers.InputLayer(input_shape=(1,512)),\n","    #  use_layer,\n","     tf.keras.layers.Dense(512)\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DFez2-U3TBEL","colab_type":"code","colab":{}},"source":["# ライブラリ「TensorFlow」のtensorflowパッケージを「tf」という別名でインポート\n","import tensorflow as tf\n","from tensorflow.keras import layers       # 「レイヤーズ」モジュールのインポート\n","\n","# 定数（モデル定義時に必要となる数値）\n","INPUT_FEATURES = 2  # 入力（特徴）の数： 2\n","LAYER1_NEURONS = 3  # ニューロンの数： 3\n","LAYER2_NEURONS = 3  # ニューロンの数： 3\n","OUTPUT_RESULTS = 1  # 出力結果の数： 1\n","model = tf.keras.models.Sequential([    # モデルの生成\n","\n","  # 隠れ層：1つ目のレイヤー\n","  layers.Dense(                 # 全結合層\n","      input_shape=(INPUT_FEATURES,),       # 入力の形状（＝入力層）\n","      name='layer1',                       # 表示用に名前付け\n","      kernel_initializer='glorot_uniform', # 重さの初期化（一様分布のランダム値）\n","      bias_initializer='zeros',            # バイアスの初期化（0）\n","      units=LAYER1_NEURONS,                # ユニットの数\n","      activation='tanh'),                  # 活性化関数\n","  use_layer,\n","  # 隠れ層：2つ目のレイヤー\n","  layers.Dense(                 # 全結合層\n","      name='layer2',                       # 表示用に名前付け\n","      kernel_initializer='glorot_uniform', # 重さの初期化\n","      bias_initializer='zeros',            # バイアスの初期化\n","      units=LAYER2_NEURONS,                # ユニットの数\n","      activation='tanh'),                  # 活性化関数\n","\n","  # 出力層\n","  layers.Dense(                 # 全結合層\n","      name='layer_out',                    # 表示用に名前付け\n","      kernel_initializer='glorot_uniform', # 重さの初期化\n","      bias_initializer='zeros',            # バイアスの初期化\n","      units=OUTPUT_RESULTS,                # ユニットの数\n","      activation='tanh'),                  # 活性化関数\n","\n","], name='sequential_constructor'           # モデルにも名前付け\n",")\n","\n","# 以上でモデル設計は完了\n","model.summary()      "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ADaD3UwyRTm2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"46c0079c-3632-416c-9e7c-d98d3686cd3f"},"source":["# import tensorflow as tf  \n","# import tensorflow_hub as hub\n","import numpy as np\n","t1 = 'The recent rapid progress of neural network-based natural language understanding research, especially on learning semantic text representations, can enable truly novel products such as Smart Compose and Talk to Books. It can also help improve performance on a variety of natural language tasks which have limited amounts of training data, such as building strong text classifiers from as few as 100 labeled examples. Below, we discuss two papers reporting recent progress on semantic representation research at Google, as well as two new models available for download on TensorFlow Hub that we hope developers will use to build new and exciting applications. In “Learning Semantic Textual Similarity from Conversations”, we introduce a new way to learn sentence representations for semantic textual similarity. The intuition is that sentences are semantically similar if they have a similar distribution of responses. For example, “How old are you?” and “What is your age?” are both questions about age, which can be answered by similar responses such as “I am 20 years old”. In contrast, while “How are you?” and “How old are you?” contain almost identical words, they have very different meanings and lead to different responses. In this work, we aim to learn semantic similarity by way of a response classification task: given a conversational input, we wish to classify the correct response from a batch of randomly selected responses. But, the ultimate goal is to learn a model that can return encodings representing a variety of natural language relationships, including similarity and relatedness. By adding another prediction task (In this case, the SNLI entailment dataset) and forcing both through shared encoding layers, we get even better performance on similarity measures such as the STSBenchmark (a sentence similarity benchmark) and CQA task B (a question/question similarity task). This is because logical entailment is quite different from simple equivalence and provides more signal for learning complex semantic representations. ' \n","t2 = 'The recent rapid progress of neural network-based natural language understanding research, especially on learning semantic text representations, can enable truly novel products such as Smart Compose and Talk to Books. It can also help improve performance on a variety of natural language tasks which have limited amounts of training data, such as building strong text classifiers from as few as 100 labeled examples. Below, we discuss two papers reporting recent progress on semantic representation research at Google, as well as two new models available for download on TensorFlow Hub that we hope developers will use to build new and exciting applications. In “Learning Semantic Textual Similarity from Conversations”, we introduce a new way to learn sentence representations for semantic textual similarity. The intuition is that sentences are semantically similar if they have a similar distribution of responses. For example, “How old are you?” and “What is your age?” are both questions about age, which can be answered by similar responses such as “I am 20 years old”. In contrast, while “How are you?” and “How old are you?” contain almost identical words, they have very different meanings and lead to different responses. In this work, we aim to learn semantic similarity by way of a response classification task: given a conversational input, we wish to classify the correct response from a batch of randomly selected responses. But, the ultimate goal is to learn a model that can return encodings representing a variety of natural language relationships, including similarity and relatedness. By adding another prediction task (In this case, the SNLI entailment dataset) and forcing both through shared encoding layers, we get even better performance on similarity measures such as the STSBenchmark (a sentence similarity benchmark) and CQA task B (a question/question similarity task). This is because logical entailment is quite different from simple equivalence and provides more signal for learning complex semantic representations. ' \n","t1='The recent rapid progress of neural network-based natural language understanding research especially a'\n","t2='The recent rapid progress of neural network-based natural language understanding research especially a'\n","# with tf.Session() as session: \n","#      session.run([tf.global_variables_initializer(), tf.tables_initializer()]) \n","#      embeddings = session.run(embed([t1, t2]))\n","embeddings=np.array(model_1([t1, t1]))\n","print(np.allclose(embeddings[0, :], embeddings[1, :]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["False\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GO8nPkZyTLD5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"31c5b1c2-7314-4ffd-c08c-e52bba0cdce3"},"source":["t1 = 'The recent rapid progress of neural network-based natural language understanding research, especially on learning semantic text representations, can enable truly novel products such as Smart Compose and Talk to Books. It can also help improve performance on a variety of natural language tasks which have limited amounts of training data, such as building strong text classifiers from as few as 100 labeled examples. Below, we discuss two papers reporting recent progress on semantic representation research at Google, as well as two new models available for download on TensorFlow Hub that we hope developers will use to build new and exciting applications. In “Learning Semantic Textual Similarity from Conversations”, we introduce a new way to learn sentence representations for semantic textual similarity. The intuition is that sentences are semantically similar if they have a similar distribution of responses. For example, “How old are you?” and “What is your age?” are both questions about age, which can be answered by similar responses such as “I am 20 years old”. In contrast, while “How are you?” and “How old are you?” contain almost identical words, they have very different meanings and lead to different responses. In this work, we aim to learn semantic similarity by way of a response classification task: given a conversational input, we wish to classify the correct response from a batch of randomly selected responses. But, the ultimate goal is to learn a model that can return encodings representing a variety of natural language relationships, including similarity and relatedness. By adding another prediction task (In this case, the SNLI entailment dataset) and forcing both through shared encoding layers, we get even better performance on similarity measures such as the STSBenchmark (a sentence similarity benchmark) and CQA task B (a question/question similarity task). This is because logical entailment is quite different from simple equivalence and provides more signal for learning complex semantic representations.The recent rapid progress of neural network-based natural language understanding research, especially on learning semantic text representations, can enable truly novel products such as Smart Compose and Talk to Books. It can also help improve performance on a variety of natural language tasks which have limited amounts of training data, such as building strong text classifiers from as few as 100 labeled examples. Below, we discuss two papers reporting recent progress on semantic representation research at Google, as well as two new models available for download on TensorFlow Hub that we hope developers will use to build new and exciting applications. In “Learning Semantic Textual Similarity from Conversations”, we introduce a new way to learn sentence representations for semantic textual similarity. The intuition is that sentences are semantically similar if they have a similar distribution of responses. For example, “How old are you?” and “What is your age?” are both questions about age, which can be answered by similar responses such as “I am 20 years old”. In contrast, while “How are you?” and “How old are you?” contain almost identical words, they have very different meanings and lead to different responses. In this work, we aim to learn semantic similarity by way of a response classification task: given a conversational input, we wish to classify the correct response from a batch of randomly selected responses. But, the ultimate goal is to learn a model that can return encodings representing a variety of natural language relationships, including similarity and relatedness. By adding another prediction task (In this case, the SNLI entailment dataset) and forcing both through shared encoding layers, we get even better performance on similarity measures such as the STSBenchmark (a sentence similarity benchmark) and CQA task B (a question/question similarity task). This is because logical entailment is quite different from simple equivalence and provides more signal for learning complex semantic representations.The recent rapid progress of neural network-based natural language understanding research, especially on learning semantic text representations, can enable truly novel products such as Smart Compose and Talk to Books. It can also help improve performance on a variety of natural language tasks which have limited amounts of training data, such as building strong text classifiers from as few as 100 labeled examples. Below, we discuss two papers reporting recent progress on semantic representation research at Google, as well as two new models available for download on TensorFlow Hub that we hope developers will use to build new and exciting applications. In “Learning Semantic Textual Similarity from Conversations”, we introduce a new way to learn sentence representations for semantic textual similarity. The intuition is that sentences are semantically similar if they have a similar distribution of responses. For example, “How old are you?” and “What is your age?” are both questions about age, which can be answered by similar responses such as “I am 20 years old”. In contrast, while “How are you?” and “How old are you?” contain almost identical words, they have very different meanings and lead to different responses. In this work, we aim to learn semantic similarity by way of a response classification task: given a conversational input, we wish to classify the correct response from a batch of randomly selected responses. But, the ultimate goal is to learn a model that can return encodings representing a variety of natural language relationships, including similarity and relatedness. By adding another prediction task (In this case, the SNLI entailment dataset) and forcing both through shared encoding layers, we get even better performance on similarity measures such as the STSBenchmark (a sentence similarity benchmark) and CQA task B (a question/question similarity task). This is because logical entailment is quite different from simple equivalence and provides more signal for learning complex semantic representations learning complex semantic representations learning complex semantic representations learning complex semantic representations learning complex semantic representations learning complex semantic representations learning complex semantic representations learning complex semantic representations learning complex semantic representations learning complex semantic representations learning complex semantic representations learning complex semantic representations learning complex semantic representations learning complex semantic representations'\n","t2 = 'The recent rapid progress of neural network-based natural language understanding research, especially on learning semantic text representations, can enable truly novel products such as Smart Compose and Talk to Books. It can also help improve performance on a variety of natural language tasks which have limited amounts of training data, such as building strong text classifiers from as few as 100 labeled examples. Below, we discuss two papers reporting recent progress on semantic representation research at Google, as well as two new models available for download on TensorFlow Hub that we hope developers will use to build new and exciting applications. In “Learning Semantic Textual Similarity from Conversations”, we introduce a new way to learn sentence representations for semantic textual similarity. The intuition is that sentences are semantically similar if they have a similar distribution of responses. For example, “How old are you?” and “What is your age?” are both questions about age, which can be answered by similar responses such as “I am 20 years old”. In contrast, while “How are you?” and “How old are you?” contain almost identical words, they have very different meanings and lead to different responses. In this work, we aim to learn semantic similarity by way of a response classification task: given a conversational input, we wish to classify the correct response from a batch of randomly selected responses. But, the ultimate goal is to learn a model that can return encodings representing a variety of natural language relationships, including similarity and relatedness. By adding another prediction task (In this case, the SNLI entailment dataset) and forcing both through shared encoding layers, we get even better performance on similarity measures such as the STSBenchmark (a sentence similarity benchmark) and CQA task B (a question/question similarity task). This is because logical entailment is quite different from simple equivalence and provides more signal for learning complex semantic representations.The recent rapid progress of neural network-based natural language understanding research, especially on learning semantic text representations, can enable truly novel products such as Smart Compose and Talk to Books. It can also help improve performance on a variety of natural language tasks which have limited amounts of training data, such as building strong text classifiers from as few as 100 labeled examples. Below, we discuss two papers reporting recent progress on semantic representation research at Google, as well as two new models available for download on TensorFlow Hub that we hope developers will use to build new and exciting applications. In “Learning Semantic Textual Similarity from Conversations”, we introduce a new way to learn sentence representations for semantic textual similarity. The intuition is that sentences are semantically similar if they have a similar distribution of responses. For example, “How old are you?” and “What is your age?” are both questions about age, which can be answered by similar responses such as “I am 20 years old”. In contrast, while “How are you?” and “How old are you?” contain almost identical words, they have very different meanings and lead to different responses. In this work, we aim to learn semantic similarity by way of a response classification task: given a conversational input, we wish to classify the correct response from a batch of randomly selected responses. But, the ultimate goal is to learn a model that can return encodings representing a variety of natural language relationships, including similarity and relatedness. By adding another prediction task (In this case, the SNLI entailment dataset) and forcing both through shared encoding layers, we get even better performance on similarity measures such as the STSBenchmark (a sentence similarity benchmark) and CQA task B (a question/question similarity task). This is because logical entailment is quite different from simple equivalence and provides more signal for learning complex semantic representations.The recent rapid progress of neural network-based natural language understanding research, especially on learning semantic text representations, can enable truly novel products such as Smart Compose and Talk to Books. It can also help improve performance on a variety of natural language tasks which have limited amounts of training data, such as building strong text classifiers from as few as 100 labeled examples. Below, we discuss two papers reporting recent progress on semantic representation research at Google, as well as two new models available for download on TensorFlow Hub that we hope developers will use to build new and exciting applications. In “Learning Semantic Textual Similarity from Conversations”, we introduce a new way to learn sentence representations for semantic textual similarity. The intuition is that sentences are semantically similar if they have a similar distribution of responses. For example, “How old are you?” and “What is your age?” are both questions about age, which can be answered by similar responses such as “I am 20 years old”. In contrast, while “How are you?” and “How old are you?” contain almost identical words, they have very different meanings and lead to different responses. In this work, we aim to learn semantic similarity by way of a response classification task: given a conversational input, we wish to classify the correct response from a batch of randomly selected responses. But, the ultimate goal is to learn a model that can return encodings representing a variety of natural language relationships, including similarity and relatedness. By adding another prediction task (In this case, the SNLI entailment dataset) and forcing both through shared encoding layers, we get even better performance on similarity measures such as the STSBenchmark (a sentence similarity benchmark) and CQA task B (a question/question similarity task). This is because logical entailment is quite different from simple equivalence and provides more signal for learning complex semantic representations learning complex semantic representations learning complex semantic representations learning complex semantic representations learning complex semantic representations learning complex semantic representations learning complex semantic representations learning complex semantic representations learning complex semantic representations learning complex semantic representations learning complex semantic representations learning complex semantic representations learning complex semantic representations'\n","flg=False\n","n=4\n","for i in range(100):\n","  emb1=np.round(embed(t1),n)\n","  emb2=np.round(embed(t2),n)\n","  flg=np.allclose(emb1,emb2)\n","  if flg:\n","    break\n","print(flg)\n","  # print(np.max(np.round(emb1,6)-np.round(emb2,6)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["False\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mwXzLZdgUngl","colab_type":"code","colab":{}},"source":["print(np.round(emb1[0][0:10],6))\n","print(np.round(emb2[0][0:10],6))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Q8xC417VNSf","colab_type":"code","colab":{}},"source":["embed.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y4awS5ziUwcj","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uZHrPbND5_7n","colab_type":"text"},"source":["#xling-many/1\n","https://tfhub.dev/google/universal-sentence-encoder-xling-many/1"]},{"cell_type":"code","metadata":{"id":"Hlv1nT136KXX","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2-U9O-KvJFyL","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-ou4d9lA9Y-7","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FCyR_DUrQk4N","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598328888957,"user_tz":-540,"elapsed":22598,"user":{"displayName":"于勝龍","photoUrl":"","userId":"09684187088444140086"}},"outputId":"1a0e5a1e-5582-46b4-9780-3b827a3b3f63"},"source":["strtt='22'\n","def embed(str_in):\n","  v3=embed_ml3(str_in)\n","  # v3=np.squeeze(v3)\n","  v1=embed_xm1_single(str_in)\n","  # v1=np.squeeze(v1)\n","  res=np.concatenate([v1,v3],axis=1)\n","  # res=np.add(v1,v3)\n","  return res\n","embed(strtt)[0].shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1024,)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"kaY1eA2nEpHd","colab_type":"code","colab":{}},"source":["hiden_n='SNLI/Classifier/tanh_layer_0/dense/kernel'\n","english_sentences=[\"aaa\", \"bbbb\", \"aaa\"]\n","# hid=xling_8_embed.variable_map[hiden_n]\n","hid=xling_8_embed.get_input_info_dict()\n","sharded_0 = session.run(hid, feed_dict={text_input: english_sentences})\n","print(sharded_0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nNR6HzIqhDFI","colab_type":"text"},"source":["#pip install -U sentence-transformers\n","https://github.com/UKPLab/sentence-transformers"]},{"cell_type":"code","metadata":{"id":"GcUzYcAKhEOb","colab_type":"code","colab":{}},"source":["!pip install -U sentence-transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1xmGVqmahPJa","colab_type":"code","colab":{}},"source":["from sentence_transformers import SentenceTransformer\n","model = SentenceTransformer('bert-base-nli-mean-tokens')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5X7PWkIghSiT","colab_type":"code","colab":{}},"source":["sentences = ['入院を必要とする頭部・腹部以外の外傷・火傷がある',\n","    'Sentences are passed as a list of string.', \n","    'The quick brown fox jumps over the lazy dog.']\n","sentence_embeddings = model.encode(sentences)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NO3HruU2n5O2","colab_type":"code","colab":{}},"source":["def embed(sentences):  \n","  sentence_embeddings = model.encode(sentences)\n","  return sentence_embeddings"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1TDfoT3DhU9V","colab_type":"code","colab":{}},"source":["for sentence, embedding in zip(sentences, sentence_embeddings):\n","    print(\"Sentence:\", sentence)\n","    print(\"Embedding:\", embedding)\n","    print(\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8cd9pxKN_2tS","colab_type":"code","colab":{}},"source":["!pip install tensorflow_text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jdNoBVxvskXK","colab_type":"text"},"source":["# train use"]},{"cell_type":"markdown","metadata":{"id":"n-nuxyxwIfUI","colab_type":"text"},"source":["## resnet+use"]},{"cell_type":"markdown","metadata":{"id":"kETqDmG7dPQ-","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"oBaGE25CIjEe","colab_type":"code","colab":{}},"source":["# Convert class vectors to binary class matrices.\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pCE1K25xdQyp","colab_type":"text"},"source":["### create data"]},{"cell_type":"code","metadata":{"id":"XZihJBZidS_h","colab_type":"code","colab":{}},"source":["# df_tcd.to_json('/content/mount/My Drive/00_work/guidance/20200701/07_tcd/word_tcd.json',orient='records',force_ascii=False)\n","from itertools import permutations\n","import json\n","\n","def get_perm_lst(dt_lst):\n","  perm_lst=[]\n","  cnt =len(dt_lst)\n","  for i  in range(cnt):\n","    pe_lst=permutations(dt_lst,i+1)\n","    for dt in pe_lst:\n","      perm='、'.join(map(str, dt))\n","      perm_lst.append(perm)\n","  return perm_lst\n","fn_tcd='/content/mount/My Drive/00_work/guidance/20200701/07_tcd/word_tcd.json'\n","json_tcd = json.load(open(fn_tcd, \"r\"))\n","cols=[ 'title','confirmation','detail','synonyms']\n","dt_train_list=[]\n","wd='word'\n","for tcd in json_tcd:\n","  dt_col_lst=[]\n","  for col in cols:\n","    dt=tcd[col]\n","    if len(dt)>1:\n","      # print(len(dt),col,dt)\n","      dt_col_lst.append(dt)\n","  if len(dt_col_lst)>0:\n","    perm_list=get_perm_lst(dt_col_lst)\n","    for perm in perm_list:\n","      dt_train_list.append([tcd[wd],perm])\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","path='/content/mount/My Drive/00_work/guidance/train_dt.csv'\n","df=pd.read_csv(path,sep=',',header=0)\n","df.shape\n","df_t=pd.DataFrame(dt_train_list,columns=['word','guid_t'])\n","df_tc=pd.merge(df,df_t,how='left',left_on='title',right_on='word')\n","df_n=df_tc.dropna(how='any').loc[:,['remark','word','guid_t','label']]\n","dt_x=df_n.loc[:,['remark','guid_t']].values.tolist()\n","dt_y=df_n.loc[:,['label']].values.tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2BgnHPfldcqq","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text\n","import os\n","import random\n","import glob\n","import matplotlib.pyplot as plt \n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.applications.resnet50 import ResNet50\n","from keras.models import Sequential, Model\n","from keras.layers import Input, Flatten, Dense\n","from keras import optimizers\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","module_url = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\"\n","embedding_layer = hub.KerasLayer(module_url)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QIFyQfg8dhwC","colab_type":"text"},"source":["### mode create"]},{"cell_type":"code","metadata":{"id":"BppQbJcjdjwb","colab_type":"code","colab":{}},"source":["\n","from keras.applications.resnet50 import ResNet50\n","img_size=32\n","nb_classes=2\n","hidden_layer = tf.keras.layers.Dense(512, activation='relu')\n","output_layer = tf.keras.layers.Dense(nb_classes, activation='softmax')\n","flat_layer=tf.keras.layers.Flatten()\n","dropout=tf.keras.layers.Dropout(0.5)\n","inputs = tf.keras.layers.Input(shape=(2,), dtype=tf.string)\n","\n","\n","# input_tensor = Input(shape=(img_size,img_size,1))\n","## train skratch ==>> weights=None ##\n","# ResNet50Layer = ResNet50(include_top=False, weights=None ,input_tensor=input_tensor)\n","ResNet50Layer = ResNet50(include_top=False, weights=None ,input_shape=(img_size,img_size,1))\n","\n","x1 = embedding_layer(inputs[:,0])\n","x2 = embedding_layer(inputs[:,1])\n","\n","ho=tf.concat([x1,x2],1)\n","resin=tf.reshape(ho,(-1,img_size,img_size,1))\n","resou=ResNet50Layer(resin)\n","\n","ho=flat_layer(resou)\n","x2 = hidden_layer(ho)\n","x2=dropout(x2)\n","outputs = output_layer(x2)\n","\n","model = tf.keras.Model(inputs=inputs, outputs=outputs)\n","\n","#hyper param\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=optimizers.SGD(lr=1e-3, momentum=0.9),\n","              metrics=['accuracy'])\n","\n","# model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iiRMdNVzdta0","colab_type":"text"},"source":["### training"]},{"cell_type":"code","metadata":{"id":"Deu-f_N-dvcM","colab_type":"code","colab":{}},"source":["\n","nb_classes=2\n","dt_y = tf.keras.utils.to_categorical(dt_y, nb_classes)\n","x_train, x_test, y_train, y_test = train_test_split(dt_x, \n","                                                    dt_y, \n","                                                    test_size=0.3, \n","                                                    stratify=dt_y, \n","                                                    random_state=42)\n","log_filepath='/content/'\n","es_cb = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=2, verbose=1, mode='auto')\n","# tb_cb = tf.keras.callbacks.TensorBoard(log_dir=log_filepath, histogram_freq=1)\n","fpath ='/content/mount/My Drive/00_work/resmodel/'+\\\n"," 'weights.{val_accuracy:.6f}-.{accuracy:.6f}-{epoch:02d}-{loss:.2f}-{val_loss:.2f}.hdf5'\n","cp_cb = tf.keras.callbacks.ModelCheckpoint(filepath=fpath, \n","                                        monitor='val_accuracy',\n","                                        # monitor='accuracy',\n","                                         mode='max',\n","                                        verbose=1,\n","                                        save_best_only=True\n","                                        save_weights_only=False)\n","# model = tf.keras.models.load_model('/content/mount/My Drive/00_work/model/weights.0.897210-.0.906931-01-0.43-0.59.hdf5')\n","model.fit( np.asarray(x_train), \n","           np.asarray(y_train), \n","          # batch_size=10,\n","          # verbose=0, \n","          epochs=110, \n","          validation_data=( np.asarray(x_test),  np.asarray(y_test)),\n","          callbacks=[cp_cb]\n","          )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QlejHSf7ddP9","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-w79g_mlXTDn","colab_type":"text"},"source":["## model src 1"]},{"cell_type":"code","metadata":{"id":"v_Tp8YGNsuHu","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text\n","module_url = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\"\n","# module_url = \"https://tfhub.dev/google/universal-sentence-encoder-xling-many/1\"\n","# embedding_layer = hub.KerasLayer(module_url,trainable=True)\n","embedding_layer = hub.KerasLayer(module_url)\n","hidden_layer = tf.keras.layers.Dense(512, activation='relu')\n","output_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n","flat_layer=tf.keras.layers.Flatten()\n","dropout=tf.keras.layers.Dropout(0.5)\n","inputs = tf.keras.layers.Input(shape=(2,), dtype=tf.string)\n","x1 = embedding_layer(inputs[:,0])\n","x2 = embedding_layer(inputs[:,1])\n","# h1=hidden_layer(x1)\n","# h2=hidden_layer(x2)\n","\n","ho=tf.concat([x1,x2],1)\n","# ho=flat_layer(ho)\n","x2 = hidden_layer(ho)\n","x2=dropout(x2)\n","outputs = output_layer(x2)\n","\n","model = tf.keras.Model(inputs=inputs, outputs=outputs)\n","# model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"62Z2awSoO2Jr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"4263118e-1d84-4a2a-8be3-a1acba833ba5"},"source":["ho.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([None, 1024])"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"lSOSHZACJak0","colab_type":"text"},"source":["## model src old"]},{"cell_type":"code","metadata":{"id":"R8sgmiGjNwNq","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text\n","module_url = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\"\n","# module_url = \"https://tfhub.dev/google/universal-sentence-encoder-xling-many/1\"\n","embedding_layer = hub.KerasLayer(module_url,trainable=True)\n","# embedding_layer = hub.KerasLayer(module_url)\n","hidden_layer = tf.keras.layers.Dense(1024, activation='relu')\n","output_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n","# flat_layer=tf.keras.layers.Flatten(input_shape=(2,))\n","flat_layer=tf.keras.layers.Flatten()\n","\n","inputs = tf.keras.layers.Input(shape=(2,), dtype=tf.string)\n","x = embedding_layer(tf.squeeze(tf.cast(inputs, tf.string)))\n","# x = embedding_layer(inputs)\n","x1=tf.reshape(x,[-1,1024])\n","# x1=flat_layer(x)\n","x2 = hidden_layer(x1)\n","x2=tf.keras.layers.Dropout(0.5)(x2)\n","outputs = output_layer(x2)\n","\n","model = tf.keras.Model(inputs=inputs, outputs=outputs)\n","# model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yw3pM3NkL5E1","colab_type":"code","colab":{}},"source":["k=embedding_layer(inputs[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zcgZT1QAK-Fl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"3717bc9e-4c78-4b1b-8f10-eb2e6d74444f"},"source":["x1.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([None, 512])"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"2gyCFqPvY559","colab_type":"code","colab":{}},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_XoavpX-xbf1","colab_type":"code","colab":{}},"source":["# dt_x=np.array(dt_x)\n","\n","from sklearn.model_selection import train_test_split\n","x_train, x_test, y_train, y_test = train_test_split(dt_x, \n","                                                    dt_y, \n","                                                    test_size=0.3, \n","                                                    stratify=dt_y, \n","                                                    random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"26tjAMRcGq2-","colab_type":"text"},"source":["## training"]},{"cell_type":"code","metadata":{"id":"S8fKVv6btCYM","colab_type":"code","colab":{}},"source":["log_filepath='/content/'\n","es_cb = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=2, verbose=1, mode='auto')\n","# tb_cb = tf.keras.callbacks.TensorBoard(log_dir=log_filepath, histogram_freq=1)\n","fpath ='/content/mount/My Drive/00_work/model/'+\\\n"," 'weights.{val_accuracy:.6f}-.{accuracy:.6f}-{epoch:02d}-{loss:.2f}-{val_loss:.2f}.hdf5'\n","cp_cb = tf.keras.callbacks.ModelCheckpoint(filepath=fpath, \n","                                        monitor='val_accuracy',\n","                                        # monitor='accuracy',\n","                                         mode='max',\n","                                        verbose=1,\n","                                        save_best_only=True,\n","                                        save_weights_only=False)\n","'''\n","save_weights_only: Trueなら，モデルの重みが保存されます (model.save_weights(filepath))，そうでないなら，モデルの全体が保存されます (model.save(filepath))．\n","'''\n","# fnmodel='/content/mount/My Drive/00_work/model/weights.0.981311-.0.985870-04-0.04-0.05.hdf5'\n","# fnmodel='/content/mount/My Drive/00_work/model/23path_to_my_model.h5'\n","# model = tf.keras.models.load_model()\n","# model = tf.compat.v1.keras.experimental.load_from_saved_model(fnmodel, custom_objects={'KerasLayer':hub.KerasLayer})\n","fnddd='/content/mount/My Drive/00_work/model/weights.0.992992-.0.998665-10-0.01-0.06.hdf5'\n","model = tf.keras.models.load_model(fnddd,custom_objects={'KerasLayer':hub.KerasLayer})\n","# fnmodel='/content/mount/My Drive/00_work/model/33path_to_my_model.h5'\n","# model=tf.saved_model.load(fnmodel)\n","model.compile(optimizer='rmsprop', \n","              loss='binary_crossentropy', \n","              metrics=['accuracy'])\n","model.fit(x_train, \n","          y_train, \n","          # batch_size=10,\n","          # verbose=0, \n","          epochs=110, \n","          validation_data=(x_test, y_test),\n","          callbacks=[cp_cb]\n","          )\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wNycrXWVuS9n","colab_type":"text"},"source":["## predict"]},{"cell_type":"code","metadata":{"id":"sj-a33J_uW0R","colab_type":"code","colab":{}},"source":["import tensorflow_hub as hub\n","import tensorflow_text\n","import numpy as np\n","import pandas as pd\n","import json\n","from  datetime import datetime as dt\n","import tensorflow as tf\n","fn='weights.0.983517-.0.993046-05-0.02-0.06.hdf5'\n","fnddd='/content/mount/My Drive/00_work/resmodel/'+fn\n","model = tf.keras.models.load_model(fnddd,custom_objects={'KerasLayer':hub.KerasLayer})\n","\n","# # col='all'\n","# col='synonyms'\n","def get_remark_dt(col):\n","# col='all'\n","  basedir='/content/mount/My Drive/00_work/guidance/20200701/05_all/'\n","# col='synonyms'\n","  basedir='/content/mount/My Drive/00_work/guidance/20200701/04_word/'\n","\n","  fn=\"/word_\"+col+\".json\"\n","  json_remark = json.load(open(basedir+\"sentence.json\", \"r\"))\n","  json_title = json.load(open(basedir+fn, \"r\"))\n","  return json_remark,json_title\n","def get_remark_dt2(fn_in):\n","  basedir='/content/mount/My Drive/00_work/guidance/20200701/05_all/'\n","\n","  # col='synonyms'\n","  # basedir='/content/mount/My Drive/00_work/guidance/20200701/04_word/'\n","\n","  fn_remark='/content/mount/My Drive/00_work/guidance/remak/'+fn_in\n","  json_remark = json.load(open(fn_remark, \"r\"))\n","  fn=\"/word_\"+col+\".json\"\n","  json_title = json.load(open(basedir+fn, \"r\"))\n","  return json_remark,json_title\n","col='all'\n","# col='synonyms'\n","\n","fn='notification_case_sentence'\n","# fn='human_power_extraction_sentence'\n","json_remark,json_title=get_remark_dt2(fn+'.json')\n","# json_remark,json_title=get_remark_dt(col)\n","\n","results = []\n","print('remark vec start',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","\n","for i,remark in enumerate(json_remark):\n","    rg_list=[]\n","    r_doc=remark['sentence']\n","    print(i,r_doc)\n","    for j, g_doc in enumerate(json_title):\n","      rg_list.append([r_doc,g_doc[col]])\n","    sims=model.predict(np.array(rg_list))\n","    clssims=[]\n","    for sim in sims:\n","     clssims.append(np.argmax(sim))\n","    sim_rg_list=[]\n","    for doc_id, sim in enumerate(clssims): \n","      sim_rg_list.append([doc_id,sim])\n","    sim_rg_list_sort=sorted(sim_rg_list,key=lambda x:x[1], reverse=True)\n","    for sim_rg in sim_rg_list_sort[0:3]:\n","      doc_id=sim_rg[0]\n","      sim_mean=sim_rg[1]\n","      gs_w=json_title[doc_id]['word']\n","      gs_s=json_title[doc_id][col]\n","      results.append([r_doc,gs_w,gs_s,sim_mean])\n","df_out=pd.DataFrame(results,columns=['sentence','word',col,'similarity'])\n","outdir='/content/mount/My Drive/00_work/guidance/output/00_nozawa/'\n","df_out.to_csv(outdir+col+fn+\"_resnet_deep_modified_sentence_word_similarity.csv\")\n","print('finish out',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q_LHK3mGv759","colab_type":"text"},"source":["## test"]},{"cell_type":"code","metadata":{"id":"xUvJIzHCvKyM","colab_type":"code","colab":{}},"source":["import tensorflow_hub as hub\n","import tensorflow_text\n","import numpy as np\n","import pandas as pd\n","import json\n","from  datetime import datetime as dt\n","import tensorflow as tf\n","fn='weights.0.983517-.0.993046-05-0.02-0.06.hdf5'\n","fnddd='/content/mount/My Drive/00_work/resmodel/'+fn\n","model = tf.keras.models.load_model(fnddd,custom_objects={'KerasLayer':hub.KerasLayer})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r8Opsc-txKxS","colab_type":"code","colab":{}},"source":["sims=model.predict(np.asarray(dt_x[0:200]))\n","\n","clssims=[]\n","for sim in sims:\n","     clssims.append(np.argmax(sim))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MuboGIL7WwZ6","colab_type":"code","colab":{}},"source":["#save h5\n","fnddd='/content/mount/My Drive/00_work/model/weights.0.992862-.0.998721-13-0.01-0.05.hdf5'\n","fnddd='/content/mount/My Drive/00_work/model/weights.0.992862-.0.998721-13-0.01-0.05.hdf5'\n","# model.save(fnddd)\n","new_model = tf.keras.models.load_model(fnddd,custom_objects={'KerasLayer':hub.KerasLayer})\n","new_model.summary()\n","\n","# fnddd='/content/mount/My Drive/00_work/model/33path_to_my_model.h5'\n","# # tf.compat.v1.keras.experimental.export_saved_model(model, fnddd) tf1.o\n","# # tf.keras.models.save_model(model, fnddd,save_format=\"tf\")\n","# # tf.saved_model.save(model, fnddd)\n","# model_l=tf.saved_model.load(fnddd)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y3SeDGIpW8oG","colab_type":"code","colab":{}},"source":["# tf.keras.models.load_model(fnddd)\n","reloaded_model = tf.compat.v1.keras.experimental.load_from_saved_model(fnddd, custom_objects={'KerasLayer':hub.KerasLayer})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P7tpZwRKB83Y","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"bK88ANfPtxTK","colab_type":"code","colab":{}},"source":["model.predict([\"21 Pictures That Will Make You Feel Like You're 99 Years Old\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XsVsajEKt0XV","colab_type":"code","colab":{}},"source":["model.predict(['Google announces TensorFlow 2.0'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ysLZPGQorO6U","colab_type":"text"},"source":["# use qa"]},{"cell_type":"markdown","metadata":{"id":"U4ImSqu1ofMX","colab_type":"text"},"source":["## samle"]},{"cell_type":"code","metadata":{"id":"UFHEWC4Drjr4","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","import numpy as np\n","import tensorflow_text\n","module = hub.load('https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PTpM-Emmr3We","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"ab7c0575-07cb-4e7b-a554-cf1fcfd4d126"},"source":["questions = [\"What is your age?\"]\n","responses = [\"I am 20 years old.\", \"good morning\"]\n","response_contexts = [\"I will be 21 next year.\", \"great day.\"]\n","\n","questions = [\"顔が腫れ上がり、額にケロイドが出来たのに…\"]\n","responses = [\"ネグレクトの可能性-被れ汗疹\",\"受傷部位、受傷頻度※受傷理由入院加療が必要な外傷の有無-水疱\",\"受傷部位、受傷頻度※受傷理由頭部へのあざの有無※入院加療が必要な外傷の有無\"]\n","response_contexts = ['湿疹','水ぶくれ','痣']\n","\n","questions = [\"顔が腫れ上がり、額にケロイドが出来たのに…\",\"この間なんか頭を窓にぶつけられ、腕につめをたてられて、血がでました\"]\n","responses = ['受傷部位、受傷頻度※受傷理由入院加療が必要な外傷の有無-出血',\n","'受傷部位、受傷頻度※受傷理由頭部へのあざの有無※入院加療が必要な外傷の有無',\n","'・腹部や太腿内側などの柔らかい組織にある傷は虐待が疑われる。・首に内出血がある場合は首を絞められた可能性を疑う。線上の出血などはその可能性が高い。-'\n","]\n","response_contexts = ['血',\n","'痣',\n","'内出血',]\n","question_embeddings = module.signatures['question_encoder'](\n","            tf.constant(questions))\n","response_embeddings = module.signatures['response_encoder'](\n","        input=tf.constant(responses),\n","        context=tf.constant(response_contexts))\n","\n","np.inner(question_embeddings['outputs'], response_embeddings['outputs'])\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.1929994 , 0.16828038, 0.11203844],\n","       [0.34298226, 0.32539332, 0.35639504]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"WBwsniQKePjJ","colab_type":"text"},"source":["##qa sim1"]},{"cell_type":"code","metadata":{"id":"KjeHdX1reRCd","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DajYcy_Fce-b","colab_type":"text"},"source":["## qa sim2"]},{"cell_type":"code","metadata":{"id":"m6_fWP2tcdSE","colab_type":"code","colab":{}},"source":["\n","def embed_qa_sim(q,r,rc):  \n","  question_embeddings = module.signatures['question_encoder'](\n","              tf.constant(q))\n","  response_embeddings = module.signatures['question_encoder'](\n","              tf.constant(r))\n","\n","  res=cosine_similarity(question_embeddings['outputs'], response_embeddings['outputs'])\n","  return res"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IshOE5YlcwiM","colab_type":"text"},"source":["## qa sim3"]},{"cell_type":"code","metadata":{"id":"r_wn5Tlscyvu","colab_type":"code","colab":{}},"source":["def embed_qa_sim(q,r,rc):  \n","  cnt=len(q)\n","  question_embeddings = module.signatures['response_encoder'](\n","          input=tf.constant(q),\n","          context=tf.constant(rc[0:cnt]))\n","  response_embeddings = module.signatures['response_encoder'](\n","          input=tf.constant(r),\n","          context=tf.constant(rc))\n","\n","  res=cosine_similarity(question_embeddings['outputs'], response_embeddings['outputs'])\n","  return res"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XoUUxNsisy0M","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a7489e5a-faa8-49a1-d48c-90d118a88a14"},"source":["array([[0.3073252 , 0.24394946, 0.19755541]], dtype=float32)\n","array([[0.29280186, 0.2029297 , 0.16828036]], dtype=float32)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([2, 512])"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"kaK9p-p6IH_d","colab_type":"text"},"source":["#tem computer"]},{"cell_type":"code","metadata":{"id":"td7Vu0P7IKzN","colab_type":"code","colab":{}},"source":["guidance=[\n","'身体的虐待。虐待、あざ、火傷、叩く、蹴る、殴る、拘束。身体的虐待にあたる行為。身体的な傷の跡はないか。打撲傷、あざ（内出血）、骨折、頭蓋内出血などの頭部外傷、内臓損傷、刺傷、たばこな。どによる火傷などの外傷を生じるような行為。首を絞める、殴る、蹴る、叩く、投げ落とす、激しく揺さぶる、熱湯をかける、布団蒸し。にする、おぼれさせる、逆さ吊りにする、異物を飲ませる、（体罰の意図を持ち）食事を。抜く、戸外に締め出す、縄などにより一室に拘束するなどの行為。意図的に子どもを病気にさせる。など',\n","'身体的虐待。虐待、あざ、火傷、叩く、蹴る、殴る、拘束。身体的虐待にあたる行為。身体的な傷の跡はないか。打撲傷、あざ（内出血）、骨折、頭蓋内出血などの頭部外傷、内臓損傷、刺傷、たばこな。どによる火傷などの外傷を生じるような行為。首を絞める、殴る、蹴る、叩く、投げ落とす、激しく揺さぶる、熱湯をかける、布団蒸し。にする、おぼれさせる、逆さ吊りにする、異物を飲ませる、（体罰の意図を持ち）食事を。抜く、戸外に締め出す、縄などにより一室に拘束するなどの行為。意図的に子どもを病気にさせる。など',\n","'身体的虐待。虐待、あざ、火傷、叩く、蹴る、殴る、拘束。身体的虐待にあたる行為。身体的な傷の跡はないか。打撲傷、あざ（内出血）、骨折、頭蓋内出血などの頭部外傷、内臓損傷、刺傷、たばこな。どによる火傷などの外傷を生じるような行為。首を絞める、殴る、蹴る、叩く、投げ落とす、激しく揺さぶる、熱湯をかける、布団蒸し。にする、おぼれさせる、逆さ吊りにする、異物を飲ませる、（体罰の意図を持ち）食事を。抜く、戸外に締め出す、縄などにより一室に拘束するなどの行為。意図的に子どもを病気にさせる。など',\n","'身体的虐待。虐待、あざ、火傷、叩く、蹴る、殴る、拘束。身体的虐待にあたる行為。身体的な傷の跡はないか。打撲傷、あざ（内出血）、骨折、頭蓋内出血などの頭部外傷、内臓損傷、刺傷、たばこな。どによる火傷などの外傷を生じるような行為。首を絞める、殴る、蹴る、叩く、投げ落とす、激しく揺さぶる、熱湯をかける、布団蒸し。にする、おぼれさせる、逆さ吊りにする、異物を飲ませる、（体罰の意図を持ち）食事を。抜く、戸外に締め出す、縄などにより一室に拘束するなどの行為。意図的に子どもを病気にさせる。など',\n","'身体的虐待。虐待、あざ、火傷、叩く、蹴る、殴る、拘束。身体的虐待にあたる行為。身体的な傷の跡はないか。打撲傷、あざ（内出血）、骨折、頭蓋内出血などの頭部外傷、内臓損傷、刺傷、たばこな。どによる火傷などの外傷を生じるような行為。首を絞める、殴る、蹴る、叩く、投げ落とす、激しく揺さぶる、熱湯をかける、布団蒸し。にする、おぼれさせる、逆さ吊りにする、異物を飲ませる、（体罰の意図を持ち）食事を。抜く、戸外に締め出す、縄などにより一室に拘束するなどの行為。意図的に子どもを病気にさせる。など',\n","'身体的虐待。虐待、あざ、火傷、叩く、蹴る、殴る、拘束。身体的虐待にあたる行為。身体的な傷の跡はないか。打撲傷、あざ（内出血）、骨折、頭蓋内出血などの頭部外傷、内臓損傷、刺傷、たばこな。どによる火傷などの外傷を生じるような行為。首を絞める、殴る、蹴る、叩く、投げ落とす、激しく揺さぶる、熱湯をかける、布団蒸し。にする、おぼれさせる、逆さ吊りにする、異物を飲ませる、（体罰の意図を持ち）食事を。抜く、戸外に締め出す、縄などにより一室に拘束するなどの行為。意図的に子どもを病気にさせる。など',\n","'ネグレクト。虐待、置き去り。ネグレストにあたる行為。子どもに対する責任を放棄していないか。子どもの健康・安全への配慮を怠っているなど。例えば、「重大な病気になっても病院に連れて行かない」、「乳幼児を家に残したまま外出。する」など。なお、親がパチンコに熱中している間、乳幼児を自動車の中に放置し、熱中症で子ども。が死亡したり、誘拐されたり、乳幼児だけを家に残して火災で子どもが焼死したりする。事件も、ネグレクトという虐待の結果であることに留意する。子どもの意思に反して学校等に登校させない。子どもが学校等に登校するように促すなど。の子どもの教育を保障する努力をしない。子どもにとって必要な情緒的欲求に応えていない（愛情遮断など）。食事、衣服、住居などが極端に不適切で、健康状態を損なうほどの無関心・怠慢。例えば。「適切な食事を与えない」。「下着など長期間ひどく不潔なままにする」。「極端に。不潔な環境の中で生活をさせる」など。子どもを遺棄したり、置き去りにする。祖父母、きょうだい、保護者の恋人などの同居人や自宅に出入りする第三者が身体的、性的、心理的な虐待にあたる行為を行っているにもかかわらず、それを放置する。など',\n","'ネグレクト。虐待、置き去り。ネグレストにあたる行為。子どもに対する責任を放棄していないか。子どもの健康・安全への配慮を怠っているなど。例えば、「重大な病気になっても病院に連れて行かない」、「乳幼児を家に残したまま外出。する」など。なお、親がパチンコに熱中している間、乳幼児を自動車の中に放置し、熱中症で子ども。が死亡したり、誘拐されたり、乳幼児だけを家に残して火災で子どもが焼死したりする。事件も、ネグレクトという虐待の結果であることに留意する。子どもの意思に反して学校等に登校させない。子どもが学校等に登校するように促すなど。の子どもの教育を保障する努力をしない。子どもにとって必要な情緒的欲求に応えていない（愛情遮断など）。食事、衣服、住居などが極端に不適切で、健康状態を損なうほどの無関心・怠慢。例えば。「適切な食事を与えない」。「下着など長期間ひどく不潔なままにする」。「極端に。不潔な環境の中で生活をさせる」など。子どもを遺棄したり、置き去りにする。祖父母、きょうだい、保護者の恋人などの同居人や自宅に出入りする第三者が身体的、性的、心理的な虐待にあたる行為を行っているにもかかわらず、それを放置する。など',\n","'ネグレクト。虐待、置き去り。ネグレストにあたる行為。子どもに対する責任を放棄していないか。子どもの健康・安全への配慮を怠っているなど。例えば、「重大な病気になっても病院に連れて行かない」、「乳幼児を家に残したまま外出。する」など。なお、親がパチンコに熱中している間、乳幼児を自動車の中に放置し、熱中症で子ども。が死亡したり、誘拐されたり、乳幼児だけを家に残して火災で子どもが焼死したりする。事件も、ネグレクトという虐待の結果であることに留意する。子どもの意思に反して学校等に登校させない。子どもが学校等に登校するように促すなど。の子どもの教育を保障する努力をしない。子どもにとって必要な情緒的欲求に応えていない（愛情遮断など）。食事、衣服、住居などが極端に不適切で、健康状態を損なうほどの無関心・怠慢。例えば。「適切な食事を与えない」。「下着など長期間ひどく不潔なままにする」。「極端に。不潔な環境の中で生活をさせる」など。子どもを遺棄したり、置き去りにする。祖父母、きょうだい、保護者の恋人などの同居人や自宅に出入りする第三者が身体的、性的、心理的な虐待にあたる行為を行っているにもかかわらず、それを放置する。など',\n","'ネグレクト。虐待、置き去り。ネグレストにあたる行為。子どもに対する責任を放棄していないか。子どもの健康・安全への配慮を怠っているなど。例えば、「重大な病気になっても病院に連れて行かない」、「乳幼児を家に残したまま外出。する」など。なお、親がパチンコに熱中している間、乳幼児を自動車の中に放置し、熱中症で子ども。が死亡したり、誘拐されたり、乳幼児だけを家に残して火災で子どもが焼死したりする。事件も、ネグレクトという虐待の結果であることに留意する。子どもの意思に反して学校等に登校させない。子どもが学校等に登校するように促すなど。の子どもの教育を保障する努力をしない。子どもにとって必要な情緒的欲求に応えていない（愛情遮断など）。食事、衣服、住居などが極端に不適切で、健康状態を損なうほどの無関心・怠慢。例えば。「適切な食事を与えない」。「下着など長期間ひどく不潔なままにする」。「極端に。不潔な環境の中で生活をさせる」など。子どもを遺棄したり、置き去りにする。祖父母、きょうだい、保護者の恋人などの同居人や自宅に出入りする第三者が身体的、性的、心理的な虐待にあたる行為を行っているにもかかわらず、それを放置する。など',\n","'ネグレクト。虐待、置き去り。ネグレストにあたる行為。子どもに対する責任を放棄していないか。子どもの健康・安全への配慮を怠っているなど。例えば、「重大な病気になっても病院に連れて行かない」、「乳幼児を家に残したまま外出。する」など。なお、親がパチンコに熱中している間、乳幼児を自動車の中に放置し、熱中症で子ども。が死亡したり、誘拐されたり、乳幼児だけを家に残して火災で子どもが焼死したりする。事件も、ネグレクトという虐待の結果であることに留意する。子どもの意思に反して学校等に登校させない。子どもが学校等に登校するように促すなど。の子どもの教育を保障する努力をしない。子どもにとって必要な情緒的欲求に応えていない（愛情遮断など）。食事、衣服、住居などが極端に不適切で、健康状態を損なうほどの無関心・怠慢。例えば。「適切な食事を与えない」。「下着など長期間ひどく不潔なままにする」。「極端に。不潔な環境の中で生活をさせる」など。子どもを遺棄したり、置き去りにする。祖父母、きょうだい、保護者の恋人などの同居人や自宅に出入りする第三者が身体的、性的、心理的な虐待にあたる行為を行っているにもかかわらず、それを放置する。など',\n","'ネグレクト。虐待、置き去り。ネグレストにあたる行為。子どもに対する責任を放棄していないか。子どもの健康・安全への配慮を怠っているなど。例えば、「重大な病気になっても病院に連れて行かない」、「乳幼児を家に残したまま外出。する」など。なお、親がパチンコに熱中している間、乳幼児を自動車の中に放置し、熱中症で子ども。が死亡したり、誘拐されたり、乳幼児だけを家に残して火災で子どもが焼死したりする。事件も、ネグレクトという虐待の結果であることに留意する。子どもの意思に反して学校等に登校させない。子どもが学校等に登校するように促すなど。の子どもの教育を保障する努力をしない。子どもにとって必要な情緒的欲求に応えていない（愛情遮断など）。食事、衣服、住居などが極端に不適切で、健康状態を損なうほどの無関心・怠慢。例えば。「適切な食事を与えない」。「下着など長期間ひどく不潔なままにする」。「極端に。不潔な環境の中で生活をさせる」など。子どもを遺棄したり、置き去りにする。祖父母、きょうだい、保護者の恋人などの同居人や自宅に出入りする第三者が身体的、性的、心理的な虐待にあたる行為を行っているにもかかわらず、それを放置する。など',\n","'性的虐待。虐待、性的、触らせる。性的虐待にあたる行為。子どもを性的な対象としていないか。子どもへの性交、性的行為（教唆を含む）。子どもの性器を触る又は子どもに触らせるなどの性的行為（教唆を含む）。子どもに性器や性交を見せる。子どもをポルノグラフィーの被写体などにする。など。',\n","'性的虐待。虐待、性的、触らせる。性的虐待にあたる行為。子どもを性的な対象としていないか。子どもへの性交、性的行為（教唆を含む）。子どもの性器を触る又は子どもに触らせるなどの性的行為（教唆を含む）。子どもに性器や性交を見せる。子どもをポルノグラフィーの被写体などにする。など。',\n","'性的虐待。虐待、性的、触らせる。性的虐待にあたる行為。子どもを性的な対象としていないか。子どもへの性交、性的行為（教唆を含む）。子どもの性器を触る又は子どもに触らせるなどの性的行為（教唆を含む）。子どもに性器や性交を見せる。子どもをポルノグラフィーの被写体などにする。など。',\n","'性的虐待。虐待、性的、触らせる。性的虐待にあたる行為。子どもを性的な対象としていないか。子どもへの性交、性的行為（教唆を含む）。子どもの性器を触る又は子どもに触らせるなどの性的行為（教唆を含む）。子どもに性器や性交を見せる。子どもをポルノグラフィーの被写体などにする。など。',\n","'心理的虐待。虐待、脅かす、脅迫、差別、無視。心理的虐待にあたる行為。子どもを脅かしたり、無視したり、差別的な扱いをしていないか。言葉による脅かし、脅迫など。子どもを無視したり、拒否的な態度を示すことなど。子どもの心を傷付けることを繰り返し言う。子どもの自尊心を傷付けるような言動など。他のきょうだいと著しく差別的な扱いをする。配偶者やその他の家族などに対し暴力をふるう。子どものきょうだいに、身体的、性的、心理的な虐待、ネグレストにあたる行為を行う。など。',\n","'心理的虐待。虐待、脅かす、脅迫、差別、無視。心理的虐待にあたる行為。子どもを脅かしたり、無視したり、差別的な扱いをしていないか。言葉による脅かし、脅迫など。子どもを無視したり、拒否的な態度を示すことなど。子どもの心を傷付けることを繰り返し言う。子どもの自尊心を傷付けるような言動など。他のきょうだいと著しく差別的な扱いをする。配偶者やその他の家族などに対し暴力をふるう。子どものきょうだいに、身体的、性的、心理的な虐待、ネグレストにあたる行為を行う。など。',\n","'心理的虐待。虐待、脅かす、脅迫、差別、無視。心理的虐待にあたる行為。子どもを脅かしたり、無視したり、差別的な扱いをしていないか。言葉による脅かし、脅迫など。子どもを無視したり、拒否的な態度を示すことなど。子どもの心を傷付けることを繰り返し言う。子どもの自尊心を傷付けるような言動など。他のきょうだいと著しく差別的な扱いをする。配偶者やその他の家族などに対し暴力をふるう。子どものきょうだいに、身体的、性的、心理的な虐待、ネグレストにあたる行為を行う。など。',\n","'心理的虐待。虐待、脅かす、脅迫、差別、無視。心理的虐待にあたる行為。子どもを脅かしたり、無視したり、差別的な扱いをしていないか。言葉による脅かし、脅迫など。子どもを無視したり、拒否的な態度を示すことなど。子どもの心を傷付けることを繰り返し言う。子どもの自尊心を傷付けるような言動など。他のきょうだいと著しく差別的な扱いをする。配偶者やその他の家族などに対し暴力をふるう。子どものきょうだいに、身体的、性的、心理的な虐待、ネグレストにあたる行為を行う。など。',\n","'心理的虐待。虐待、脅かす、脅迫、差別、無視。心理的虐待にあたる行為。子どもを脅かしたり、無視したり、差別的な扱いをしていないか。言葉による脅かし、脅迫など。子どもを無視したり、拒否的な態度を示すことなど。子どもの心を傷付けることを繰り返し言う。子どもの自尊心を傷付けるような言動など。他のきょうだいと著しく差別的な扱いをする。配偶者やその他の家族などに対し暴力をふるう。子どものきょうだいに、身体的、性的、心理的な虐待、ネグレストにあたる行為を行う。など。',\n","'心理的虐待。虐待、脅かす、脅迫、差別、無視。心理的虐待にあたる行為。子どもを脅かしたり、無視したり、差別的な扱いをしていないか。言葉による脅かし、脅迫など。子どもを無視したり、拒否的な態度を示すことなど。子どもの心を傷付けることを繰り返し言う。子どもの自尊心を傷付けるような言動など。他のきょうだいと著しく差別的な扱いをする。配偶者やその他の家族などに対し暴力をふるう。子どものきょうだいに、身体的、性的、心理的な虐待、ネグレストにあたる行為を行う。など。',\n","'心理的虐待。虐待、脅かす、脅迫、差別、無視。心理的虐待にあたる行為。子どもを脅かしたり、無視したり、差別的な扱いをしていないか。言葉による脅かし、脅迫など。子どもを無視したり、拒否的な態度を示すことなど。子どもの心を傷付けることを繰り返し言う。子どもの自尊心を傷付けるような言動など。他のきょうだいと著しく差別的な扱いをする。配偶者やその他の家族などに対し暴力をふるう。子どものきょうだいに、身体的、性的、心理的な虐待、ネグレストにあたる行為を行う。など。',\n","'心理的虐待。虐待、脅かす、脅迫、差別、無視。心理的虐待にあたる行為。子どもを脅かしたり、無視したり、差別的な扱いをしていないか。言葉による脅かし、脅迫など。子どもを無視したり、拒否的な態度を示すことなど。子どもの心を傷付けることを繰り返し言う。子どもの自尊心を傷付けるような言動など。他のきょうだいと著しく差別的な扱いをする。配偶者やその他の家族などに対し暴力をふるう。子どものきょうだいに、身体的、性的、心理的な虐待、ネグレストにあたる行為を行う。など。',\n","'心理的虐待。虐待、脅かす、脅迫、差別、無視。心理的虐待にあたる行為。子どもを脅かしたり、無視したり、差別的な扱いをしていないか。言葉による脅かし、脅迫など。子どもを無視したり、拒否的な態度を示すことなど。子どもの心を傷付けることを繰り返し言う。子どもの自尊心を傷付けるような言動など。他のきょうだいと著しく差別的な扱いをする。配偶者やその他の家族などに対し暴力をふるう。子どものきょうだいに、身体的、性的、心理的な虐待、ネグレストにあたる行為を行う。など。',\n","'身体的影響。虐待のよる子供への身体的影響。子どもの発育状態。打撲、熱傷など外から見てわかる傷、骨折、頭蓋内出血など外から見えない傷、栄養障害や体重増加不良、低身長。愛情不足により成長ホルモンが抑えられた結果、成長不全を呈することもある。最近の研究では、保護者から暴力や暴言を受けた子どもの脳に委縮が見られた、との知見も明らかになりつつある。',\n","'身体的影響。虐待のよる子供への身体的影響。子どもの発育状態。打撲、熱傷など外から見てわかる傷、骨折、頭蓋内出血など外から見えない傷、栄養障害や体重増加不良、低身長。愛情不足により成長ホルモンが抑えられた結果、成長不全を呈することもある。最近の研究では、保護者から暴力や暴言を受けた子どもの脳に委縮が見られた、との知見も明らかになりつつある。',\n","'身体的影響。虐待のよる子供への身体的影響。子どもの発育状態。打撲、熱傷など外から見てわかる傷、骨折、頭蓋内出血など外から見えない傷、栄養障害や体重増加不良、低身長。愛情不足により成長ホルモンが抑えられた結果、成長不全を呈することもある。最近の研究では、保護者から暴力や暴言を受けた子どもの脳に委縮が見られた、との知見も明らかになりつつある。',\n","'身体的影響。虐待のよる子供への身体的影響。子どもの発育状態。打撲、熱傷など外から見てわかる傷、骨折、頭蓋内出血など外から見えない傷、栄養障害や体重増加不良、低身長。愛情不足により成長ホルモンが抑えられた結果、成長不全を呈することもある。最近の研究では、保護者から暴力や暴言を受けた子どもの脳に委縮が見られた、との知見も明らかになりつつある。',\n","'身体的影響。虐待のよる子供への身体的影響。子どもの発育状態。打撲、熱傷など外から見てわかる傷、骨折、頭蓋内出血など外から見えない傷、栄養障害や体重増加不良、低身長。愛情不足により成長ホルモンが抑えられた結果、成長不全を呈することもある。最近の研究では、保護者から暴力や暴言を受けた子どもの脳に委縮が見られた、との知見も明らかになりつつある。',\n","'知的発達面への影響。虐待のよる子供への知的発達への影響。子どもの知的発達状態。安心できない環境での生活により、落ち着いて学習に向かうことができない場合や、学校への登校もままならない場合などにより、もともとの能力に比しても発達が十分に得られないことがある。また、保護者が子どもの知的発達にとって必要なやり取りを行わない場合や、年齢や発達レベルにそぐわない過大な要求をする場合があり、その結果として子どもの知的発達を阻害してしまうことがある。',\n","'知的発達面への影響。虐待のよる子供への知的発達への影響。子どもの知的発達状態。安心できない環境での生活により、落ち着いて学習に向かうことができない場合や、学校への登校もままならない場合などにより、もともとの能力に比しても発達が十分に得られないことがある。また、保護者が子どもの知的発達にとって必要なやり取りを行わない場合や、年齢や発達レベルにそぐわない過大な要求をする場合があり、その結果として子どもの知的発達を阻害してしまうことがある。',\n","'知的発達面への影響。虐待のよる子供への知的発達への影響。子どもの知的発達状態。安心できない環境での生活により、落ち着いて学習に向かうことができない場合や、学校への登校もままならない場合などにより、もともとの能力に比しても発達が十分に得られないことがある。また、保護者が子どもの知的発達にとって必要なやり取りを行わない場合や、年齢や発達レベルにそぐわない過大な要求をする場合があり、その結果として子どもの知的発達を阻害してしまうことがある。',\n","'知的発達面への影響。虐待のよる子供への知的発達への影響。子どもの知的発達状態。安心できない環境での生活により、落ち着いて学習に向かうことができない場合や、学校への登校もままならない場合などにより、もともとの能力に比しても発達が十分に得られないことがある。また、保護者が子どもの知的発達にとって必要なやり取りを行わない場合や、年齢や発達レベルにそぐわない過大な要求をする場合があり、その結果として子どもの知的発達を阻害してしまうことがある。',\n","'知的発達面への影響。虐待のよる子供への知的発達への影響。子どもの知的発達状態。安心できない環境での生活により、落ち着いて学習に向かうことができない場合や、学校への登校もままならない場合などにより、もともとの能力に比しても発達が十分に得られないことがある。また、保護者が子どもの知的発達にとって必要なやり取りを行わない場合や、年齢や発達レベルにそぐわない過大な要求をする場合があり、その結果として子どもの知的発達を阻害してしまうことがある。',\n","'対人関係の障害。虐待による子どもの対人関係への影響。子どもの対人関係に問題がないか。子どもにとって最も安心を与えられる存在であるはずの保護者から虐待を受けることにより、子どもは欲求を適切に満たされることのない状態となり、愛着対象との基本的な信頼関係を構築することができなくなる。その結果、他人を信頼し愛着関係を形成することが困難になり、対人関係における問題を生じることがある。',\n","'対人関係の障害。虐待による子どもの対人関係への影響。子どもの対人関係に問題がないか。子どもにとって最も安心を与えられる存在であるはずの保護者から虐待を受けることにより、子どもは欲求を適切に満たされることのない状態となり、愛着対象との基本的な信頼関係を構築することができなくなる。その結果、他人を信頼し愛着関係を形成することが困難になり、対人関係における問題を生じることがある。',\n","'対人関係の障害。虐待による子どもの対人関係への影響。子どもの対人関係に問題がないか。子どもにとって最も安心を与えられる存在であるはずの保護者から虐待を受けることにより、子どもは欲求を適切に満たされることのない状態となり、愛着対象との基本的な信頼関係を構築することができなくなる。その結果、他人を信頼し愛着関係を形成することが困難になり、対人関係における問題を生じることがある。',\n","'対人関係の障害。虐待による子どもの対人関係への影響。子どもの対人関係に問題がないか。子どもにとって最も安心を与えられる存在であるはずの保護者から虐待を受けることにより、子どもは欲求を適切に満たされることのない状態となり、愛着対象との基本的な信頼関係を構築することができなくなる。その結果、他人を信頼し愛着関係を形成することが困難になり、対人関係における問題を生じることがある。',\n","'対人関係の障害。虐待による子どもの対人関係への影響。子どもの対人関係に問題がないか。子どもにとって最も安心を与えられる存在であるはずの保護者から虐待を受けることにより、子どもは欲求を適切に満たされることのない状態となり、愛着対象との基本的な信頼関係を構築することができなくなる。その結果、他人を信頼し愛着関係を形成することが困難になり、対人関係における問題を生じることがある。',\n","'低い自己評価。自己否定。虐待による子どもの自己肯定感の欠如。子どもの自尊心が欠如していないか。子どもは、自分が悪いから虐待されると思うことや、自分は愛情を受けるに値する存在ではないと感じることがあり、自己評価が低下し、自己肯定感を持てない状態となることがある。',\n","'低い自己評価。自己否定。虐待による子どもの自己肯定感の欠如。子どもの自尊心が欠如していないか。子どもは、自分が悪いから虐待されると思うことや、自分は愛情を受けるに値する存在ではないと感じることがあり、自己評価が低下し、自己肯定感を持てない状態となることがある。',\n","'低い自己評価。自己否定。虐待による子どもの自己肯定感の欠如。子どもの自尊心が欠如していないか。子どもは、自分が悪いから虐待されると思うことや、自分は愛情を受けるに値する存在ではないと感じることがあり、自己評価が低下し、自己肯定感を持てない状態となることがある。',\n","'低い自己評価。自己否定。虐待による子どもの自己肯定感の欠如。子どもの自尊心が欠如していないか。子どもは、自分が悪いから虐待されると思うことや、自分は愛情を受けるに値する存在ではないと感じることがあり、自己評価が低下し、自己肯定感を持てない状態となることがある。',\n","'低い自己評価。自己否定。虐待による子どもの自己肯定感の欠如。子どもの自尊心が欠如していないか。子どもは、自分が悪いから虐待されると思うことや、自分は愛情を受けるに値する存在ではないと感じることがあり、自己評価が低下し、自己肯定感を持てない状態となることがある。',\n","'行動コントロールの問題。虐待による子どもの粗暴な言動。子どもが乱暴な言動をしていないか。保護者から暴力を受けた子どもは、暴力で問題を解決することを学習し、学校や地域で粗暴な行動をとるようになり、攻撃的・衝動的な行動をとったり、欲求のままに行動する場合がある。',\n","'行動コントロールの問題。虐待による子どもの粗暴な言動。子どもが乱暴な言動をしていないか。保護者から暴力を受けた子どもは、暴力で問題を解決することを学習し、学校や地域で粗暴な行動をとるようになり、攻撃的・衝動的な行動をとったり、欲求のままに行動する場合がある。',\n","'行動コントロールの問題。虐待による子どもの粗暴な言動。子どもが乱暴な言動をしていないか。保護者から暴力を受けた子どもは、暴力で問題を解決することを学習し、学校や地域で粗暴な行動をとるようになり、攻撃的・衝動的な行動をとったり、欲求のままに行動する場合がある。',\n","'行動コントロールの問題。虐待による子どもの粗暴な言動。子どもが乱暴な言動をしていないか。保護者から暴力を受けた子どもは、暴力で問題を解決することを学習し、学校や地域で粗暴な行動をとるようになり、攻撃的・衝動的な行動をとったり、欲求のままに行動する場合がある。',\n","'多動。虐待による子どもの落ち着きのない言動。子どもの落ち着きのない行動、（ADHDでも同様の兆候が見られるため注意）。虐待を受けて養育されることは、子どもを刺激に対して過敏にさせることがあり、落ち着きのない行動が現れることがある。ADHD。に似た症状を示すため、その鑑別が必要となる場合がある。',\n","'多動。虐待による子どもの落ち着きのない言動。子どもの落ち着きのない行動、（ADHDでも同様の兆候が見られるため注意）。虐待を受けて養育されることは、子どもを刺激に対して過敏にさせることがあり、落ち着きのない行動が現れることがある。ADHD。に似た症状を示すため、その鑑別が必要となる場合がある。',\n","'多動。虐待による子どもの落ち着きのない言動。子どもの落ち着きのない行動、（ADHDでも同様の兆候が見られるため注意）。虐待を受けて養育されることは、子どもを刺激に対して過敏にさせることがあり、落ち着きのない行動が現れることがある。ADHD。に似た症状を示すため、その鑑別が必要となる場合がある。',\n","'多動。虐待による子どもの落ち着きのない言動。子どもの落ち着きのない行動、（ADHDでも同様の兆候が見られるため注意）。虐待を受けて養育されることは、子どもを刺激に対して過敏にさせることがあり、落ち着きのない行動が現れることがある。ADHD。に似た症状を示すため、その鑑別が必要となる場合がある。',\n","'心的外傷後ストレス障害。PTSD。虐待による子どもへの精神的影響。子どもの非行・問題行為。受けた心の傷（トラウマ）は適切な治療を受けないまま放置されると将来にわたって心的外傷後ストレス障害（PTSD）として残り、思春期等に至って問題行動として出現する場合がある。',\n","'心的外傷後ストレス障害。PTSD。虐待による子どもへの精神的影響。子どもの非行・問題行為。受けた心の傷（トラウマ）は適切な治療を受けないまま放置されると将来にわたって心的外傷後ストレス障害（PTSD）として残り、思春期等に至って問題行動として出現する場合がある。',\n","'心的外傷後ストレス障害。PTSD。虐待による子どもへの精神的影響。子どもの非行・問題行為。受けた心の傷（トラウマ）は適切な治療を受けないまま放置されると将来にわたって心的外傷後ストレス障害（PTSD）として残り、思春期等に至って問題行動として出現する場合がある。',\n","'心的外傷後ストレス障害。PTSD。虐待による子どもへの精神的影響。子どもの非行・問題行為。受けた心の傷（トラウマ）は適切な治療を受けないまま放置されると将来にわたって心的外傷後ストレス障害（PTSD）として残り、思春期等に至って問題行動として出現する場合がある。',\n","'心的外傷後ストレス障害。PTSD。虐待による子どもへの精神的影響。子どもの非行・問題行為。受けた心の傷（トラウマ）は適切な治療を受けないまま放置されると将来にわたって心的外傷後ストレス障害（PTSD）として残り、思春期等に至って問題行動として出現する場合がある。',\n","'偽成熟性。虐待による子どもへの精神的影響。子どもが過度に大人の顔色を気にしていないか。大人の顔色を見ながら生活することから、大人の欲求に従って先取りした行動をとることがある。精神的に不安定な保護者に代わり、大人としての役割分担を果たさなければならないこともあり、ある面では大人びた行動をとることがある。一見よくできた子どもに思える一方で、思春期等に問題が表出することもある。',\n","'偽成熟性。虐待による子どもへの精神的影響。子どもが過度に大人の顔色を気にしていないか。大人の顔色を見ながら生活することから、大人の欲求に従って先取りした行動をとることがある。精神的に不安定な保護者に代わり、大人としての役割分担を果たさなければならないこともあり、ある面では大人びた行動をとることがある。一見よくできた子どもに思える一方で、思春期等に問題が表出することもある。',\n","'偽成熟性。虐待による子どもへの精神的影響。子どもが過度に大人の顔色を気にしていないか。大人の顔色を見ながら生活することから、大人の欲求に従って先取りした行動をとることがある。精神的に不安定な保護者に代わり、大人としての役割分担を果たさなければならないこともあり、ある面では大人びた行動をとることがある。一見よくできた子どもに思える一方で、思春期等に問題が表出することもある。',\n","'偽成熟性。虐待による子どもへの精神的影響。子どもが過度に大人の顔色を気にしていないか。大人の顔色を見ながら生活することから、大人の欲求に従って先取りした行動をとることがある。精神的に不安定な保護者に代わり、大人としての役割分担を果たさなければならないこともあり、ある面では大人びた行動をとることがある。一見よくできた子どもに思える一方で、思春期等に問題が表出することもある。',\n","'偽成熟性。虐待による子どもへの精神的影響。子どもが過度に大人の顔色を気にしていないか。大人の顔色を見ながら生活することから、大人の欲求に従って先取りした行動をとることがある。精神的に不安定な保護者に代わり、大人としての役割分担を果たさなければならないこともあり、ある面では大人びた行動をとることがある。一見よくできた子どもに思える一方で、思春期等に問題が表出することもある。',\n","'偽成熟性。虐待による子どもへの精神的影響。子どもが過度に大人の顔色を気にしていないか。大人の顔色を見ながら生活することから、大人の欲求に従って先取りした行動をとることがある。精神的に不安定な保護者に代わり、大人としての役割分担を果たさなければならないこともあり、ある面では大人びた行動をとることがある。一見よくできた子どもに思える一方で、思春期等に問題が表出することもある。',\n","'精神的病状。虐待による子どもへの精神的影響。子どもに記憶障害、意識がもうろうとした状態、解離性同一性障害がみられないか。反復性のトラウマにより、精神的に病的な症状を呈することがある。例えば、記憶障害、意識がもうろうとした状態、さらには強い防衛機制としての解離が出現し、解離性同一性障害に至る場合もある。',\n","'精神的病状。虐待による子どもへの精神的影響。子どもに記憶障害、意識がもうろうとした状態、解離性同一性障害がみられないか。反復性のトラウマにより、精神的に病的な症状を呈することがある。例えば、記憶障害、意識がもうろうとした状態、さらには強い防衛機制としての解離が出現し、解離性同一性障害に至る場合もある。',\n","'精神的病状。虐待による子どもへの精神的影響。子どもに記憶障害、意識がもうろうとした状態、解離性同一性障害がみられないか。反復性のトラウマにより、精神的に病的な症状を呈することがある。例えば、記憶障害、意識がもうろうとした状態、さらには強い防衛機制としての解離が出現し、解離性同一性障害に至る場合もある。',\n","'精神的病状。虐待による子どもへの精神的影響。子どもに記憶障害、意識がもうろうとした状態、解離性同一性障害がみられないか。反復性のトラウマにより、精神的に病的な症状を呈することがある。例えば、記憶障害、意識がもうろうとした状態、さらには強い防衛機制としての解離が出現し、解離性同一性障害に至る場合もある。',\n","'精神的病状。虐待による子どもへの精神的影響。子どもに記憶障害、意識がもうろうとした状態、解離性同一性障害がみられないか。反復性のトラウマにより、精神的に病的な症状を呈することがある。例えば、記憶障害、意識がもうろうとした状態、さらには強い防衛機制としての解離が出現し、解離性同一性障害に至る場合もある。',\n","'硬膜下血腫。虐待による本病の発症。子どもが歩行開始前か。歩行開始前の乳児の硬膜下血腫の９５％は虐待と言われている',\n","'硬膜下血腫。虐待による本病の発症。子どもが歩行開始前か。歩行開始前の乳児の硬膜下血腫の９５％は虐待と言われている',\n","'硬膜下血腫。虐待による本病の発症。子どもが歩行開始前か。歩行開始前の乳児の硬膜下血腫の９５％は虐待と言われている',\n","'硬膜下血腫。虐待による本病の発症。子どもが歩行開始前か。歩行開始前の乳児の硬膜下血腫の９５％は虐待と言われている',\n","'硬膜下血腫。虐待による本病の発症。子どもが歩行開始前か。歩行開始前の乳児の硬膜下血腫の９５％は虐待と言われている',\n","'硬膜下血腫。虐待による本病の発症。子どもが歩行開始前か。歩行開始前の乳児の硬膜下血腫の９５％は虐待と言われている',\n","'硬膜下血腫。虐待による本病の発症。子どもが歩行開始前か。歩行開始前の乳児の硬膜下血腫の９５％は虐待と言われている',\n","'硬膜下血腫。虐待による本病の発症。子どもが歩行開始前か。歩行開始前の乳児の硬膜下血腫の９５％は虐待と言われている',\n","'硬膜下血腫。虐待による本病の発症。子どもが歩行開始前か。歩行開始前の乳児の硬膜下血腫の９５％は虐待と言われている',\n","'乳幼児揺さぶられ症候群。揺さぶられっ子症候群、SBS。虐待による本病の発症。子どもに対する激しく揺さぶる行為。子どもの首が激しく揺さぶられることで頭蓋内出血（硬膜下血腫が多い）や脳の断裂を起こすことがある。発症は乳児が中心だが、それ以上の年齢でも起こりうる。眼底の出血を伴うことが多いので、眼科的診察が必要となる。',\n","'乳幼児揺さぶられ症候群。揺さぶられっ子症候群、SBS。虐待による本病の発症。子どもに対する激しく揺さぶる行為。子どもの首が激しく揺さぶられることで頭蓋内出血（硬膜下血腫が多い）や脳の断裂を起こすことがある。発症は乳児が中心だが、それ以上の年齢でも起こりうる。眼底の出血を伴うことが多いので、眼科的診察が必要となる。',\n","'乳幼児揺さぶられ症候群。揺さぶられっ子症候群、SBS。虐待による本病の発症。子どもに対する激しく揺さぶる行為。子どもの首が激しく揺さぶられることで頭蓋内出血（硬膜下血腫が多い）や脳の断裂を起こすことがある。発症は乳児が中心だが、それ以上の年齢でも起こりうる。眼底の出血を伴うことが多いので、眼科的診察が必要となる。',\n","'乳幼児揺さぶられ症候群。揺さぶられっ子症候群、SBS。虐待による本病の発症。子どもに対する激しく揺さぶる行為。子どもの首が激しく揺さぶられることで頭蓋内出血（硬膜下血腫が多い）や脳の断裂を起こすことがある。発症は乳児が中心だが、それ以上の年齢でも起こりうる。眼底の出血を伴うことが多いので、眼科的診察が必要となる。',\n","'乳幼児揺さぶられ症候群。揺さぶられっ子症候群、SBS。虐待による本病の発症。子どもに対する激しく揺さぶる行為。子どもの首が激しく揺さぶられることで頭蓋内出血（硬膜下血腫が多い）や脳の断裂を起こすことがある。発症は乳児が中心だが、それ以上の年齢でも起こりうる。眼底の出血を伴うことが多いので、眼科的診察が必要となる。',\n","'乳幼児揺さぶられ症候群。揺さぶられっ子症候群、SBS。虐待による本病の発症。子どもに対する激しく揺さぶる行為。子どもの首が激しく揺さぶられることで頭蓋内出血（硬膜下血腫が多い）や脳の断裂を起こすことがある。発症は乳児が中心だが、それ以上の年齢でも起こりうる。眼底の出血を伴うことが多いので、眼科的診察が必要となる。',\n","'乳幼児揺さぶられ症候群。揺さぶられっ子症候群、SBS。虐待による本病の発症。子どもに対する激しく揺さぶる行為。子どもの首が激しく揺さぶられることで頭蓋内出血（硬膜下血腫が多い）や脳の断裂を起こすことがある。発症は乳児が中心だが、それ以上の年齢でも起こりうる。眼底の出血を伴うことが多いので、眼科的診察が必要となる。',\n","'代理によるミュンヒハウゼン症候群。代理によるミュンヒハウゼン症候群、ミュンヒハウゼン、MSBP周囲の関心や同情を引くために子どもを故意に病人に仕立て上げる。保護者の報告との乖離、不自然な検査所見の組合せや推移、一般の医学では考えにくい症状、子どもと離れない不自然な保護者の態度が見られないか。MSBPとは保護者が巧妙な虚偽や症状の捏造によって、子どもに病的な状態を持続的に造り出す、子ども虐待の一形態である。MSBPは致死率の高い虐待の形であり、MSBPの保護者は実母が非常に多い。MSBPは、比較的良い全身状態にもかかわらず重篤な検査所見がある、保護者の報告との乖離、不自然な検査所見の組合せや推移、一般の医学では考えにくい症状、子どもと離れない不自然な保護者の態度などから疑われるが、医療者が疑いを持つまでに長期間を要することも少なくなく、確定するのは困難を伴う。子どもを守ることができて、確証を得られるのは、保護者と子どもを分離して、症状の消失を確かめることによるが、分離には保護者が抵抗することが多く、一時保護が必要になることが多い。',\n","'代理によるミュンヒハウゼン症候群。代理によるミュンヒハウゼン症候群、ミュンヒハウゼン、MSBP周囲の関心や同情を引くために子どもを故意に病人に仕立て上げる。保護者の報告との乖離、不自然な検査所見の組合せや推移、一般の医学では考えにくい症状、子どもと離れない不自然な保護者の態度が見られないか。MSBPとは保護者が巧妙な虚偽や症状の捏造によって、子どもに病的な状態を持続的に造り出す、子ども虐待の一形態である。MSBPは致死率の高い虐待の形であり、MSBPの保護者は実母が非常に多い。MSBPは、比較的良い全身状態にもかかわらず重篤な検査所見がある、保護者の報告との乖離、不自然な検査所見の組合せや推移、一般の医学では考えにくい症状、子どもと離れない不自然な保護者の態度などから疑われるが、医療者が疑いを持つまでに長期間を要することも少なくなく、確定するのは困難を伴う。子どもを守ることができて、確証を得られるのは、保護者と子どもを分離して、症状の消失を確かめることによるが、分離には保護者が抵抗することが多く、一時保護が必要になることが多い。',\n","'代理によるミュンヒハウゼン症候群。代理によるミュンヒハウゼン症候群、ミュンヒハウゼン、MSBP周囲の関心や同情を引くために子どもを故意に病人に仕立て上げる。保護者の報告との乖離、不自然な検査所見の組合せや推移、一般の医学では考えにくい症状、子どもと離れない不自然な保護者の態度が見られないか。MSBPとは保護者が巧妙な虚偽や症状の捏造によって、子どもに病的な状態を持続的に造り出す、子ども虐待の一形態である。MSBPは致死率の高い虐待の形であり、MSBPの保護者は実母が非常に多い。MSBPは、比較的良い全身状態にもかかわらず重篤な検査所見がある、保護者の報告との乖離、不自然な検査所見の組合せや推移、一般の医学では考えにくい症状、子どもと離れない不自然な保護者の態度などから疑われるが、医療者が疑いを持つまでに長期間を要することも少なくなく、確定するのは困難を伴う。子どもを守ることができて、確証を得られるのは、保護者と子どもを分離して、症状の消失を確かめることによるが、分離には保護者が抵抗することが多く、一時保護が必要になることが多い。',\n","'代理によるミュンヒハウゼン症候群。代理によるミュンヒハウゼン症候群、ミュンヒハウゼン、MSBP周囲の関心や同情を引くために子どもを故意に病人に仕立て上げる。保護者の報告との乖離、不自然な検査所見の組合せや推移、一般の医学では考えにくい症状、子どもと離れない不自然な保護者の態度が見られないか。MSBPとは保護者が巧妙な虚偽や症状の捏造によって、子どもに病的な状態を持続的に造り出す、子ども虐待の一形態である。MSBPは致死率の高い虐待の形であり、MSBPの保護者は実母が非常に多い。MSBPは、比較的良い全身状態にもかかわらず重篤な検査所見がある、保護者の報告との乖離、不自然な検査所見の組合せや推移、一般の医学では考えにくい症状、子どもと離れない不自然な保護者の態度などから疑われるが、医療者が疑いを持つまでに長期間を要することも少なくなく、確定するのは困難を伴う。子どもを守ることができて、確証を得られるのは、保護者と子どもを分離して、症状の消失を確かめることによるが、分離には保護者が抵抗することが多く、一時保護が必要になることが多い。',\n","'代理によるミュンヒハウゼン症候群。代理によるミュンヒハウゼン症候群、ミュンヒハウゼン、MSBP周囲の関心や同情を引くために子どもを故意に病人に仕立て上げる。保護者の報告との乖離、不自然な検査所見の組合せや推移、一般の医学では考えにくい症状、子どもと離れない不自然な保護者の態度が見られないか。MSBPとは保護者が巧妙な虚偽や症状の捏造によって、子どもに病的な状態を持続的に造り出す、子ども虐待の一形態である。MSBPは致死率の高い虐待の形であり、MSBPの保護者は実母が非常に多い。MSBPは、比較的良い全身状態にもかかわらず重篤な検査所見がある、保護者の報告との乖離、不自然な検査所見の組合せや推移、一般の医学では考えにくい症状、子どもと離れない不自然な保護者の態度などから疑われるが、医療者が疑いを持つまでに長期間を要することも少なくなく、確定するのは困難を伴う。子どもを守ることができて、確証を得られるのは、保護者と子どもを分離して、症状の消失を確かめることによるが、分離には保護者が抵抗することが多く、一時保護が必要になることが多い。',\n","'子ども虐待の発生要因。①保護者側の事情、被虐待歴、愛着不全、社会的未成熟、人格の偏り、精神疾患、薬物依存、知的障害などによる養育能力の問題、子ども理解の歪み、不適切な育児方法の獲得など、②家庭内のストレス、経済困窮、家族関係の不和、看護・介護を要する状況、育児負担の過重、転居・転職など、③社会的孤立、親戚、友人、近隣、関係機関等との社会的なつながりがないなど、④保護者から見た子どもの問題、発達の問題、障害や性格などを起因とする育てにくさ、期待に応えない行動など',\n","'子ども虐待の発生要因。①保護者側の事情、被虐待歴、愛着不全、社会的未成熟、人格の偏り、精神疾患、薬物依存、知的障害などによる養育能力の問題、子ども理解の歪み、不適切な育児方法の獲得など、②家庭内のストレス、経済困窮、家族関係の不和、看護・介護を要する状況、育児負担の過重、転居・転職など、③社会的孤立、親戚、友人、近隣、関係機関等との社会的なつながりがないなど、④保護者から見た子どもの問題、発達の問題、障害や性格などを起因とする育てにくさ、期待に応えない行動など',\n","'子ども虐待の発生要因。①保護者側の事情、被虐待歴、愛着不全、社会的未成熟、人格の偏り、精神疾患、薬物依存、知的障害などによる養育能力の問題、子ども理解の歪み、不適切な育児方法の獲得など、②家庭内のストレス、経済困窮、家族関係の不和、看護・介護を要する状況、育児負担の過重、転居・転職など、③社会的孤立、親戚、友人、近隣、関係機関等との社会的なつながりがないなど、④保護者から見た子どもの問題、発達の問題、障害や性格などを起因とする育てにくさ、期待に応えない行動など',\n","'子ども虐待の発生要因。①保護者側の事情、被虐待歴、愛着不全、社会的未成熟、人格の偏り、精神疾患、薬物依存、知的障害などによる養育能力の問題、子ども理解の歪み、不適切な育児方法の獲得など、②家庭内のストレス、経済困窮、家族関係の不和、看護・介護を要する状況、育児負担の過重、転居・転職など、③社会的孤立、親戚、友人、近隣、関係機関等との社会的なつながりがないなど、④保護者から見た子どもの問題、発達の問題、障害や性格などを起因とする育てにくさ、期待に応えない行動など',\n","\n","]\n","sentences=[\n","           'うちは母親が常にイライラしており、精神的、身体的虐待が慢性化した家庭でした',\n","'また殴られるという身体的虐待を受けていたので「抵抗できない恐怖」と「理由が分からない恐怖」が心に刻まれていると思います',\n","'私は、母親から身体的虐待を受けてます',\n","'私は、物心ついた頃から母親から精神的、身体的虐待を最近まで受け続けてきました',\n","'私は物覚えつくころにはもう(たぶんその前から)、主に母親から身体的虐待、精神的虐待を受けていました',\n","'身体的虐待、心理的虐待、ネグレクトも経験しました',\n","'また、ネグレクトで常にカビの生えた生乾きの下着や服を着ており、お風呂は週一、ゴミ屋敷の中でコンビニ弁当を食べたりと、精神的、身体的に毎日ボロボロでした',\n","'母はネグレクトで、毎日両親二人でパチンコに行っては、帰って来るのは午前様で、夕食は店屋物を買って来て、動物に餌でも与えるような感じで、ほらと渡されていました',\n","'妹と母と父は、私が幼いころから精神的ネグレクトとネグレクトとモラハラと暴力で、毎日絶え間なく虐待し、私の貯金も無断で全て使い果たしました',\n","'母のネグレクトや言葉の暴力、そして実妹の同様の攻撃・嫌がらせなどで苦しみ、最近、投薬を受け、カウンセリングに通い始めました',\n","'身体が成長するとガードができるため次は家庭内で自分の存在が消えるネグレクトが始まりました',\n","'自分がいつか子供に虐待してしまうのではないか、もうネグレクトをしてるんじゃないかと',\n","'私は中学1年生まで実父に性的虐待をうけていました',\n","'私は小学3、4年生の頃、祖父から性的虐待を受けていました',\n","'親からは心理的虐待を、親の愛人からは性的虐待を受けていました',\n","'小学生から高校生まで父親性的から性的虐待を受けていました',\n","'両親からネグレクト、精神的虐待、心理的虐待を受けて育ちました',\n","'母と再婚した父に心理的虐待、ネグレクトを受けていた者です',\n","'私は母親に心理的虐待を受けています',\n","'虐待と聞くと「身体的虐待」を真っ先に思いつくのですが、私の場合は両親からの過干渉とモラハラだったので、「心理的虐待」ではありますが、表面的には見えない言葉の使い方をしてくるので、「子供の乱用」という言葉がピッタリきました',\n","'今31ですが、17くらいまで家族から身体・心理的虐待を受けていました',\n","'わたしの受けた虐待は身体的、性的、心理的虐待の3つです',\n","'心理的虐待/ネグレクトを受けながら育ちました',\n","'物心ついた時から、親から身体的、心理的虐待を受けてきました',\n","'「今は寝ないで!」と脅迫的に叩き続けたこともあります',\n","'実の父は厳しい人で優秀な姉と比較され布団たたきで繰り返し内出血で体が真紫色に腫れるまで殴られたり、死ね、お前なんか生まれてこなければ良かったのに',\n","'鼻血が出るまで殴られ続け妹は殴られて鼓膜が破れ、母は骨折しました',\n","'悪いことをすると暴力を受けたりベランダへ放り出したり…特に母からの虐待は酷く、痣が出来たり骨折するまでフライパンや麺棒などで殴られました',\n","'父は酒乱だったのですが、酔って母と大喧嘩したとき、止めに入った私が手を骨折したにもかかわらず、大怪我をした私を長時間怒鳴り散らしたのです',\n","'母は…身体的暴力を振るう人で骨折も何度もされて私の足は普通のようには歩けません',\n","'子供が二人いるのですが、下の子が知的障害があって、何かとストレスたまることやイライラがよくあります',\n","'長男は知的障害があり悪さばかりをして手に負えず、叩いたり蹴ったり突飛ばしたり',\n","'結果、私は発達障害の中のLD「学習障害」なのだそうです',\n","'これは、私が発達障害の虐待児だったからそう思うのかもしれませんが',\n","'私は発達障害で虐待されて育ち一人で暮らすことが難しい為、今も怯えて生きてます',\n","'幼い頃に親から虐待された被害者は、本来なら最も信頼できる存在であるはずの親を信頼できなくなり、警戒心が強くなります',\n","'あの日を境にいま言語障害、男性恐怖症、対人恐怖症、暴力により足の関節に金属を入れていて走ることは出来ません',\n","'虐待する親は、周囲の人の顔色を優先するあまり、自分の子供の優先度を下げて子供に我慢や無理を強いる',\n","'クラスメートが家庭の話をしている時に、『自分は、この人達とは住んでる世界が違う』と感じ、対人関係もうまくいきませんでした',\n","'一般の幼稚園・学校に行くも、やっぱり自閉症スペクトラムという概念が無く、そこでのルール順守、適応能力、学ぶ力、友人関係、自己管理が欠落しており、ただの甘え、怠けとレッテルを貼られ、いじめ、体罰などの暴力行為で体調を崩そうが、精神を病もうが、無理やり登園・登校させられました',\n","'幼少期より親兄弟の虐待を受けて育ち、自分への嫌悪感や自信欠乏から、愛した旦那との間にできた初めての娘を殺してしまうと盲信し、精神科に搬送され、産後うつと分かり娘は乳児院に保護された',\n","'虐待する親は、周囲の人の顔色を優先するあまり、自分の子供の優先度を下げて子供に我慢や無理を強いる',\n","'そう思ってしまう自分に自己嫌悪になります',\n","'母の機嫌が悪くなるともうこの世の終わりなにもしらない私は愛されようと必死だった今なら、それは親がおかしいよ親もきっとやり方がわからなくて困っているんだあなたは逃げていいんだよって思えるけどねあるがままの自分を認めてもらえなかったその子は自己肯定感が低く、そのストレスやトラウマから精神疾患を患うことが多いのが事実虐待はこんなにも深い傷を子供の心に残す',\n","'自己喪失、「自分」というものが全くなかった私は、長年鬱状態で毎日死ぬことばかり考えていた',\n","'中学になってから、私は「親が子供を叩くのはおかしい!」と反抗するようになって、一時暴力は奮わなくなりましたが、1ヶ月位経つと、また何時ものように殴ったり、蹴ったりします',\n","'実は、私も調べてみましたが、ゆらさんにおける性的虐待のトラウマ症状が専門書にかかれてある諸々の症状にあてはまり、性的トラウマを中心に治療していくことによりうつや乖離の改善、自己破壊行動の抑制に繋がるようです',\n","'私は母親から痣が沢山出来るほど殴られ叩かれその度に死にたくなる衝動に駆られます',\n","'父からの暴力、そして中学時代に始まったいじめの影響で、私は粗暴な同性を過度に恐れるようになっていました',\n","'父も未診断ですが、ADHDの傾向があるようです',\n","'行動も(扉をすごい勢いで閉めたり、車を乱暴に運転したり)乱暴になってきて、すごく怖いです',\n","'虐待なのかどうかわからないのですが、言うこと聞かなかったり、わがままな行動を繰り返したりすると勝手な行動を起こし、最悪の場合は店の前で暴れることも',\n","'心が離れ、お試し行動が増え、悪循環でした',\n","'彼らを許せば、暴力を受け苦しみ、その心の傷に苦しんでいる状態にいる今の私が自分自身に対し心に蓋をし、そっぽを向いている状態になるからです',\n","'今は行政を使って相談している最中ですが仮に親と離れて無事に暮らせても心の傷は消えることは難しく、忘れることも出来ません',\n","'母の機嫌が悪くなるともうこの世の終わりなにもしらない私は愛されようと必死だった今なら、それは親がおかしいよ親もきっとやり方がわからなくて困っているんだあなたは逃げていいんだよって思えるけどねあるがままの自分を認めてもらえなかったその子は自己肯定感が低く、そのストレスやトラウマから精神疾患を患うことが多いのが事実虐待はこんなにも深い傷を子供の心に残す',\n","'なんせ今虐待している大人は小さい頃に受けた親からの虐待がトラウマになってるケースが多いからです!',\n","'大人から浴びせられた言葉は今でもトラウマになっています',\n","'幼い頃から精神的虐待が絶え間なく続き、離れて十数年経つ今でも人に怯え、顔色を伺い心の中でごめんなさいを繰り返す日々です',\n","'母は感情の波の激しい人で、ずっと母の顔色を伺いながら生きていました',\n","'私の父は家父長制の思想に近く「俺が絶対」であり、さらに感情が高ぶると怒鳴り散らし、いつも顔色を見る生活でした',\n","'私は毎日父親の顔色を伺って生活して居ました',\n","'なので徐々に人の顔色ばかり伺う人間となり、もういい大人になった今でも常に自信がなく自尊心が低いため鬱を繰り返しています',\n","'虐待する親は、周囲の人の顔色を優先するあまり、自分の子供の優先度を下げて子供に我慢や無理を強いる',\n","'身体的な虐待は一切なかったものの、ものごころついた頃から父の、これ以上ない怒りの表情や声色、ドアを思い切りしめる態度や家中に響く大きな音、歯を食いしばったり握りこぶしを見せたり机を叩くなどの威圧など…精神的な圧迫で男性恐怖症、人間不信に陥り、結婚はもちろん恋愛もできなくなりました',\n","'教祖は90歳を超えて死に、今はこの世にいませんが、教祖と父による性的虐待・DVにより、男性恐怖症にもなりました',\n","'性的犯罪で子どもは人格否定され異性や人間不信・嫌悪を持たされます',\n","'幼少期より親兄弟の虐待を受けて育ち、自分への嫌悪感や自信欠乏から、愛した旦那との間にできた初めての娘を殺してしまうと盲信し、精神科に搬送され、産後うつと分かり娘は乳児院に保護された',\n","'父は、戦争により精神不安定になった祖父から、酷い暴力を受けて育ち、その影響もあるのか、病的酩酊となりました',\n","'「ガン!!」そんな音がしたあと、後頭部から、血が流れ出しました',\n","'しかしとてもじゃないけど、血がでるまでこんなことされて私は猛烈に頭にきて押し返しました',\n","'万引きを繰り返す私を小4の頃…母は…私の頭をお皿で殴りました',\n","'全裸で外に出されたり、水筒で頭を殴られたり、風呂で溺死させられそうになったり、言い出したらきりがないです',\n","'母が働いて家にいない間、気に入らないことがあると何時間も怒鳴り続ける、過干渉やお前は人間のクズだ、などの暴言、体にアザができるまで暴力を振るったり頭に茶碗をぶつけられ何針も体に残る傷をつけられ、学校でもいじめにあい不登校になってどちらにも居場所がなく祖父母に殺意を覚え毎晩、今日こそ殺そうそれか自殺するしか逃げ場はないと追い詰められているなか、遠くの支援学校に転校という形で中3のときにやっと逃げることができました',\n","'腕とか蹴ってくるし、頭とかめちゃ叩かれます!',\n","'言い返す暇もなくお腹にｹﾘを入れられ拳で顔面殴られ頭から顔から体中殴る蹴るが続き、髪の毛だけをつかまれ隣の部屋へ引きずられ、また殴る蹴る…どのくらいの時間が過ぎたのか',\n","'陶器で何度も頭を殴られたり、日常的に理由もなく自分や弟に罵詈雑言を繰り返しました',\n","'頭の上に乗られたり、パイプ椅子で殴られたりしたおかげか、頭がボコボコです',\n","'揺さぶられたせいで、あなたの赤ちゃんの脳が損傷を受けていたり、 頭蓋骨の中で出血が起こったりしているのに、治療を受けないま まにしていたら、赤ちゃんの状態は悪くなる一方です。',\n","'あなたの赤ちゃんが激しく揺さぶられたのではないかと思ったら、最も大切なことは、一刻も早く、赤ちゃんを病院に連れて行くことです。',\n","'無理に泣きやませようと激しく揺さぶらないでください！',\n","'赤ちゃんの「泣きの特徴」と「激しく揺さぶってはいけないこと」を家族に知ってもらいましょう！',\n","'泣かれてイライラするのはみんな一緒です。赤ちゃんのお世話をする全ての人に揺さぶりの危険性を知ってもらいましょう！',\n","'物心ついたときから体調の不快感があり、幼稚園児の時に母親に訴えても聞き入れてもらえず、それを機に意識がなく、返事をしないと顔を叩く、体を強く揺さぶる、怒鳴るの繰り返しで、意識が戻ると恐怖でいっぱいでした',\n","'なぜ、顔を叩いたのか、揺さぶったのか、怒鳴ったのかの説明が一切なく、自分が悪い事をしているからだと、病気が分かるまで自分自身を責める毎日でした',\n","'「この母親が虐待などするはずがない」と、思わせることが まれではないため、何か「おかしい」と MSBP を疑うことが大切です。',\n","'「これまでに診たことがない」というような、非常に稀な症状であることがある。そのために、様々な検査がおこなわれる。',\n","'両親または養育者によって、子どもに病的な状態が持続的に作られ、医師がその子どもにはさまざまな検査や治療が必要であると誤診するような、巧妙な虚偽や症状の捏造によって作られる子ども虐待の特異な形。',\n","'実際に何らかの薬を飲ませるなどして病気を捏造することもあれば、痙攣が起きていないにもかかわらず虚偽の報告をしたり、子どもの尿に血液などを混入させて血尿として受診するなどの模倣の形をとることがある。',\n","'うちの子、難しい病気なんでしょう?と繰り返し聞いてくる。よくいる心配性なお母さんという感じだった。時には母親を疑ってみる姿勢がないと、不必要に採血したり、子供を傷つけてしまうと反省した',\n","'なんせ今虐待している大人は小さい頃に受けた親からの虐待がトラウマになってるケースが多いからです!',\n","'子どものころから、虚しさや、喪失感、孤立感、生きているのに死んだような感じでした',\n","'専門家によれば、虐待の背景には、何らかの“家庭の不調和”が見られるケースが少なくないといいます',\n","'私の精神疾患もあり旦那と殴り合いの喧嘩も多々ありました(今はかなり落ち着きました)子どもに手を出すことはありませんが、息子には暴力はこの世に存在し消えるには難しいと',\n","\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GlxcLo5SI4La","colab_type":"code","colab":{}},"source":["\n","from sklearn.metrics.pairwise import cosine_similarity\n","remark_list=[]\n","guidan_list=[]\n","guidan_context_list=[]\n","simlst=[]\n","for gui, sent in zip(guidance,sentences):\n","  vs=embed_ml3(sent)\n","  vg=embed_ml3(gui)\n","  sim=cosine_similarity(vs,vg)\n","\n","  # print(sim[0][0])\n","  simlst.append(sim[0][0])\n","  # remark_list.append(sent)\n","  # guidan_list.append(gui)\n","  # guidan_context_list.append(''.join(map(str,list(reversed(gui)))))\n","  # # guidan_context_list.append('')\n","\n","# print('guidance vec end',dt.utcnow().strftime( '%y/%m/%d %H:%M:%S'))\n","# sim_rg_matrix_org=embed_qa_sim( remark_list, guidan_list,guidan_context_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1WDdcPUQKC6M","colab_type":"code","colab":{}},"source":["sim_rg_matrix_org\n","import pandas as pd\n","df=pd.DataFrame(simlst)\n","df.to_csv('aa.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9hdR_VicvLWK","colab_type":"text"},"source":["#KNN"]},{"cell_type":"code","metadata":{"id":"B-0CKOlwvM9K","colab_type":"code","colab":{}},"source":["#https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn\n","#Import scikit-learn dataset library\n","from sklearn import datasets\n","#Load dataset\n","wine = datasets.load_wine()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vr2eiFvivZAc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1600307179397,"user_tz":-540,"elapsed":1329,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"49f2c523-5026-436d-e2ce-4ac61e9b68a4"},"source":["# print the names of the features\n","print(wine.feature_names)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"D8xV_W_wvlEW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600307225716,"user_tz":-540,"elapsed":630,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"aa3d8473-184b-49f9-83e5-1479b66845b0"},"source":["# print the label species(class_0, class_1, class_2)\n","print(wine.target_names)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['class_0' 'class_1' 'class_2']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PA1LQTxWvn7n","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"status":"ok","timestamp":1600307263011,"user_tz":-540,"elapsed":1259,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"133382d4-ea10-4c15-94ae-30d82d9b2eae"},"source":["# print the wine data (top 5 records)\n","print(wine.data[0:5])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n","  2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]\n"," [1.320e+01 1.780e+00 2.140e+00 1.120e+01 1.000e+02 2.650e+00 2.760e+00\n","  2.600e-01 1.280e+00 4.380e+00 1.050e+00 3.400e+00 1.050e+03]\n"," [1.316e+01 2.360e+00 2.670e+00 1.860e+01 1.010e+02 2.800e+00 3.240e+00\n","  3.000e-01 2.810e+00 5.680e+00 1.030e+00 3.170e+00 1.185e+03]\n"," [1.437e+01 1.950e+00 2.500e+00 1.680e+01 1.130e+02 3.850e+00 3.490e+00\n","  2.400e-01 2.180e+00 7.800e+00 8.600e-01 3.450e+00 1.480e+03]\n"," [1.324e+01 2.590e+00 2.870e+00 2.100e+01 1.180e+02 2.800e+00 2.690e+00\n","  3.900e-01 1.820e+00 4.320e+00 1.040e+00 2.930e+00 7.350e+02]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"T-7fWuQCvr07","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"status":"ok","timestamp":1600307251162,"user_tz":-540,"elapsed":581,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"f3cb4145-9600-404c-f5a8-cf1b3549120b"},"source":["# print the wine labels (0:Class_0, 1:Class_1, 2:Class_3)\n","print(wine.target)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TwSmj2GTvwSX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600307269710,"user_tz":-540,"elapsed":878,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"73dcdff3-dfa9-4706-e02f-52556c9cb67f"},"source":["# print data(feature)shape\n","print(wine.data.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(178, 13)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VZvbjMQZvyJt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600307280775,"user_tz":-540,"elapsed":566,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"abe360c3-905e-417a-f61c-548bb6856199"},"source":["# print target(or label)shape\n","print(wine.target.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(178,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iDs9AZbmv2I_","colab_type":"code","colab":{}},"source":["# Import train_test_split function\n","from sklearn.model_selection import train_test_split\n","\n","# Split dataset into training set and test set\n","X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.3) # 70% training and 30% test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DE_cWxnpv2-V","colab_type":"code","colab":{}},"source":["#Import knearest neighbors Classifier model\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","#Create KNN Classifier\n","knn = KNeighborsClassifier(n_neighbors=5)\n","\n","#Train the model using the training sets\n","knn.fit(X_train, y_train)\n","\n","#Predict the response for test dataset\n","y_pred = knn.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7fj2Ss0-v6fW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600307320066,"user_tz":-540,"elapsed":771,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"f2d14f1f-c0b9-4e25-d7df-ab965088f59d"},"source":["#Import scikit-learn metrics module for accuracy calculation\n","from sklearn import metrics\n","# Model Accuracy, how often is the classifier correct?\n","print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 0.7592592592592593\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lBgH0h5qwBfq","colab_type":"code","colab":{}},"source":["#Import knearest neighbors Classifier model\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","#Create KNN Classifier\n","knn = KNeighborsClassifier(n_neighbors=7)\n","\n","#Train the model using the training sets\n","knn.fit(X_train, y_train)\n","\n","#Predict the response for test dataset\n","y_pred = knn.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QnWOrkx0wEOH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600307350765,"user_tz":-540,"elapsed":661,"user":{"displayName":"yu. shenglong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwuxc3wT6sD4ow5bb6_oIWUY3cGDX3PR6cTt0-w=s64","userId":"09595788892918776995"}},"outputId":"e74e6eb4-7d7d-41a1-adde-3e63a9bf4434"},"source":["#Import scikit-learn metrics module for accuracy calculation\n","from sklearn import metrics\n","# Model Accuracy, how often is the classifier correct?\n","print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 0.7592592592592593\n"],"name":"stdout"}]}]}